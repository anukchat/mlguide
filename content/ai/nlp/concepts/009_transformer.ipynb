{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "**<mark>(Machine Translation Example)</mark>**\n",
    "\n",
    "**Transformer** is popular SOTA (state-of-the-art) architecture and used in today's a lot of successful works in neural methods.<br>\n",
    "\n",
    "Finally we'll implement transformer using previously learned architectures - such as, language modeling, encoder-decoder, and attention.\n",
    "\n",
    "As you saw in [exercise 08](./08_attention.ipynb), <marl>attention captures the distant relationship and contexts in sequences.</mark>\n",
    "\n",
    "**Transformer is motivated by this successful architecture.**\n",
    "\n",
    "![Attention](./images/attend_image.png)<br>\n",
    "*From : \"08 Attention (Machine Translation Example)\"*\n",
    "\n",
    "In this example, we will implement the architecture written in the famous paper \"[**Attention Is All You Need**](https://arxiv.org/abs/1706.03762)\".<br>\n",
    "\n",
    "As you saw in [exercise 08](./08_attention.ipynb), we have used RNN architecture (GRU gate) for **getting contexts in encoder and decoder**. \n",
    "\n",
    "However, <mark>in this transformer architecture, attention is also used even for getting contexts in encoder and decoder</mark>, instead of using RNN architecture. (In below architecture, you will find that there's no RNN layers.)<br>\n",
    "\n",
    "Total 3 attend layers **(encoder's self-attention, decoder's self-attention, and encoder-decoder cross attention**) are then applied in this network.\n",
    "\n",
    "![Transformer](./images/transformer.png)<br>\n",
    "*From : \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\" (Vaswani, et al., 2017)*\n",
    "\n",
    "Unlike soft attention in [exercise 08](./08_attention.ipynb), it applies the following attention - which is called \"**scaled dot-product attention**\" - in these 3 parts of attend layers.<br>\n",
    "\n",
    "As you can see below, this <mark>model measures the similarity by the dot-product operation</mark>, and 3 networks for composing query, key, and value will be trained.\n",
    "\n",
    "ðŸ‘‰ When the query vector and key vector are similar, it will have a large value of dot product between these vectors. \n",
    "\n",
    "ðŸ‘‰ Such like soft attention (in [exercise 08](./08_attention.ipynb)), the matrix $Q \\cdot K^T$ will then have the relationship mapping (weight's mapping) between query and key. \n",
    "\n",
    "ðŸ‘‰ Finally, by applying dot-product operation again between this result and value's vector, the final feature vectors will be obtained in each token.<br>\n",
    "\n",
    "ðŸ‘‰ To say intuitively, first dot-product operation asks each keys by queries in the sequence, and then composes the objectives by combining between its results and values by the second dot-product operation.\n",
    "\n",
    "![Multi-head attention](./images/multi_head_attention.png)\n",
    "\n",
    "In the <mark>attend layer in encoding and decoding (see below), input1 and input2 are the same sequence in above scaled dot-product attention.</mark> This architecture is called **self-attention**.<br>\n",
    "\n",
    "\n",
    "For instance, the word \"this\" in \"this is a pen\" will have the close relationship with the word \"pen\". The self-attention captures this kind of self-regressive relations in the sequence.<br>\n",
    "\n",
    "By capturing these relations, the sequence will be well annotated in both encoder and decoder, instead of using RNN layer.\n",
    "\n",
    "![Self-attention parts](./images/transformer_self_attention.png)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "ðŸ’¡ **Note** : On contrary, the above cross-attention layer is for machine translation task.\n",
    "\n",
    "In the self-attention layer in decoder, each time segment should not refer to the future segment. It then applies the masked attention, instead of applying fully-connected attention.<br>\n",
    "\n",
    "Later we will see about this causal attention.\n",
    "\n",
    "By the design of this architecture, <mark>transformers will have the ability to capture the distant contexts</mark> and we can also expect to have the ability to capture more difficult contexts rather than RNN-based encoder-decoder.\n",
    "\n",
    "</div>\n",
    "\n",
    "**Transformer** is today's key part for SOTA (state-of-the-art) language models and a lot of today's famous algorithms (such as, BERT, T5, GPT, etc) use transformers in its architectures.<br>\n",
    "\n",
    "As you saw in [basic language model's example](./05_language_model_basic.ipynb), we can train transformers with unlabeled data (i.e, self-supervised learning) - such as, next word's prediction, masked word's prediction.\n",
    "\n",
    "By this unsupervised fashion, a lot of today's algorithms learn a lot of language properties with existing large corpus (such as, Wikipedia), and can then be fine-tuned for downstream tasks with small number of labeled data - such as, sentiment detection, text classification, summarization, or modern instruction fine-tuning, etc. (See [here](https://tsmatz.wordpress.com/2022/10/24/huggingface-japanese-ner-named-entity-recognition/) for fine-tuning example in Hugging Face.)<br>\n",
    "\n",
    "For example, OpenAI GPT-3 175B is trained on 3.7 trillion tokens.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "ðŸ’¡ **Note** : Transformers are categorized into 3 types : **encoder-only**, **decoder-only**, and **encoder-decoder**. (See [here](https://arxiv.org/abs/2304.13712) for the summary of encoder-only, decoder-only, and encoder-decoder language models.)<br>\n",
    "\n",
    "For instance, you can use encoder-only transformer for applying classification (such as, named entity recognitions), generating images, sentence embedding, and more.<br>\n",
    "\n",
    "A lot of today's popular large language models (LLMs) - such as, **ChatGPT**, **LLaMA**, etc - are **decoder-only models**. These are **in-context generative models**, in which the **output is generated by using only input (prompt)**, and the context of input's text (**prompting**) is then **very important** to get the optimal results.\n",
    "\n",
    "These models are trained by 3-stages for practical use : \n",
    "\n",
    "1. pretraining (self-supervised learning), \n",
    "2. supervised fine-tuning (SFT), and \n",
    "3. alignment. \n",
    " \n",
    "(See [here](https://arxiv.org/abs/2203.02155) for these 3 training strategies.)\n",
    "\n",
    "</div>\n",
    "\n",
    "Now let's see the implementation of each part in this transofrmer architecture, and train the model for machine translation task.<br>\n",
    "\n",
    "For learning purpose, we'll implement all layers (modules) from scratch, but you can also use built-in ```torch.nn.MultiheadAttention``` in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.13.1 torchtext==0.14.1 --extra-index-url https://download.pytorch.org/whl/cu114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I use Engligh-French dataset by [Anki](https://www.manythings.org/anki/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-19 07:59:20--  http://www.manythings.org/anki/fra-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6720195 (6.4M) [application/zip]\n",
      "Saving to: â€˜fra-eng.zipâ€™\n",
      "\n",
      "fra-eng.zip         100%[===================>]   6.41M  11.6MB/s    in 0.6s    \n",
      "\n",
      "2023-02-19 07:59:21 (11.6 MB/s) - â€˜fra-eng.zipâ€™ saved [6720195/6720195]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  fra-eng.zip\n",
      "  inflating: fra-eng/_about.txt      \n",
      "  inflating: fra-eng/fra.txt         \n"
     ]
    }
   ],
   "source": [
    "!unzip fra-eng.zip -d fra-eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\r\n",
      "Go.\tMarche.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)\r\n",
      "Go.\tEn route !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)\r\n",
      "Go.\tBouge !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)\r\n",
      "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 fra-eng/fra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197463 fra-eng/fra.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fra-eng/fra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Va !', 'Go.'], dtype='<U349')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "pathobj = Path(\"fra-eng/fra.txt\")\n",
    "text_all = pathobj.read_text(encoding=\"utf-8\")\n",
    "lines = text_all.splitlines()\n",
    "train_data = [line.split(\"\\t\") for line in lines]\n",
    "train_data = np.array(train_data)[:,[1,0]]\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training set, text length in the latter part is longer (and includes multiple sentences) than the former part.<br>\n",
    "\n",
    "Therefore we will shuffle entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Je suis heureux que vous veniez.', \"I'm glad you're coming.\"],\n",
       "      dtype='<U349')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(train_data)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data consists of multiple sentences, it converts to a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tsmatsuz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "tokenizer_en = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "tokenizer_fr = nltk.data.load(\"tokenizers/punkt/french.pickle\")\n",
    "fr_list = []\n",
    "en_list = []\n",
    "for x in train_data:\n",
    "    x1 = tokenizer_fr.tokenize(x[0])\n",
    "    x2 = tokenizer_en.tokenize(x[1])\n",
    "    if len(x1) == len(x2):\n",
    "        fr_list += x1\n",
    "        en_list += x2\n",
    "train_data = np.column_stack((fr_list, en_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the better performance (accuracy), I standarize the input text as follows.\n",
    "- Make all words to lowercase in order to reduce words\n",
    "- Make \"-\" (hyphen) to space\n",
    "- Remove all punctuation except \" ' \" (e.g, Ken's bag, ces't, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['je suis heureux que vous veniez', \"i'm glad you're coming\"],\n",
       "      dtype='<U250')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "train_data = np.char.lower(train_data)\n",
    "train_data = np.char.replace(train_data, \"-\", \" \")\n",
    "for x in string.punctuation.replace(\"'\", \"\"):\n",
    "    train_data = np.char.replace(train_data, x, \"\")\n",
    "for x in \"Â«Â»\":\n",
    "    train_data = np.char.replace(train_data, x, \"\")\n",
    "train_data = np.char.strip(train_data)\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ```<start>``` and ```<end>``` tokens in string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<start> je suis heureux que vous veniez <end>',\n",
       "       \"<start> i'm glad you're coming <end>\"], dtype='<U264')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.array([[\" \".join([\"<start>\", x, \"<end>\"]), \" \".join([\"<start>\", y, \"<end>\"])] for x, y in train_data])\n",
    "# print first row\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequence inputs\n",
    "\n",
    "We will generate the sequence of word's indices (i.e, tokenize) from text.\n",
    "\n",
    "![Index vectorize](images/index_vectorize2.png)\n",
    "\n",
    "First we create a list of vocabulary (```vocab```) for both source text (French) and target text (English) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "max_word = 10000\n",
    "\n",
    "# create space-split tokenizer\n",
    "tokenizer = get_tokenizer(None)\n",
    "\n",
    "# define tokenization function\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        tokens = tokenizer(text)\n",
    "        yield tokens\n",
    "\n",
    "# build vocabulary list for French\n",
    "vocab_fr = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data[:,0]),\n",
    "    specials=[\"<unk>\"],\n",
    "    max_tokens=max_word,\n",
    ")\n",
    "vocab_fr.set_default_index(vocab_fr[\"<unk>\"])\n",
    "\n",
    "# build vocabulary list for English\n",
    "vocab_en = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data[:,1]),\n",
    "    specials=[\"<unk>\"],\n",
    "    max_tokens=max_word,\n",
    ")\n",
    "vocab_en.set_default_index(vocab_en[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated token index is ```0, 1, ... , vocab_size - 1```.<br>\n",
    "\n",
    "Now we will set ```vocab_size``` as a token id in padded positions for both French and English respctively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_index_fr = vocab_fr.__len__()\n",
    "vocab_fr.append_token(\"<pad>\")\n",
    "\n",
    "pad_index_en = vocab_en.__len__()\n",
    "vocab_en.append_token(\"<pad>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list for both index-to-word and word-to-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos_fr = vocab_fr.get_itos()\n",
    "stoi_fr = vocab_fr.get_stoi()\n",
    "\n",
    "itos_en = vocab_en.get_itos()\n",
    "stoi_en = vocab_en.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of token index in French (source) is 10001.\n",
      "The padded index in French (source) is 10000.\n",
      "The number of token index in English (target) is 10001.\n",
      "The padded index in English (target) is 10000.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(\"The number of token index in French (source) is {}.\".format(vocab_fr.__len__()))\n",
    "print(\"The padded index in French (source) is {}.\".format(stoi_fr[\"<pad>\"]))\n",
    "print(\"The number of token index in English (target) is {}.\".format(vocab_en.__len__()))\n",
    "print(\"The padded index in English (target) is {}.\".format(stoi_en[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a collator function, which is used for pre-processing in data loader.\n",
    "\n",
    "In this collator,\n",
    "\n",
    "ðŸ‘‰ First we create a list of word's indices for source (French) and target (English) respectively as follows.\n",
    "\n",
    "```<start> this is pen <end>``` --> ```[2, 7, 5, 14, 1]```\n",
    "\n",
    "ðŸ‘‰ For target (English) sequence, we separate into features (x) and labels (y).<br>\n",
    "In this task, we predict the next word in target (English) sequence using the current word's sequence (English) and the encoded context of source (French).<br>\n",
    "We then separate target sequence into the sequence iteself (x) and the following label (y).\n",
    "\n",
    "<u>before</u> :\n",
    "\n",
    "```[2, 7, 5, 14, 1]```\n",
    "\n",
    "<u>after</u> :\n",
    "\n",
    "```x : [2, 7, 5, 14, 1]```\n",
    "\n",
    "```y : [7, 5, 14, 1, -100]```\n",
    "\n",
    "> Note : Here I set -100 as an unknown label id, because PyTorch cross-entropy function (```torch.nn.functional.cross_entropy()```) has a property ```ignore_index``` which default value is -100.\n",
    "\n",
    "ðŸ‘‰ Finally we pad the inputs (for both source and target) as follows.<br>\n",
    "The padded index in features is ```pad_index``` and the padded index in label is -100. (See above note.)\n",
    "\n",
    "```x : [2, 7, 5, 14, 1, N, ... , N]```\n",
    "\n",
    "```y : [7, 5, 14, 1, -100, -100, ... , -100]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seq_len_fr = 45\n",
    "seq_len_en = 38\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, feature_source_list, feature_target_list = [], [], []\n",
    "    for text_fr, text_en in batch:\n",
    "        # (1) tokenize to a list of word's indices\n",
    "        tokens_fr = vocab_fr(tokenizer(text_fr))\n",
    "        tokens_en = vocab_en(tokenizer(text_en))\n",
    "        # (2) separate into features and labels in target tokens (English)\n",
    "        y = tokens_en[1:]\n",
    "        y.append(-100)\n",
    "        # (3) limit length to seq_len and pad sequence\n",
    "        y = y[:seq_len_en]\n",
    "        tokens_fr = tokens_fr[:seq_len_fr]\n",
    "        tokens_en = tokens_en[:seq_len_en]\n",
    "        y += [-100] * (seq_len_en - len(y))\n",
    "        tokens_fr += [pad_index_fr] * (seq_len_fr - len(tokens_fr))\n",
    "        tokens_en += [pad_index_en] * (seq_len_en - len(tokens_en))\n",
    "        # add to list\n",
    "        label_list.append(y)\n",
    "        feature_source_list.append(tokens_fr)\n",
    "        feature_target_list.append(tokens_en)\n",
    "    # convert to tensor\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64).to(device)\n",
    "    feature_source_list = torch.tensor(feature_source_list, dtype=torch.int64).to(device)\n",
    "    feature_target_list = torch.tensor(feature_target_list, dtype=torch.int64).to(device)\n",
    "    return label_list, feature_source_list, feature_target_list\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    list(zip(train_data[:,0], train_data[:,1])),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape in batch : torch.Size([32, 38])\n",
      "feature source shape in batch : torch.Size([32, 45])\n",
      "feature target shape in batch : torch.Size([32, 38])\n",
      "***** label sample *****\n",
      "tensor([  14,  427,  164,   14,    8,  593,    1, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100], device='cuda:0')\n",
      "***** features (source) sample *****\n",
      "tensor([    2,    13,    39,   255,    53,    15,   641,     1, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000], device='cuda:0')\n",
      "***** features (target) sample *****\n",
      "tensor([    2,    14,   427,   164,    14,     8,   593,     1, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
      "        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for labels, sources, targets in dataloader:\n",
    "    break\n",
    "\n",
    "print(\"label shape in batch : {}\".format(labels.size()))\n",
    "print(\"feature source shape in batch : {}\".format(sources.size()))\n",
    "print(\"feature target shape in batch : {}\".format(targets.size()))\n",
    "print(\"***** label sample *****\")\n",
    "print(labels[0])\n",
    "print(\"***** features (source) sample *****\")\n",
    "print(sources[0])\n",
    "print(\"***** features (target) sample *****\")\n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "In the formal algorithms of transformer, the set of embedded tokens are encoded by positions without any additional parameters.\n",
    "\n",
    "![Positional encoding](./images/transformer_positional_encoding.png)\n",
    "\n",
    "If there's no positional encoding, <mark>the sequence will be treated as a bag of tokens in neural networks.</mark>\n",
    "\n",
    "**The positional information is needed for position-aware processing in attention.**\n",
    "\n",
    "There exist several ways (variations) for positional encoding.<br>\n",
    "\n",
    "In this example, I'll apply the following positional encoding method (called **sinusoidal positional encoding**), which is introduced in the [original paper](https://arxiv.org/abs/1706.03762) of transformer.\n",
    "\n",
    "ðŸ‘‰ The positional vector $PE(t)$ is :\n",
    "\n",
    "$$ \n",
    "PE(t,2i) = \\sin(t / 10000^{2i / d_e})\n",
    "PE(t,2i+1) = \\cos(t / 10000^{2i / d_e})\n",
    "\n",
    "\n",
    "for 0 \\leq i \\lt d_e / 2\n",
    "$$\n",
    "where $t = \\{0, 1, \\ldots\\}$ is position (time-step) and $d_e$ is embedding dimemsion.\n",
    "\n",
    "ðŸ‘‰ For $t$-th token in the sequence, the embedding $E(t) \\in \\mathbb{R}^{d_e}$ then becomes $ E(t) + PE(t) $.\n",
    "\n",
    "> Note : Here we assume that $d_e$ (embedding dimension) is an even number. (When it's an odd number, the dimension between $E(t)$ and $PE(t)$ differs.)\n",
    "\n",
    "ðŸ‘‰ By applying this positional encoding, we can expect that the attention network will easily learn the position in each tokens, since there always exist a $ 2 \\times 2 $ matrix $\\mathbf{M}_{ik}$ (depending on $i$ and $k$) which satisfies\n",
    "\n",
    "$$ \\begin{pmatrix} PE(t+k,2i)\\\\PE(t+k,2i+1) \\end{pmatrix} = \\mathbf{M}_{ik} \\begin{pmatrix} PE(t,2i)\\\\PE(t,2i+1) \\end{pmatrix} $$\n",
    "\n",
    "for any $t$.\n",
    "\n",
    "> Note : In GPT, positional encoding is not a fixed encoding (not like above sinusoidal positional encoding) and it's also learned in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len, embedding_dim):\n",
    "        assert(embedding_dim % 2 == 0)\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 1 / 10000^{2i / d_e}\n",
    "        #   --> (embedding_dim / 2, )\n",
    "        interval = 1.0 / (10000**(torch.arange(0, embedding_dim, 2.0) / embedding_dim))\n",
    "        # t\n",
    "        #   --> (seq_len, )\n",
    "        position = torch.arange(0, seq_len).float()\n",
    "        # t / 10000^{2i / d_e}\n",
    "        #   --> (seq_len, embedding_dim / 2)\n",
    "        radian = position[:, None] * interval[None, :]\n",
    "        # sin(t / 10000^{2i / d_e})\n",
    "        #   --> (seq_len, embedding_dim / 2, 1)\n",
    "        sin = torch.sin(radian).unsqueeze(dim=-1)\n",
    "        # cos(t / 10000^{2i / d_e})\n",
    "        #   --> (seq_len, embedding_dim / 2, 1)\n",
    "        cos = torch.cos(radian).unsqueeze(dim=-1)\n",
    "        # PE\n",
    "        #   --> (seq_len, embedding_dim / 2, 2)\n",
    "        pe_tmp = torch.concat((sin, cos), dim=-1)\n",
    "        # reshape\n",
    "        #   --> (seq_len, embedding_dim)\n",
    "        d = pe_tmp.size()[1]\n",
    "        self.pe = pe_tmp.view(-1, d * 2).to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return inputs + self.pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the output of positional encoding layer.<br>\n",
    "In the following example, positional vectors will become :\n",
    "\n",
    "```\n",
    "[\n",
    "  [sin(0), cos(0), sin(0/100), cos(0/100)],\n",
    "  [sin(1), cos(1), sin(1/100), cos(1/100)],\n",
    "  [sin(2), cos(2), sin(2/100), cos(2/100)],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### positional vector #####\n",
      "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
      "        [ 0.9093, -0.4161,  0.0200,  0.9998]], device='cuda:0')\n",
      "##### input vector #####\n",
      "tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.]],\n",
      "\n",
      "        [[13., 14., 15., 16.],\n",
      "         [17., 18., 19., 20.],\n",
      "         [21., 22., 23., 24.]]], device='cuda:0')\n",
      "##### output #####\n",
      "tensor([[[ 1.0000,  3.0000,  3.0000,  5.0000],\n",
      "         [ 5.8415,  6.5403,  7.0100,  9.0000],\n",
      "         [ 9.9093,  9.5839, 11.0200, 12.9998]],\n",
      "\n",
      "        [[13.0000, 15.0000, 15.0000, 17.0000],\n",
      "         [17.8415, 18.5403, 19.0100, 21.0000],\n",
      "         [21.9093, 21.5839, 23.0200, 24.9998]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test = PositionalEncoding(3, 4).to(device)\n",
    "print(\"##### positional vector #####\")\n",
    "print(test.pe)\n",
    "# The input size should be (batch_size, seq_len, embedding_dim)\n",
    "x = torch.arange(1, 25).float()\n",
    "x = x.view(2, 3, 4).to(device)\n",
    "print(\"##### input vector #####\")\n",
    "print(x)\n",
    "y = test(x)\n",
    "print(\"##### output #####\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multi-head and Scaled Dot-Product Attention\n",
    "\n",
    "Next I'll implement attention layer as follows.<br>\n",
    "\n",
    "For the purpose learning, we'll **implement the scaled dot-product attention layer from scratch**.\n",
    "\n",
    "**Note** : In PyTorch, you can use built-in **torch.nn.MultiheadAttention**.\n",
    "\n",
    "In 3 parts of attention (encoder's self-attention, decoder's self-attention, and encoder-decoder cross attention), it runs the following steps. (See above description for the semantics of this model.) :\n",
    "\n",
    "ðŸ‘‰ The embedded inputs in the sequence are processed by dense networks (fully-connected feed-forward networks), and \"query\" ($Q$), \"key\" ($K$), and \"value\" ($V$) are then generated.\n",
    "\n",
    "ðŸ‘‰  Compute the relationship score between $Q$ and $K$ by the dot product, $Q \\cdot K^T$.\n",
    "\n",
    "ðŸ‘‰  Scale the score by multiplying $\\frac{1}{\\sqrt{d}}$, where $d$ is the number of attention dimension.\n",
    "\n",
    "ðŸ‘‰  In decoder side, apply causal mask.<br>\n",
    "Later I'll explain details about this optional causal mask ...\n",
    "\n",
    "ðŸ‘‰  The relationship between $Q$ and $K$ are softmaxed, i.e, normalized by $\\displaystyle \\frac{e^{s_i}}{e^{s_0} + e^{s_1} + \\cdots + e^{s_{t-1}}}$ where $(s_0, s_1, \\ldots , s_{t-1})$ is the relationship vector and $t$ is time-step.<br>\n",
    "\n",
    "Note that the above scaling by $\\frac{1}{\\sqrt{d}}$ works as a softmax temprature in this step.\n",
    "\n",
    "ðŸ‘‰  Finally the result (softmaxed score) is performed by the dot product with $V$.<br>\n",
    "The final result is then $\\displaystyle \\verb|softmax| \\left( \\frac{Q \\cdot K^T}{\\sqrt{d}} \\right) \\cdot V$.\n",
    "\n",
    "As mentioned arlier, input1 and input2 in the following picture will be the same in ecoder's and decoder's self-attention parts.\n",
    "\n",
    "![Multi-head attention](./images/transformer_attention.png)\n",
    "\n",
    "To make **multiple attention work in parallel**, model **dimension** (here 256) is **divided into multiple heads** (here 8), and each head will then have $\\frac(model_dim,head_num)$ dimension (here 32). \n",
    "\n",
    "ðŸ‘‰  Finally, these separated heads are concatenated and then applied dense network to obtain model dimension's result. (See above picture.)<br>\n",
    "\n",
    "This technique will make our model have rich expression without losing the computing costs.\n",
    "\n",
    "**Now it's time to explain about causal mask.**\n",
    "\n",
    "In self-attention on decoder's side (when ```use_causal_mask=True``` in the following code), <mark>each token only refers past tokens and cannot access to the future tokens.</mark> (See the following picture.)\n",
    "\n",
    "![Causal reference](images/transformer_causal_reference.png)\n",
    "\n",
    "For this reason, the softmax operation in decoder's self-attention is performed only on lower triangular matrix as follows.<br>\n",
    "\n",
    "This is because we apply optional mask before softmax operation.\n",
    "\n",
    "![Causal attention in decoder](images/transformer_causal_attention.png)\n",
    "\n",
    "**Note** : To make a lower triangular matrix, here I use ```torch.tril()``` in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MyMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Initializes a MyMultiHeadAttention module.\n",
    "\n",
    "    Args:\n",
    "        embedding_dim (int): The number of embedding dimensiom\n",
    "        attention_dim (int): The number of dimension within attention unit\n",
    "        num_heads (int): The number of the divided heads (See above.)\n",
    "        use_causal_mask (bool): Whether to mask the future tokens (See above.)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        attention_dim,\n",
    "        num_heads,\n",
    "        use_causal_mask=False):\n",
    "\n",
    "        assert(attention_dim % num_heads == 0)\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "        self.head_dim = int(attention_dim / num_heads)\n",
    "        self.use_causal_mask = use_causal_mask\n",
    "\n",
    "        self.q_layer = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.k_layer = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.v_layer = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.output_linear = nn.Linear(attention_dim, attention_dim, bias=False)\n",
    "\n",
    "    \"\"\"\n",
    "    When self-attention, input2 will be None\n",
    "    \"\"\"\n",
    "    def forward(self, input1, mask1, input2=None, mask2=None):\n",
    "        if input2 is None:\n",
    "            input2 = input1\n",
    "        if mask2 is None:\n",
    "            mask2 = mask1\n",
    "\n",
    "        # get size\n",
    "        seq_len1 = input1.size()[1]\n",
    "        seq_len2 = input2.size()[1]\n",
    "\n",
    "        # apply query/key/value net - see above 1\n",
    "        #   --> (batch_size, seq_len, attention_dim)\n",
    "        q = self.q_layer(input1)\n",
    "        k = self.k_layer(input2)\n",
    "        v = self.v_layer(input2)\n",
    "\n",
    "        # divide into multiple heads :\n",
    "        #   --> (batch_size, seq_len, num_heads, attention_dim / num_heads)\n",
    "        q = q.view(-1, seq_len1, self.num_heads, self.head_dim)\n",
    "        k = k.view(-1, seq_len2, self.num_heads, self.head_dim)\n",
    "        v = v.view(-1, seq_len2, self.num_heads, self.head_dim)\n",
    "\n",
    "        # compute Q K^T - see above 2\n",
    "        #   --> (batch_size, seq_len1, seq_len2, num_heads)\n",
    "        score = torch.einsum(\"bihd,bjhd->bijh\", q, k)\n",
    "        \n",
    "        # scale the result by 1/sqrt(d) - see above 3\n",
    "        #   --> (batch_size, seq_len1, seq_len2, num_heads)\n",
    "        score = score / self.head_dim**0.5\n",
    "\n",
    "        # generate causal mask matrix - see above 4\n",
    "        # (for decoder's self-attention only)\n",
    "        #   --> (seq_len1, seq_len2)\n",
    "        causal_mask = torch.ones(seq_len1, seq_len2).int().to(device)\n",
    "        if self.use_causal_mask:\n",
    "            # when applying causal mask, the shape of input1 and input2 should be same\n",
    "            assert(seq_len1 == seq_len2)\n",
    "            causal_mask = torch.tril(causal_mask)\n",
    "\n",
    "        # generate sequence mask matrix\n",
    "        #   --> (batch_size, seq_len1, 1) @ (batch_size, 1, seq_len2) = (batch_size, seq_len1, seq_len2)\n",
    "        # (note : bmm should be used for TensorFloat32. Here I then use torch.einsum() instead.)\n",
    "        seq_mask = torch.einsum(\n",
    "            \"bxt,bty->bxy\",\n",
    "            mask1.unsqueeze(dim=2),\n",
    "            mask2.unsqueeze(dim=1))\n",
    "        # seq_mask = mask1.unsqueeze(dim=2) @ mask2.unsqueeze(dim=1)\n",
    "\n",
    "        # generate final mask matrix\n",
    "        #   --> (batch_size, seq_len1, seq_len2)\n",
    "        mask = causal_mask * seq_mask\n",
    "        #   --> (batch_size, seq_len1, seq_len2, 1)\n",
    "        mask = mask.unsqueeze(dim=3)\n",
    "        #   --> (batch_size, seq_len1, seq_len2, num_heads)\n",
    "        mask = mask.expand(-1, -1, -1, self.num_heads)\n",
    "\n",
    "        # apply softmax with mask - see above 5\n",
    "        #   --> (batch_size, seq_len1, seq_len2, num_heads)\n",
    "        score = score.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        score = F.softmax(score, dim=2)\n",
    "        # values in input1's padded position will become \"nan\" by\n",
    "        # softmax operation, because it's divided by zero.\n",
    "        score = score.nan_to_num()\n",
    "\n",
    "        # dot product with V - see above 6\n",
    "        #   --> (batch_size, seq_len1, num_heads, attention_dim / num_heads)\n",
    "        out = torch.einsum(\"bijh,bjhd->bihd\", score, v)\n",
    "\n",
    "        # concatenate all heads and apply linear\n",
    "        #   --> (batch_size, seq_len1, attention_dim)\n",
    "        out = out.reshape(-1, seq_len1, self.attention_dim)\n",
    "        #   --> (batch_size, seq_len1, attention_dim)\n",
    "        out = self.output_linear(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "Now let's implement encoder side using previously generated multi-head attention module.<br>\n",
    "\n",
    "Same as [exercise 08](./08_attention.ipynb), the <mark>purpose of encoder is to generate the context of source sequence </mark> (French text).\n",
    "\n",
    "However, unlike earlier exercise, we don't use RNN (GRU) and apply the scaled dot-product attention (self-attention) instead.\n",
    "\n",
    "As you can see in below picture (as it shows with \"Nx\"), the encoder in transformer is multi-layered architecture, in which it has the repeated layers.<br>\n",
    "\n",
    "ðŸ‘‰ For this reason, we'll first implement the following repeatable single layer of component.\n",
    "\n",
    "![Encoding Layer](./images/transformer_encoding_layer.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "ðŸ’¡ **Note**: As you can see above, it adds an output of identity layer (which is written by \"Add&Norm\" in above picture) in the end of each layers. \n",
    "\n",
    "This is a known technique called **residual learning** in order to address a degradation problem of training accuracy in deep networks. (See \"[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\" (He, et al., 2015) for details.)<br>\n",
    "\n",
    "![Residual Network](./images/transformer_residual01.png)<br>\n",
    " \n",
    "Today, a lot of transformers place the layer normalization between the residual blocks as follows. (In this example, we'll create the code implementation accompanying the original paper.)<br>\n",
    "\n",
    "![Residual Network](./images/transformer_residual02.png)\n",
    "\n",
    "**Note** : I have also added a dropout for regularization in the following code.<br>\n",
    "\n",
    "For the effect of a dropout, please see [here](https://tsmatz.wordpress.com/2017/09/13/overfitting-for-regression-and-deep-learning/).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleEncodingLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = MyMultiHeadAttention(\n",
    "            embedding_dim=model_dim,\n",
    "            attention_dim=model_dim,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(model_dim, eps=0.001)\n",
    "        self.output_dense1 = nn.Linear(model_dim, hidden_dim)\n",
    "        self.output_dense2 = nn.Linear(hidden_dim, model_dim)\n",
    "\n",
    "    def forward(self, inputs, masks):\n",
    "        # apply self-attention\n",
    "        attention_outputs = self.self_attention(inputs, masks)\n",
    "        # add & layer norm (with dropout)\n",
    "        attention_outputs = F.dropout(attention_outputs, p=0.1)\n",
    "        attention_outputs = attention_outputs + inputs\n",
    "        attention_outputs = self.norm(attention_outputs)\n",
    "        # feed forward\n",
    "        linear_outputs = self.output_dense1(attention_outputs)\n",
    "        linear_outputs = F.relu(linear_outputs)\n",
    "        linear_outputs = self.output_dense2(linear_outputs)\n",
    "        # add & layer norm (with dropout)\n",
    "        linear_outputs = F.dropout(linear_outputs, p=0.1)\n",
    "        linear_outputs = linear_outputs + attention_outputs\n",
    "        linear_outputs = self.norm(linear_outputs)\n",
    "\n",
    "        return linear_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With previously generated component (```SingleEncodingLayer```), now we implement the multi-layered encoder as follows.\n",
    "\n",
    "![Encoding Layer](./images/transformer_encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, padding_idx, model_dim, num_layers, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            model_dim,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=model_dim,\n",
    "        )\n",
    "        self.encoding_layers = nn.ModuleList([\n",
    "            SingleEncodingLayer(\n",
    "                model_dim=model_dim,\n",
    "                num_heads=num_heads,\n",
    "                hidden_dim=hidden_dim,\n",
    "            )\n",
    "            for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # get mask\n",
    "        masks = (inputs != self.padding_idx).int()\n",
    "        # apply embedding\n",
    "        outputs = self.embedding(inputs)\n",
    "        # apply positional encoding\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        # apply multi-layered encoders\n",
    "        for enc_layer in self.encoding_layers:\n",
    "            outputs = enc_layer(outputs, masks)\n",
    "\n",
    "        return outputs, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Next implement decoder side.<br>\n",
    "\n",
    "Same as encoder, we'll first implement a repeatable single layer component as the following picture shows.\n",
    "\n",
    "![Decoding Layer](./images/transformer_decoding_layer.png)\n",
    "\n",
    "Unlike encoder, both self-attention and cross-attention are applied in decoder. (See above.)\n",
    "\n",
    "In the first attention, the target sequence (English) is encoded by self-attention. As I have mentioned above, causal masking is applied in this decoder's self-attention. (Set the property ```use_causal_mask=True``` in our custom attention.)<br>\n",
    "\n",
    "The next attention (cross-attention) is for machine translation. Same as [exercise 08](./08_attention.ipynb), both the encoder's outputs (contexts) for source (French) and the current decoded results for target (English) are fed into this attention, and the attended results between decoder's inputs and encoder's outputs will be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleDecodingLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = MyMultiHeadAttention(\n",
    "            embedding_dim=model_dim,\n",
    "            attention_dim=model_dim,\n",
    "            num_heads=num_heads,\n",
    "            use_causal_mask=True,\n",
    "        )\n",
    "        self.cross_attention = MyMultiHeadAttention(\n",
    "            embedding_dim=model_dim,\n",
    "            attention_dim=model_dim,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(model_dim, eps=0.001)\n",
    "        self.output_dense1 = nn.Linear(model_dim, hidden_dim)\n",
    "        self.output_dense2 = nn.Linear(hidden_dim, model_dim)\n",
    "\n",
    "    def forward(self, inputs, masks, enc_outputs, enc_masks):\n",
    "        # self-attention with causal masking\n",
    "        attention_outputs = self.self_attention(inputs, masks)\n",
    "        # add & layer norm (with dropout)\n",
    "        attention_outputs = F.dropout(attention_outputs, p=0.1)\n",
    "        attention_outputs = attention_outputs + inputs\n",
    "        attention_outputs = self.norm(attention_outputs)\n",
    "        # encoder-decoder attention\n",
    "        scored_outputs = self.cross_attention(\n",
    "            input1=attention_outputs,\n",
    "            mask1=masks,\n",
    "            input2=enc_outputs,\n",
    "            mask2=enc_masks,\n",
    "        )\n",
    "        # add & layer norm (with dropout)\n",
    "        scored_outputs = F.dropout(scored_outputs, p=0.1)\n",
    "        scored_outputs = scored_outputs + attention_outputs\n",
    "        scored_outputs = self.norm(scored_outputs)\n",
    "        # feed forward\n",
    "        linear_outputs = self.output_dense1(scored_outputs)\n",
    "        linear_outputs = F.relu(linear_outputs)\n",
    "        linear_outputs = self.output_dense2(linear_outputs)\n",
    "        # add & layer norm (with dropout)\n",
    "        linear_outputs = F.dropout(linear_outputs, p=0.1)\n",
    "        linear_outputs = linear_outputs + scored_outputs\n",
    "        linear_outputs = self.norm(linear_outputs)\n",
    "\n",
    "        return linear_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build multi-layered decoder with previous layer component (```SingleDecodingLayer```).\n",
    "\n",
    "![Decoder](./images/transformer_decoder.png)\n",
    "\n",
    "The outputs is used for predicting the next vocabulary by one-hot outputs, and the output's shape will then be ```(batch_size, sequence_length, vocabulary_size)```.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "ðŸ’¡ **Note** : In this example, the final softmax will be applied in loss computation, and here I don't explicitly implement this operation in this module. (The decoder will then output logits, not probabilities.)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, padding_idx, model_dim, num_layers, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            model_dim,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.pos_encoding = PositionalEncoding(\n",
    "            seq_len=seq_len,\n",
    "            embedding_dim=model_dim,\n",
    "        )\n",
    "        self.decoding_layers = nn.ModuleList([\n",
    "            SingleDecodingLayer(\n",
    "                model_dim=model_dim,\n",
    "                num_heads=num_heads,\n",
    "                hidden_dim=hidden_dim,\n",
    "            )\n",
    "            for _ in range(num_layers)])\n",
    "        self.output_dense = nn.Linear(model_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_inputs, enc_outputs, enc_masks):\n",
    "        # get mask\n",
    "        target_masks = (target_inputs != self.padding_idx).int()\n",
    "        # apply embedding\n",
    "        outputs = self.embedding(target_inputs)\n",
    "        # apply positional encoding\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        # apply multi-layered decoders\n",
    "        for dec_layer in self.decoding_layers:\n",
    "            outputs = dec_layer(\n",
    "                inputs=outputs,\n",
    "                masks=target_masks,\n",
    "                enc_outputs=enc_outputs,\n",
    "                enc_masks=enc_masks,\n",
    "            )\n",
    "        # apply final Linear\n",
    "        #   (batch_size, seq_len, model_dim) --> (batch_size, seq_len, vocab_size)\n",
    "        outputs = self.output_dense(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model (Transformer)\n",
    "\n",
    "Using previous models, now we build the training loop.\n",
    "\n",
    "First we set the following parameters for training.<br>\n",
    "\n",
    "In this example, the training dataset consists of single sentences (not long text) and I have reduced parameters to speed up the training, compared with the parameters which is used in the original paper.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "ðŸ’¡ **Note** : By increasing parameters, transformers will have the ability to capture more difficult contexts, but it'll need more training and corpus.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 256\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "hidden_dim = 1024\n",
    "\n",
    "### In the original paper, the following parameters are used.\n",
    "#model_dim = 512\n",
    "#num_heads = 8\n",
    "#num_layers = 6\n",
    "#hidden_dim = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, the following learning rate scheduler is used for this model, and we also apply this scheduling in this training.<br>\n",
    "\n",
    "The following function is used to modify learning rate in the training.\n",
    "\n",
    "$$ \\verb|lrate| = d^{-0.5}_{\\verb|model|} \\cdot \\min(\\verb|step_num|^{-0.5},\\verb|step_num|\\cdot\\verb|warmup_steps|^{-1.5}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step, model_dim, warmup_steps=4000):\n",
    "    step = float(step)\n",
    "    model_dim = float(model_dim)\n",
    "    if step == 0.0:\n",
    "        val1 = 0.0\n",
    "    else:\n",
    "        val1 = 1.0 / (step ** 0.5)\n",
    "    val2 = step / (warmup_steps ** 1.5)\n",
    "    return min(val1,val2) / (model_dim ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, put it all together and run training as follows.\n",
    "\n",
    "In below code, the loss on label id=-100 is ignored in ```cross_entropy()``` function. The padded position and the end of sequence will then be ignored in optimization.\n",
    "\n",
    "**Note** : Because the default value of  ```ignore_index``` property in **cross_entropy()** function is -100. (You can change this default value.)\n",
    "\n",
    "Transformer has ability to capture complex contexts, but I note that here I just simply apply training by using primitive data of single sentence in small epochs.<br>\n",
    "\n",
    "Please try more large and complex data by adjusting above parameters and the number of training epochs.\n",
    "\n",
    "You will also find that transformer is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 2.1720 - accuracy: 0.6667\n",
      "Epoch 2 - loss: 1.8233 - accuracy: 0.7147\n",
      "Epoch 3 - loss: 1.5693 - accuracy: 0.7454\n",
      "Epoch 4 - loss: 1.3702 - accuracy: 0.7500\n",
      "Epoch 5 - loss: 1.1836 - accuracy: 0.7729\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "enc_model = Encoder(\n",
    "    vocab_size=vocab_fr.__len__(),\n",
    "    seq_len=seq_len_fr,\n",
    "    padding_idx=pad_index_fr,\n",
    "    model_dim=model_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    ").to(device)\n",
    "dec_model = Decoder(\n",
    "    vocab_size=vocab_en.__len__(),\n",
    "    seq_len=seq_len_en,\n",
    "    padding_idx=pad_index_en,\n",
    "    model_dim=model_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    ").to(device)\n",
    "\n",
    "all_params = list(enc_model.parameters()) + list(dec_model.parameters())\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=all_params,\n",
    "    lr=get_lr(0, model_dim),\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    ")\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for labels, sources, targets in dataloader:\n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        enc_outputs, enc_masks = enc_model(sources)\n",
    "        logits = dec_model(targets, enc_outputs, enc_masks)\n",
    "        loss = F.cross_entropy(logits.transpose(1,2), labels)\n",
    "        loss.backward()\n",
    "        # update learning rate and step\n",
    "        lr = get_lr(step, model_dim)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        # calculate accuracy\n",
    "        pred_labels = logits.argmax(dim=2)\n",
    "        num_correct = (pred_labels == labels).float().sum()\n",
    "        num_total = (labels != -100).float().sum()\n",
    "        accuracy = num_correct / num_total\n",
    "        print(\"Epoch {} - loss: {:2.4f} - accuracy: {:2.4f}\".format(epoch+1, loss.item(), accuracy), end=\"\\r\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Text\n",
    "\n",
    "**Now translate French text to English text with trained model.** (All these sentences are not in training set.)\n",
    "\n",
    "Here we simply translate several brief sentences, but the metrics to evaluate text-generation task will not be so easy. (Because simply checking an exact match to a reference text is not optimal.)<br>\n",
    "\n",
    "To eveluate the trained model, use some common metrics available in text generation, such as, BLEU or ROUGE.\n",
    "\n",
    "**Note** : Here we use greedy search and this will sometimes lead to wrong sequence. For drawbacks and solutinos, see note in [this example](./05_language_model_basic.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "end_index_en = stoi_en[\"<end>\"]\n",
    "max_output = 128\n",
    "\n",
    "def translate(sentence):\n",
    "    # preprocess inputs\n",
    "    text_fr = sentence\n",
    "    text_fr = text_fr.lower()\n",
    "    text_fr = \" \".join([\"<start>\", text_fr, \"<end>\"])\n",
    "    text_en_list = [\"<start>\"]\n",
    "    text_en = \" \".join(text_en_list)\n",
    "    _, tokens_fr, tokens_en = collate_batch(list(zip([text_fr], [text_en])))\n",
    "\n",
    "    # process encoder\n",
    "    enc_outputs, enc_masks = enc_model(tokens_fr)\n",
    "\n",
    "    # process decoder\n",
    "    for loop in range(max_output):\n",
    "        logits = dec_model(\n",
    "            tokens_en,\n",
    "            enc_outputs,\n",
    "            enc_masks,\n",
    "        )\n",
    "        idx_en = logits[0][len(text_en_list) - 1].argmax()\n",
    "        next_word_en = itos_en[idx_en]\n",
    "        text_en_list.append(next_word_en)\n",
    "        if idx_en.item() == end_index_en:\n",
    "            break\n",
    "        text_en = \" \".join(text_en_list)\n",
    "        _, _, tokens_en = collate_batch(list(zip([text_fr], [text_en])))\n",
    "\n",
    "    return \" \".join(text_en_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i love the guitar <end>\n",
      "<start> he lives in japan <end>\n",
      "<start> this pen is used to him <end>\n",
      "<start> this is my favorite song <end>\n",
      "<start> he drives a car and go to new york <end>\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"j'aime la guitare\")) # i like guitar\n",
    "print(translate(\"il vit au japon\")) # he lives in Japan\n",
    "print(translate(\"ce stylo est utilisÃ© par lui\")) # this pen is used by him\n",
    "print(translate(\"c'est ma chanson prÃ©fÃ©rÃ©e\")) # that's my favorite song\n",
    "print(translate(\"il conduit une voiture et va Ã  new york\")) # he drives a car and goes to new york"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pretraining** involves training a transformer model on a vast amount of unlabelled data using self-supervised or unsupervised learning objectives. \n",
    "\n",
    "The goal is to enable the model to learn general language representations, which can then be fine-tuned on smaller, task-specific datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining for Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining transformers, or more generally encoder-decoder seq2seq models, can be divided into 3 categories: pretraining decoder only, encoder only, and both. First we take a look at pretraining only a decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/pretraining_decoder.PNG) <br>\n",
    "\n",
    "_Figure 2. Pretraining decoder on auxiliary task._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in Figure 2, we pretrain the word embeddings (the bottom most row of blocks) and also the network itself. The Linear layer is throwaway and used for the specific auxiliary task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generative Pretrained Transformer (GPT)__ was a Transformer decoder (no encoder!) with 12 layers, 768-dimensional hidden states, and 3072-dimensional FFN hidden layers. They used byte-pair encoding with 40,000 merges. I think what they mean by this is they were able to construct 40,000 matches of subword tokens. The model they trained (GPT1) was trained on BooksCorpus (over 7000 books).\n",
    "\n",
    "They formatted the inputs to the decoder in a __natural language inference__ format. This is where the model is fed 2 sentences: a _premise_ and a _hypothesis_. The model is then trained to predict whether the hypothesis is entailing, contradictory, or neutral to the premise sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/gpt_input.PNG) <br>\n",
    "\n",
    "_Figure 3. GPT input format._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Figure 3, they roughly formatted this task in a series of tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__GPT-2__ and __GPT-3__ are larger newer versions of the original GPT trained on even larger amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining for Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For encoders, <mark>we can't pretrain it like a normal language model (because encoders, in the Transformer, have bidirectional context)</mark>. \n",
    "\n",
    "The way we pretrain an encoder is feed in a sentence but with randomly words masked. The encoder is then tasked to predict the masked words. This is called __masked language modeling__.\n",
    "\n",
    "__Bidrectional Encoder Representations from Transformers (BERT)__ employs this paradigm. \n",
    "\n",
    "ðŸ‘‰ Instead, they replace an input word with `[MASK]` 80% of the time.\n",
    "\n",
    "ðŸ‘‰ 10% of the time it will replace the input word with a random token and 10% of the time it will leave the input word unchanged. \n",
    "\n",
    "The reasoning for the last 2 options (as opposed to just simply randomly masking words) is to force the model to learn meaningful representations instead of just focusing on finding the right word for the masked token. \n",
    "\n",
    "Another way of pretraining, <mark>introduced in BERT</mark>, was passing in 2 pieces of contiguous text. And the model is tasked with classifying whether or not the 2nd piece of text directly follows the 1st piece of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/bert_details.PNG) <br>\n",
    "_Figure 4. Details about BERT._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating in NLP is difficult**. Many NLP experts have built datasets that are particularly hard because of certain themes and understandings that must be understood to perform well on those datasets. \n",
    "\n",
    "Here are a few:\n",
    "* QQP (Quora Question Pairs)\n",
    "    * detect paraphrase questions\n",
    "* QNLI\n",
    "    * natural language inference\n",
    "* SST-2 \n",
    "    * sentiment analysis\n",
    "* CoLA\n",
    "    * corpus of linguistic acceptability; detect whether sentences are grammatically correct\n",
    "* STS-B\n",
    "    * semantic textual similarity\n",
    "* MRPC\n",
    "    * microsoft paraphrase corpus\n",
    "* RTE\n",
    "    * small natural language inference corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many BERT variants like RoBERTa, SpanBERT, DistilBERT, etc. __RoBERTa__ trains BERT for a longer period of time because they claim BERT was underfit and they also remove the next-sentence prediction task. __SpanBERT__ masks a contiguous span of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/roberta.PNG) <br>\n",
    "_Figure 5. RoBERTa compared to BERT._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general trend is just longer training on larger datasets with larger models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining for Encoder-Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model called __T5__ is trained with __span corruption__. Instead of masking a single word, we mask a span of words and replace it with a single token. So the <mark>model not only has to predict what's missing</mark>, but it also doesn't know how many subwords are missing (so it must learn this too!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-2.png](img/span_corruption.PNG) <br>\n",
    "\n",
    "_Figure 6. Span Corruption._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting property of T5 was that <mark>it could be finetuned to answer general questions by retrieving knowledge from its parameters.</mark>\n",
    "\n",
    "By pretraining, the model can learn trivia knowledge, syntax, coreference, lexical semantics/topics, sentiment, simple arithmetic, and also cultural aspects of society."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/in_context_learning.PNG) <br>\n",
    "_Figure 7. Learning without gradient steps._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large language models seem to be able to pick up patterns in the text from what I'm understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
