
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Word Vectors &amp; Dependency Parsing</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/intro.css?v=b40f3148" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-GJG3T4ZRZH"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-GJG3T4ZRZH');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-GJG3T4ZRZH');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/ai/nlp/concepts/001_traditional_nlp';</script>
    <script src="../../../../_static/subscription_overlay.js?v=2e74803e"></script>
    <script src="https://apis.google.com/js/platform.js"></script>
    <script src="../../../../_static/landing.js?v=93f722cb"></script>
    <link rel="canonical" href="https://mlguide.in/content/ai/nlp/concepts/001_traditional_nlp.html" />
    <link rel="icon" href="../../../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Embeddings" href="002_embeddings.html" />
    <link rel="prev" title="Natural Language Processing" href="../nlp_intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../../_static/MLGuide_logo_nb.png" class="logo__image only-light" alt=" - Home"/>
    <img src="../../../../_static/MLGuide_logo_nb.png" class="logo__image only-dark pst-js-only" alt=" - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Guide</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../python/python_toc.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../python/1_installation.html">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/2_syntax_and_symantics.html">Syntax &amp; Symantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/3_functions_and_modules.html">Functions &amp; Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/4_Object_Oriented.html">Object Oriented Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/5_Exceptions_Handling.html">Exceptions Handling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/6_Handling_Files.html">Handling Files</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/7_Datetime_Operations.html">Datetime Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/8_advanced.html">Advanced Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/conceptual_topics.html">Interpreter vs Compiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../statistics/statistics-101.html">Statistics</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../mathematics/mathematics_toc.html">Mathematics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../mathematics/linear-algebra_vectors.html">Vectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mathematics/linear-algebra_matrices.html">Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mathematics/dissimilarity_measures.html">Similarity measure</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../analytics/intro_analytics.html">Data analytics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../analytics/numpy/numpy_toc.html">Numpy</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/numpy/001_Python_NumPy.html">NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/numpy/Python_Numpy_Exercises_with_hints.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../analytics/pandas/pandas_toc.html">Pandas</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/pandas/001_Python_Pandas_DataFrame.html">Pandas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/pandas/002_Pandas_HowTos.html">How To's</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/pandas/003_Pandas_Exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../analytics/matplotlib/matplotlib_toc.html">Matplotlib</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/matplotlib/001_Python_Matplotlib.html">Matplotlib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/matplotlib/003_Python_Matplotlib_Exercises.html">Exercises</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Introduction_to_ml.html">Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/01_Introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/000_Data_Exploration.html">Exploratory Data Analysis</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/001_Data_Preparation.html">Data Preparation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/002_Regression.html">Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/003_Classification.html">Classfication</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/004_Clustering.html">Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/005_Evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/006_Advanced.html">K-Fold Cross Validation</a></li>


<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/007_Dimensionality_Reduction.html">Dimensionality Reduction</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../neural/neural_toc.html">Neural Networks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../neural/concepts/001_Introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neural/concepts/002_Backpropogation.html">Backpropogation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neural/concepts/003_Activations.html">Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neural/concepts/004_Optimization.html">Optimizations</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../neural/concepts/pytorch/pytorch_toc.html">Pytorch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../neural/concepts/pytorch/00_pytorch_fundamentals.html">Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../neural/concepts/pytorch/01_pytorch_workflow.html">Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../neural/concepts/pytorch/02_pytorch_classification.html">Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../neural/concepts/pytorch/03_pytorch_computer_vision.html">Computer Vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../neural/concepts/pytorch/04_pytorch_custom_datasets.html">Custom Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../neural/concepts/pytorch/06_pytorch_transfer_learning.html">Transfer Learning</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../nlp_intro.html">Natural Language Processing</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Word Vectors &amp; Dependency Parsing</a></li>
<li class="toctree-l2"><a class="reference internal" href="002_embeddings.html">Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="003_ngram_cnn.html">N Gram using CNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="004_word2vec.html">Word2Vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="005_language_model_basic.html">Neural Language Model</a></li>

<li class="toctree-l2"><a class="reference internal" href="006_language_model_rnn.html">Recurrent Neural Network (RNN)</a></li>

<li class="toctree-l2"><a class="reference internal" href="007_encoder_decoder.html">Encoder Decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="008_attention.html">Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="009_transformer.html">Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="010_llm_tasks.html">Language Modelling Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="011_appendix.html">Appendix</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../genai/introduction.html">Generative AI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../genai/prompt-engineering/intro.html">Prompt Engineering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/prompt-engineering/basic_prompting.html">Basic Prompting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/prompt-engineering/advance_prompts.html">Advanced Prompting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/prompt-engineering/prompts-applications.html">Prompts Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/prompt-engineering/prompts-adversarial.html">Prompts Adversarial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/prompt-engineering/prompts-reliability.html">Reliability</a></li>



</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../genai/langchain/intro.html">Langchain</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/langchain/01_LangChain_Fundamentals.html">Langchain Cookbook 1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/langchain/02_LangChain_Use_Cases.html">Langchain Cookbook 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/langchain/projects/project_toc.html">Projects</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../genai/RAG/intro.html">RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../genai/agents/intro.html">Agents</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../resources/blogs/blogs_toc.html">Blogs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources/papers/papers_toc.html">Research papers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources/books/books_toc.html">E-Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources/courses/courses_toc.html">Courses</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../intro_me.html">About me</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/anukchat/mlguide/main?urlpath=lab/tree/content/ai/nlp/concepts/001_traditional_nlp.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/anukchat/mlguide/blob/main/content/ai/nlp/concepts/001_traditional_nlp.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../../_sources/content/ai/nlp/concepts/001_traditional_nlp.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Word Vectors & Dependency Parsing</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-vectors">Word Vectors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#wordnet">WordNet</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">Limitations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">Word2Vec</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm">Algorithm</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skip-gram-with-negative-sampling">Skip-Gram With Negative Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#co-occurrence-vectors">Co-occurrence Vectors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Limitations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction">Dimensionality Reduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#glove">GloVe</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-word-vectors">Evaluating Word Vectors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dependency-parsing">Dependency Parsing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#syntactic-structure-constituency-and-dependency">Syntactic structure: constituency and dependency</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constituency"><strong><mark>Constituency</mark></strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dependency"><strong><mark>Dependency</mark></strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dependency-grammar-and-dependency-structure">Dependency grammar and dependency structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-dependency-parser">Building A Dependency Parser</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="word-vectors-dependency-parsing">
<h1>Word Vectors &amp; Dependency Parsing<a class="headerlink" href="#word-vectors-dependency-parsing" title="Link to this heading">#</a></h1>
<p><strong>References</strong> <a class="reference external" href="https://www.youtube.com/watch?v=rmVRLeJRkl4">Stanford CS224N</a></p>
<p>(This course is the gold mine by Professor Christopher Manning from Stanford University, that you wouldn’t want to miss)</p>
<section id="word-vectors">
<h2>Word Vectors<a class="headerlink" href="#word-vectors" title="Link to this heading">#</a></h2>
<p>If we want to solve Natural Language Processing (NLP) tasks with neural networks, we need some way to represent text as tensors.</p>
<p>Computers already represent textual characters as numbers that map to fonts on your screen using encodings such as ASCII or UTF-8.</p>
<a class="reference internal image-reference" href="../../../../_images/ascii-character-map.png"><img alt="Image showing diagram mapping a character to an ASCII and binary representation" src="../../../../_images/ascii-character-map.png" style="width: 50%;" /></a>
<p><a class="reference external" href="https://www.seobility.net/en/wiki/ASCII">Image source</a></p>
<p>As humans, we understand what each letter <strong>represents</strong>, and how all characters come together to form the words of a sentence.</p>
<p>However, <mark>computers by themselves do not have such an understanding</mark>, and neural network has to learn the meaning during training.</p>
<div class="alert alert-block alert-info">
<p><strong>But what does “meaning” mean to us?</strong></p>
<ul class="simple">
<li><p>idea that is represented by a word/phrase</p></li>
<li><p>idea that a person wants to express using words/signs</p></li>
<li><p>idea that is expressed in a work of writing or art</p></li>
</ul>
<p>We usually model the way we think of meaning as <strong>denotational semantics</strong>:</p>
</div>
<p>Therefore, we can use different approaches when representing text:</p>
<ul class="simple">
<li><p><strong>Character-level representation</strong>, when we represent text by treating each character as a number. Given that we have <em>C</em> different characters in our text corpus, the word <em>Hello</em> would be represented by 5x<em>C</em> tensor. Each letter would correspond to a tensor column in one-hot encoding.</p></li>
<li><p><strong>Word-level representation</strong>, in which we create a <strong>vocabulary</strong> of all words in our text, and then represent words using <mark>one-hot encoding</mark>. This approach is somehow better, because <strong>each letter by itself does not have much meaning</strong>, and thus by using higher-level semantic concepts - words - we simplify the task for the neural network.</p></li>
</ul>
<p><mark>Regardless of the representation, we first need to convert the text into a sequence of <strong>tokens</strong></mark>, one token being either a character, a word, or sometimes even part of a word.</p>
<div class="alert alert-block alert-info">
<p><strong>What is One-Hot Encoding?</strong></p>
<p>One-hot encoding is a representation of categorical variables as binary vectors. In other words, each categorical variable is represented by a binary vector of length equal to the number of categories.</p>
<a class="reference internal image-reference" href="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ggtP4a5YaRx6l09KQaYOnw.png"><img alt="One Hot Encoding" src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ggtP4a5YaRx6l09KQaYOnw.png" style="width: 50%;" /></a>
</div><p>In traditional NLP, we represent words as one-hot vectors. This is called a <strong>localist representation</strong>.</p>
<p>However, <mark>using one-hot vectors to encode words <strong>lacks</strong> similarity between words and context in general</mark>.</p>
<p>In the modern day, we use <strong>embedding vectors</strong> built on the idea of <strong>distributional semantics</strong> which defines a words meaning not by what idea it’s associated with, but by <mark>what context or window of words the word appears in.</mark></p>
<p><img alt="Distributional Semantics" src="../../../../_images/distributional_semantics.PNG" /> <br>
<em>Figure 1. Distributional Semantics.</em></p>
<section id="wordnet">
<h3>WordNet<a class="headerlink" href="#wordnet" title="Link to this heading">#</a></h3>
<p><strong>WordNet</strong> groups together words that are syonymous but is limited in capability.</p>
<div class="alert alert-block alert-info"> 
<p><strong>What is WordNet?</strong></p>
<p>WordNET is a lexical <strong>database</strong> of words in more than 200 languages in which we have adjectives, adverbs, nouns, and verbs grouped differently into a set of cognitive synonyms, where <strong>each word in the database is expressing its distinct concept</strong>.</p>
<p>The cognitive synonyms which are called <strong>synsets</strong> are presented in lexical and semantic relations.</p>
<p>Synonyms–words that denote the same concept and are interchangeable in many contexts–are grouped into unordered sets (synsets)</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/WordNet#:~:text=WordNet%20is%20a%20lexical%20database,of%20a%20dictionary%20and%20thesaurus">WordNet</a></p>
</div><p>WordNet can be used using nltk package in python, let’s understand using an example. Nltk stands for Natural Language Toolkit, is a python package for traditional natural language processing tasks.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>nltk
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 1 Download teh wordnet dataset</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;wordnet&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import wordnet</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">wordnet</span> <span class="k">as</span> <span class="n">wn</span>

<span class="c1"># look up a word using synsets(): a set of synonyms that share a common meaning.</span>
<span class="c1"># a synset is identified with a 3-part name of the form: word.pos.nn [Word, part of speech, number of synsets]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Synsets for the word &quot;invite&quot; in WordNet:</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">synset</span> <span class="ow">in</span> <span class="n">wn</span><span class="o">.</span><span class="n">synsets</span><span class="p">(</span><span class="s1">&#39;invite&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">synset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Synsets for the word &quot;invite&quot; in WordNet:


Synset(&#39;invite.n.01&#39;)
Synset(&#39;invite.v.01&#39;)
Synset(&#39;invite.v.02&#39;)
Synset(&#39;tempt.v.03&#39;)
Synset(&#39;invite.v.04&#39;)
Synset(&#39;invite.v.05&#39;)
Synset(&#39;invite.v.06&#39;)
Synset(&#39;invite.v.07&#39;)
Synset(&#39;receive.v.05&#39;)
</pre></div>
</div>
</div>
</div>
<p><strong>What do we interpret from this ?</strong></p>
<p>Let’s take some examples and understand:</p>
<ul class="simple">
<li><p>Synset(‘invite.n.01’)</p>
<ul>
<li><p><strong>Part of Speech</strong>: Noun (n)</p></li>
<li><p><strong>Interpretation</strong>: Represents the noun form of “invite,” likely meaning an invitation.</p></li>
</ul>
</li>
<li><p>Synset(‘invite.v.01’)</p>
<ul>
<li><p><strong>Part of Speech</strong>: Verb (v)</p></li>
<li><p><strong>Interpretation</strong>: Represents the primary verb sense of “invite,” such as to request someone’s presence.</p></li>
</ul>
</li>
<li><p>Synset(‘invite.v.02’)</p>
<ul>
<li><p><strong>Part of Speech</strong>: Verb (v)</p></li>
<li><p><strong>Interpretation</strong>: Another verb sense of “invite,” possibly with a different nuance, like enticing or attracting.</p></li>
</ul>
</li>
<li><p>.</p></li>
<li><p>.</p></li>
</ul>
<p>The word “<strong>invite</strong>” has multiple synsets, both as a <strong>noun</strong> and a <strong>verb</strong>, each capturing a different meaning or usage.</p>
<p>The inclusion of <strong>tempt.v.03</strong> and <strong>receive.v.05</strong> implies that some senses of “<strong>invite</strong>” are closely related or synonymous with “<strong>tempt</strong>” and “<strong>receive</strong>,” respectively.</p>
<p>“<strong>Invite</strong>” functions as both a noun and a verb, and WordNet categorizes its meanings accordingly.</p>
<p>We can further fetch the definition, examples and hypernyms of the specified synset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># check definition of a synset</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Definition:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The definition for invite as a noun:</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">wn</span><span class="o">.</span><span class="n">synset</span><span class="p">(</span><span class="s1">&#39;invite.n.01&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">definition</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------------------------&quot;</span><span class="p">)</span>
<span class="c1"># check the related examples</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Examples&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The definition for invite as a noun:</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">wn</span><span class="o">.</span><span class="n">synset</span><span class="p">(</span><span class="s1">&#39;invite.n.01&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">examples</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------------------------&quot;</span><span class="p">)</span>

<span class="c1"># check the hypernyms</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Hypernyms&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The hypernyms for invite as a noun:</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">wn</span><span class="o">.</span><span class="n">synset</span><span class="p">(</span><span class="s1">&#39;invite.n.01&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Definition:
The definition for invite as a noun:

 a colloquial expression for invitation
------------------------------------------
Examples
The definition for invite as a noun:

 [&quot;he didn&#39;t get no invite to the party&quot;]
------------------------------------------
Hypernyms
The hypernyms for invite as a noun:

 [Synset(&#39;invitation.n.01&#39;)]
</pre></div>
</div>
</div>
</div>
<section id="limitations">
<h4>Limitations<a class="headerlink" href="#limitations" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Great as a resource but missing nuance • e.g., “proficient” is listed as a synonym for “good” This is only correct in some contexts</p></li>
<li><p>Missing new meanings of words • e.g., wicked, badass, nifty, wizard, genius, ninja, bombest</p></li>
<li><p>Impossible to keep up-to-date!</p></li>
<li><p>Subjective</p></li>
<li><p><strong>Requires human labor to create and adapt</strong></p></li>
<li><p>Can’t compute accurate word similarity</p>
<ul>
<li><p>nltk does provide a way to compute word similarity, but it is too simplistic and subjective: synset1.path_similarity(synset2): Return a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. The score is in the range 0 to 1. See: <a class="reference external" href="http://www.nltk.org/howto/wordnet.html">http://www.nltk.org/howto/wordnet.html</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="word2vec">
<h3>Word2Vec<a class="headerlink" href="#word2vec" title="Link to this heading">#</a></h3>
<p><strong>Word2vec</strong> is a 2013 framework for <mark>learning word vectors </mark>.</p>
<p>👉 The idea is to go through a large corpus/body of text and define a vocabulary table where each word is represented by an embedding vector.</p>
<p>👉 Go through each position <span class="math notranslate nohighlight">\(t\)</span> in the text with center word <span class="math notranslate nohighlight">\(c\)</span> and context words <span class="math notranslate nohighlight">\(o\)</span> and calculate the <strong>probability of <span class="math notranslate nohighlight">\(o\)</span> given <span class="math notranslate nohighlight">\(c\)</span></strong>. We adjust the word vectors to maximize this probability.</p>
<p>👉 We Keep adjusting the word vectors to maximize this probability (backpropogation)</p>
<a class="reference internal image-reference" href="../../../../_images/1-word2vec-example.png"><img alt="Word2Vec" src="../../../../_images/1-word2vec-example.png" style="width: 600px; height: 300px;" /></a>
<p><em>Figure 2. Word2vec.</em></p>
<section id="algorithm">
<h4>Algorithm<a class="headerlink" href="#algorithm" title="Link to this heading">#</a></h4>
<p><strong>Objective function</strong></p>
<p>For each position <span class="math notranslate nohighlight">\(t = 1, ... , 𝑇\)</span>, predict context words within a window of fixed size <span class="math notranslate nohighlight">\(m\)</span>, given center word <span class="math notranslate nohighlight">\(w_j\)</span>. Data likelihood:</p>
<div class="math notranslate nohighlight">
\[\begin{split}Likelihood = L(\theta) = \prod_{t}^{T} \prod_{\substack{-m \leq j \leq m \\ j \neq m}} P(w_{t+j}|{w_{t}; \theta})\end{split}\]</div>
<p>The objective/loss/cost function for <span class="math notranslate nohighlight">\((1)\)</span> is the average negative log likelihood:</p>
<div class="math notranslate nohighlight">
\[\begin{split}J(\theta) = - \frac{1}{T} logL(\theta) = - \frac{1}{T} \sum_{t=1}^{T} \sum_{\substack{-m \leq j \leq m \\ j \neq m}} log P(w_{t+j}|{w_{t}; \theta})\end{split}\]</div>
<p><strong>Prediction function</strong></p>
<p>Denote by <span class="math notranslate nohighlight">\(v_{c}\)</span> and <span class="math notranslate nohighlight">\(v_{o}\)</span> respectively the center word and the context word, using <strong>softmax</strong>, we get the following prediction function of predicting <span class="math notranslate nohighlight">\(v_{c}\)</span> given <span class="math notranslate nohighlight">\(v_{o}\)</span> and the vocubulary <span class="math notranslate nohighlight">\(V\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(o|c) = \frac{exp(\mathbf{u_{o}^{T} v_{c}})}{\sum_{w \in V} exp(\mathbf{u_{w}^{T} v_{c}})} \tag{3}\]</div>
<br>
<br>
<a class="reference internal image-reference" href="../../../../_images/1-softmax.png"><img alt="../../../../_images/1-softmax.png" src="../../../../_images/1-softmax.png" style="width: 60%;" /></a>
<p>While <strong>Word2vec</strong> is a framework for learning embedding vectors. There are also variants of this framework.</p>
<ul class="simple">
<li><p><strong>Skip-grams (SG)</strong> predicts context words based on center word (this is word2vec)</p></li>
<li><p><strong>Continuous Bag of Words (CBOW)</strong> predicts the center word from context words</p></li>
</ul>
</section>
</section>
<section id="skip-gram-with-negative-sampling">
<h3>Skip-Gram With Negative Sampling<a class="headerlink" href="#skip-gram-with-negative-sampling" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
P(o ~|~ c) = \frac{exp(u_{o}^T v_c)}{\sum_{w \in V} exp(u_{w}^T v_c)}
\]</div>
<p>Consider the above equation in the previous lecture. It iterates through all the words in the vocabulary in the denominator. This is infeasible. So instead, we sample a <span class="math notranslate nohighlight">\(k\)</span> subset of negative pairs/samples. The rewritten objective function is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{t}(\theta) = log \sigma (u_{o}^{T} v_{c}) + \sum_{i = 1}^{k} \mathbb{E}_{j \sim P(w)} [log \sigma (-u_{j}^{T}v_{c})] \hspace{1em} (Eq.~1)\\
\end{split}\]</div>
<p>This objective function maximizes the probability that real context word appears for a given center word and minimizes the probability that random context words appear around a given center word.</p>
<p>The negative words are sampled such that less frequent words are sampled more often.</p>
</section>
<section id="co-occurrence-vectors">
<h3>Co-occurrence Vectors<a class="headerlink" href="#co-occurrence-vectors" title="Link to this heading">#</a></h3>
<p>Co-occurrence matrices are used to model the relationships between words. The matrix is a <span class="math notranslate nohighlight">\( V \times V \)</span> matrix where <span class="math notranslate nohighlight">\(V\)</span> is the number of words in the vocabulary.</p>
<p>The <span class="math notranslate nohighlight">\(i\)</span> th and <span class="math notranslate nohighlight">\(j\)</span> th entry of the matrix is the number of times that word <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> co-occur in the corpus.</p>
<a class="reference internal image-reference" href="../../../../_images/2-cooccurrence-matrix-example.png"><img alt="Co-occurrence Matrix" src="../../../../_images/2-cooccurrence-matrix-example.png" style="width: 60%;" /></a>
<p><em>Figure 1. Co-Occurrence Matrix.</em></p>
<p>This is a way of modeling the relationships between words. There are 2 ways to make a co-occurrence matrix:</p>
<ul class="simple">
<li><p>Using a sliding window over a corpus of text</p></li>
<li><p>Using the entire document</p>
<ul>
<li><p>this way is great at capturing general topics in the document (global context)</p></li>
</ul>
</li>
</ul>
<section id="id1">
<h4>Limitations<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Simple count co-occurrence vectors</p></li>
<li><p>Vectors increase in size with vocabulary</p></li>
<li><p>Very high dimensional: require a lot of storage (though sparse)</p></li>
<li><p>Subsequent classification models have sparsity issues, Models are less robust</p></li>
</ul>
<p>Let’s understand <strong>How to reduce the dimensionality</strong></p>
</section>
</section>
<section id="dimensionality-reduction">
<h3>Dimensionality Reduction<a class="headerlink" href="#dimensionality-reduction" title="Link to this heading">#</a></h3>
<p>Idea is to store “most” of the important information in a fixed, small number of dimensions: a dense vector
, Usually 25–1000 dimensions, similar to word2vec</p>
<div class="alert alert-block alert-info">
<p>Let’s first understand <strong>Single Value Decomposition</strong> briefly</p>
<p>Singular Value Decomposition is a method of decomposing a matrix into three other matrices, revealing many of the useful and intrinsic properties of the original matrix. Formally, for any real or complex <span class="math notranslate nohighlight">\(( m \times n )\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span>, SVD is given by:</p>
<div class="math notranslate nohighlight">
\[
A = U \Sigma V^T
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(( U )\)</span></strong> is an <span class="math notranslate nohighlight">\(( m \times m )\)</span> orthogonal matrix whose columns are called the left singular vectors of <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(\Sigma \)</span></strong> is an <span class="math notranslate nohighlight">\( m \times n \)</span> diagonal matrix containing the singular values of <span class="math notranslate nohighlight">\(A\)</span> in descending order.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(V^T\)</span></strong> (the transpose of <span class="math notranslate nohighlight">\(V\)</span>) is an <span class="math notranslate nohighlight">\(n \times n\)</span> orthogonal matrix whose rows are called the right singular vectors of <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
</ul>
</div>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular Value Decomposition</a></p>
<p><a class="reference internal" href="../../../../_images/2-svd.png"><img alt="../../../../_images/2-svd.png" src="../../../../_images/2-svd.png" style="width: 60%;" /></a> <br></p>
<p><em>Figure 2. Reducing the dimensionality of co-occurrence matrices via SVD.</em></p>
<p>SVD on raw counts doesn’t work well.</p>
<p>Problems:</p>
<ul class="simple">
<li><p>The dimensions of the matrix change very often (new words are added very frequently and corpus changes in size).</p></li>
<li><p>The matrix is extremely sparse since most words do not co-occur.</p></li>
<li><p>The matrix is very high dimensional in general (≈ 106 × 106)</p></li>
<li><p>Quadratic cost to train (i.e. to perform SVD)</p></li>
<li><p>Requires the incorporation of some hacks on X to account for the drastic imbalance in word frequency (see below)</p></li>
</ul>
</section>
<section id="glove">
<h3>GloVe<a class="headerlink" href="#glove" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../../../../_images/2-counts-based-versus-direct-prediction.png"><img alt="../../../../_images/2-counts-based-versus-direct-prediction.png" src="../../../../_images/2-counts-based-versus-direct-prediction.png" style="width: 60%;" /></a> <br></p>
<p><em>Figure 3. Count-based vs direct prediction.</em></p>
<p>GloVe combines the predictive modeling that is in the purple box with the methods for word similarity in the blue box. It also has an additional feature: encoding meaning components in vector differences.</p>
<p>Basic idea is <mark>Ratios of co-occurrence probabilities can encode meaning components </mark></p>
<a class="reference internal image-reference" href="../../../../_images/2-glove-basic-ideas.png"><img alt="../../../../_images/2-glove-basic-ideas.png" src="../../../../_images/2-glove-basic-ideas.png" style="width: 600px; height: 300px;" /></a>
<br>
<p><em>Figure 4. Encoding meaning in vector differences.</em></p>
<p>To create an objective function from this idea, we see that a <strong>log-bilinear model</strong> can be used to capture the ratio of co-occurrence probabilities.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
w_i ~ \cdot ~ w_j = log P(i~|~j)\\
w_x ~ \cdot (w_a - w_b) =log \frac{P(x~|~a)}{P(x~|~b)}\\
\end{split}\]</div>
<p><strong>GloVe</strong> uses the following objective function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J = \sum_{i, j = 1}^{V} f(X_{ij})(\underbrace{w_i^T \tilde{w}_j + b_i + \tilde{b}_j}_{pred} - \underbrace{log X_{ij}}_{label})^2 \hspace{1em} (Eq.~2)\\
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(w_i\)</span> and <span class="math notranslate nohighlight">\(w_j\)</span> are embedding vectors for the <span class="math notranslate nohighlight">\(i\)</span>-th and <span class="math notranslate nohighlight">\(j\)</span>-th words. They also have their corresponding bias terms. This objective function can directly optimize on the ground truth co-occurrence matrix <span class="math notranslate nohighlight">\(X\)</span>. The function <span class="math notranslate nohighlight">\(f(X_{ij})\)</span> is shown below. Its purpose is to limit the frequency of very frequent words.</p>
<a class="reference internal image-reference" href="../../../../_images/2-glove-subsampling-f.png"><img alt="../../../../_images/2-glove-subsampling-f.png" src="../../../../_images/2-glove-subsampling-f.png" style="width: 400px; height: 200px;" /></a>
<p><em>Figure 5. F function in GloVe.</em></p>
</section>
<section id="evaluating-word-vectors">
<h3>Evaluating Word Vectors<a class="headerlink" href="#evaluating-word-vectors" title="Link to this heading">#</a></h3>
<p>There are 2 ways to evaluate word vectors:</p>
<ul class="simple">
<li><p><strong>Intrinsic</strong>:</p>
<ul>
<li><p>Evaluation on a specific/intermediate subtask</p></li>
<li><p>Fast to compute</p></li>
<li><p>Helps to understand that system</p></li>
<li><p><strong>Not clear if really helpful unless correlation to real task is established</strong></p></li>
</ul>
</li>
<li><p><strong>Extrinsic</strong>:</p>
<ul>
<li><p>Evaluation on a real task</p></li>
<li><p>Can take a long time to compute accuracy</p></li>
<li><p>Unclear if the subsystem is the problem or its interaction or other subsystems</p></li>
<li><p>Unstable metrics: if replacing exactly one subsystem with another, accuracy may just be improved</p></li>
</ul>
</li>
</ul>
<p>For <strong>intrinsic word vector evaluation</strong>, you can evaluate it like: woman - man + king = queen.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
d = \underset{i}{argmax} \frac{(x_b - x_a + x_c)^T x_i}{||x_b - x_a + x_c||} \hspace{1em} (Eq.~3)\\
\end{split}\]</div>
<p>In GloVe, they found a dimension parameter of 300 was good with more data leading to better results.</p>
<p>There are shortcomings to GloVe. For one, many words have multiple meanings. Sarcasm and other things like mood and tone are harder to pick up.</p>
<p>One idea is to have multiple sensor vectors for all the definitions of a word. Though in practice it is often enough to just have 1 embedding vector per word as it is a weighted sum (or linear superposition) of all the definitions of that word. Because of <em>sparse encoding</em> (high-dimensional embedding dimension), we can actually separate out the senses.</p>
</section>
</section>
<section id="dependency-parsing">
<h2>Dependency Parsing<a class="headerlink" href="#dependency-parsing" title="Link to this heading">#</a></h2>
<p>Dependency parsing is the process of inferring the syntactic structure of a sentence.</p>
<section id="syntactic-structure-constituency-and-dependency">
<h3>Syntactic structure: constituency and dependency<a class="headerlink" href="#syntactic-structure-constituency-and-dependency" title="Link to this heading">#</a></h3>
</section>
<section id="constituency">
<h3><strong><mark>Constituency</mark></strong><a class="headerlink" href="#constituency" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Also known as <strong>phrase structure grammar</strong> or <strong>context-free grammars (CFGs)</strong></p></li>
<li><p>Key assumption: <strong>Phrase structure organizes words into nested constituents</strong></p>
<ul>
<li><p>Words (basic units, assigned with parts of speech) <span class="math notranslate nohighlight">\(\longrightarrow\)</span> phrases <span class="math notranslate nohighlight">\(\longrightarrow\)</span> bigger phrases <span class="math notranslate nohighlight">\(\longrightarrow\)</span> …</p></li>
</ul>
</li>
<li><p>Example:</p></li>
</ul>
<a class="reference internal image-reference" href="../../../../_images/5-constituency.png"><img alt="../../../../_images/5-constituency.png" src="../../../../_images/5-constituency.png" style="width: 600px; height: 300px;" /></a>
<br>
<p>Phrase structure does this by defining a <strong>lexicon</strong> and a <strong>grammar</strong>.</p>
<p><mark>The lexicon is simply the type of word (noun, preposition, verb, etc) mapped to the word (a dictionary mapping).</mark>.</p>
<p>The grammar is a list of rules that map from, say, a noun to a verb, or a verb to a preposition, etc. This is considered <strong>context-free grammar (CFG)</strong> because it doesn’t account for what phrases would be generated.</p>
<p>The other way at viewing linguistic structure is <strong>dependency structure</strong> and that is by looking at a word in a sentence and seeing what other words in the sentence modifies it.</p>
</section>
<section id="dependency">
<h3><strong><mark>Dependency</mark></strong><a class="headerlink" href="#dependency" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Dependency structure: <strong>which words depend on (modify or are arguments of) which other words</strong>; semantic relationship</p></li>
<li><p>Example (from the internet):</p></li>
</ul>
<a class="reference internal image-reference" href="../../../../_images/5-dependency-example.png"><img alt="../../../../_images/5-dependency-example.png" src="../../../../_images/5-dependency-example.png" style="width: 400px; height: 200px;" /></a>
<br>
<ul class="simple">
<li><p>Why dependency?</p>
<ul>
<li><p>We need to understand sentence structure in order to be able to interpret language correctly</p></li>
<li><p>Humans communicate complex ideas by composing words together into bigger units to convey complex meanings</p></li>
<li><p>We need to know what is connected to what</p></li>
<li><p>Otehrwise, ambiguity</p></li>
</ul>
</li>
<li><p>Dependency paths identify semantic relations</p>
<ul>
<li><p>e.g.: The result demonstrated that KaiC interacts rhythmically with SasA, KaiA, and KaiB.</p></li>
</ul>
</li>
</ul>
<a class="reference internal image-reference" href="../../../../_images/5-dependency-example2.png"><img alt="../../../../_images/5-dependency-example2.png" src="../../../../_images/5-dependency-example2.png" style="width: 400px; height: 200px;" /></a>
<p>Below are some examples of ambiguities in the English language (and thus why sentence structure is important):</p>
<ul class="simple">
<li><p><strong>Prepositional phrase attachment ambiguity</strong></p>
<ul>
<li><p><mark>San Jose cops kill man with knife</mark></p></li>
<li><p>did the cops kill a man holding a knife or did they use a knife to kill him</p></li>
</ul>
</li>
<li><p><strong>Coordination scope ambiguity</strong></p>
<ul>
<li><p><mark>Shuttle veteran and longtime NASA executive Fred Gregory appointed to board</mark></p></li>
<li><p>is this describing 1 person (Fred Gregory as the shuttle veteran and longtime NASA executive)</p></li>
<li><p>or is this describing 2 people (a shuttle veteran and Fred Gregory who is a longtime NASA executive)</p></li>
</ul>
</li>
<li><p><strong>Adjectival/adverbial modifier ambiguity</strong></p>
<ul>
<li><p><mark>Students get first hand job experience</mark></p>
<ul>
<li><p>what is this saying?</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Verb phrase attachment ambiguity</strong></p>
<ul>
<li><p><mark>Mutilated body washes up on Rio beach to be used for Olympics beach volleyball </mark></p>
<ul>
<li><p>is the mutilated body going to be used as a volleyball?</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="dependency-grammar-and-dependency-structure">
<h3>Dependency grammar and dependency structure<a class="headerlink" href="#dependency-grammar-and-dependency-structure" title="Link to this heading">#</a></h3>
<p>Dependency syntax postulates that syntactic structure consists of relations between lexical items, <strong>normally binary asymmetric relations</strong> (“arrows”) called <strong>dependencies</strong>.</p>
<p>Currently, there is no unified method to represent dependency structures graphically. For example, in the notebook, the arrows start from the head and point to the dependent. However, some may have the arrows start from the dependent and point to the head. Nevertheless, the following two methods are two basic ways to represent dependency structures in a graph.</p>
<br>
<p><strong><mark>Represented by a connected, acyclic, single-head tree</strong></mark></p>
<a class="reference internal image-reference" href="../../../../_images/5-dependency-grammar.png"><img alt="../../../../_images/5-dependency-grammar.png" src="../../../../_images/5-dependency-grammar.png" style="width: 600px; height: 300px;" /></a>
<p><strong><mark>Represented by dependency arcs (curved arrows) above a sentence</mark></strong></p>
<ul class="simple">
<li><p>Usually add a fake ROOT so every word is a dependent of precisely 1 other node</p></li>
</ul>
<a class="reference internal image-reference" href="../../../../_images/5-dependency-grammar-repr.png"><img alt="../../../../_images/5-dependency-grammar-repr.png" src="../../../../_images/5-dependency-grammar-repr.png" style="width: 600px; height: 300px;" /></a>
<br>
 <br><p>Dependency structure is usually done by drawing an arrow from the head to the dependent word or the words that modify the head. We also usually add a fake ROOT so every word is dependent on roughly 1 other node.</p>
<p>A <strong>dependency parser</strong> is basically an algorithm that will generate some quantified format of the dependency structure you see in Figure 3.</p>
<p>A <strong>tree bank</strong> is a huge collection of hand-annotated trees.</p>
<p>Usually, we start with writing a grammar or lexicon, but with tree banks, it’s a little different. Granted, treebanks are slower and are initially less useful than writing a grammar by hand. However, treebanks are reusable for many different tasks (dependency parsing, other things can be built to extract information from parsing). They can also be used to evaluate NLP systems. I’m imagining a hand-annotated treebank can be compared to different dependency parsers to see which algorithm works best.</p>
</section>
<section id="building-a-dependency-parser">
<h3>Building A Dependency Parser<a class="headerlink" href="#building-a-dependency-parser" title="Link to this heading">#</a></h3>
<p>Dependency parsers leverage 4 common themes:</p>
<ul class="simple">
<li><p>Bilexical affinities</p>
<ul>
<li><p>basically how likely is one word to be dependent on another word (it’s a pairing like discussion -&gt; issues)</p></li>
</ul>
</li>
<li><p>Dependency distance</p>
<ul>
<li><p>dependencies are usually between nearby words</p></li>
</ul>
</li>
<li><p>Intervening material</p>
<ul>
<li><p>dependencies rarely span intervening verbs or punctuation</p></li>
</ul>
</li>
<li><p>Valency of heads</p>
<ul>
<li><p>how many dependents on which side are usual for a head?</p></li>
</ul>
</li>
</ul>
<p>To perform dependency parsing, <strong>we parse through a sentence and choose for each word what other word is it a dependent of</strong>.</p>
<p>There are some constraints:</p>
<p><a class="reference internal" href="../../../../_images/5-parsing-general.png"><img alt="../../../../_images/5-parsing-general.png" src="../../../../_images/5-parsing-general.png" style="width: 600px; height: 300px;" /></a> <br></p>
<p><em>Figure 4. Crossing arcs.</em></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/ai/nlp/concepts"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../nlp_intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Natural Language Processing</p>
      </div>
    </a>
    <a class="right-next"
       href="002_embeddings.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Embeddings</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-vectors">Word Vectors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#wordnet">WordNet</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">Limitations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">Word2Vec</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm">Algorithm</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skip-gram-with-negative-sampling">Skip-Gram With Negative Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#co-occurrence-vectors">Co-occurrence Vectors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Limitations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction">Dimensionality Reduction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#glove">GloVe</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-word-vectors">Evaluating Word Vectors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dependency-parsing">Dependency Parsing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#syntactic-structure-constituency-and-dependency">Syntactic structure: constituency and dependency</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constituency"><strong><mark>Constituency</mark></strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dependency"><strong><mark>Dependency</mark></strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dependency-grammar-and-dependency-structure">Dependency grammar and dependency structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-dependency-parser">Building A Dependency Parser</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Anukool Chaturvedi
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div class="social-icons">
    <a href="https://twitter.com/chaturanukool" target="_blank"><i class="fab fa-twitter"></i></a>
    <a href="https://linkedin.com/in/anukool-chaturvedi" target="_blank"><i class="fab fa-linkedin"></i></a>
    <a href="https://github.com/anukchat" target="_blank"><i class="fab fa-github"></i></a>
    <a href="mailto:chaturvedianukool@gmail.com"><i class="fas fa-envelope"></i></a>
    </p>
</div>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>