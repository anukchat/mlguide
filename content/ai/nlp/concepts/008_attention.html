
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Attention</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/intro.css?v=b40f3148" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-GJG3T4ZRZH"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-GJG3T4ZRZH');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-GJG3T4ZRZH');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/ai/nlp/concepts/008_attention';</script>
    <link rel="canonical" href="https://mlguide.in/content/ai/nlp/concepts/008_attention.html" />
    <link rel="icon" href="../../../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Transformer" href="009_transformer.html" />
    <link rel="prev" title="Encoder-Decoder Architecture" href="007_encoder_decoder.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../../_static/MLGuide_logo_nb.png" class="logo__image only-light" alt=" - Home"/>
    <script>document.write(`<img src="../../../../_static/MLGuide_logo_nb.png" class="logo__image only-dark" alt=" - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Guide</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../python/python_toc.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../python/1_installation.html">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/2_syntax_and_symantics.html">Syntax &amp; Symantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/3_functions_and_modules.html">Functions &amp; Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/4_Object_Oriented.html">Object Oriented Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/5_Exceptions_Handling.html">Exceptions Handling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/6_Handling_Files.html">Handling Files</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/7_Datetime_Operations.html">Datetime Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/8_advanced.html">Advanced Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/conceptual_topics.html">Interpreter vs Compiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/exercises.html">Excercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../mathematics/mathematics_toc.html">Mathematics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../mathematics/linear-algebra_vectors.html">Vectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mathematics/linear-algebra_matrices.html">Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mathematics/dissimilarity_measures.html">Similarity measure</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../analytics/intro_analytics.html">Data analytics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../analytics/numpy/numpy_toc.html">Numpy</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/numpy/001_Python_NumPy.html">NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/numpy/Python_Numpy_Exercises_with_hints.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../analytics/pandas/pandas_toc.html">Pandas</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/pandas/001_Python_Pandas_DataFrame.html">Pandas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/pandas/002_Pandas_HowTos.html">How To's</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/pandas/003_Pandas_Exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../analytics/matplotlib/matplotlib_toc.html">Matplotlib</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/matplotlib/001_Python_Matplotlib.html">Matplotlib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/matplotlib/003_Python_Matplotlib_Exercises.html">Exercises</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Introduction_to_ml.html">Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/01_Introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/000_Data_Exploration.html">Exploratory Data Analysis</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/001_Data_Preparation.html">Data Preparation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/002_Regression.html">Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/003_Classification.html">Classfication</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/004_Clustering.html">Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/005_Evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/006_Advanced.html">K-Fold Cross Validation</a></li>


<li class="toctree-l2"><a class="reference internal" href="../../classicml/concepts/007_Dimensionality_Reduction.html">Dimensionality Reduction</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../neural/neural_toc.html">Neural Networks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../neural/concepts/001_Introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neural/concepts/002_Backpropogation.html">Backpropogation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neural/concepts/003_Activations.html">Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../neural/concepts/004_Optimization.html">Optimizations</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../neural/concepts/pytorch/pytorch_toc.html">Pytorch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../neural/concepts/pytorch/00_pytorch_fundamentals.html">Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../neural/concepts/pytorch/01_pytorch_workflow.html">Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../neural/concepts/pytorch/02_pytorch_classification.html">Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../neural/concepts/pytorch/03_pytorch_computer_vision.html">Computer Vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../neural/concepts/pytorch/04_pytorch_custom_datasets.html">Custom Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../neural/concepts/pytorch/06_pytorch_transfer_learning.html">Transfer Learning</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../nlp_intro.html">Natural Language Processing</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="001_traditional_nlp.html">Word Vectors &amp; Dependency Parsing</a></li>
<li class="toctree-l2"><a class="reference internal" href="002_embeddings.html">Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="003_ngram_cnn.html">N Gram using CNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="004_word2vec.html">Word2Vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="005_language_model_basic.html">Neural Language Model</a></li>

<li class="toctree-l2"><a class="reference internal" href="006_language_model_rnn.html">Recurrent Neural Network (RNN)</a></li>

<li class="toctree-l2"><a class="reference internal" href="007_encoder_decoder.html">Encoder Decoder</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="009_transformer.html">Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="010_llm_tasks.html">Language Modelling Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="011_appendix.html">Appendix</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../genai/introduction.html">Generative AI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../genai/concepts/transformers/01_transformers_from_scratch.html">Transformers</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../genai/langchain/langchain_toc.html">Langchain</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/langchain/intro.html">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/langchain/01_LangChain_Fundamentals.html">LangChain Cookbook üë®‚Äçüç≥üë©‚Äçüç≥</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../genai/langchain/02_LangChain_Use_Cases.html">LangChain Cookbook Part 2: Use Casesüë®‚Äçüç≥üë©‚Äçüç≥</a></li>

</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../resources/blogs/blogs_toc.html">Blogs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources/papers/papers_toc.html">Research papers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources/books/books_toc.html">E-Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources/courses/courses_toc.html">Courses</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../intro_me.html">About me</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/anukchat/mlguide/main?urlpath=lab/tree/content/ai/nlp/concepts/008_attention.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/anukchat/mlguide/blob/main/content/ai/nlp/concepts/008_attention.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../../_sources/content/ai/nlp/concepts/008_attention.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Attention</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#install-required-packages">Install required packages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-data">Prepare data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-sequence-inputs">Generate sequence inputs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-network">Build Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train">Train</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#translate-text">Translate Text</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="attention">
<h1>Attention<a class="headerlink" href="#attention" title="Link to this heading">#</a></h1>
<p><strong><mark>(Machine Translation Example)</mark></strong></p>
<p>In the previous example, we saw sequence-to-sequence encoder-decoder architecture in machine translation.<br></p>
<p>In the previous example, the input sequence is encoded into a single context, and this context is used for decoding in all units in generated tokens.</p>
<p>üëâ This <strong>architecture will not be flexible</strong>, and also <strong>not scalable</strong>.</p>
<p>For instance, in case of machine translation, it will be <mark>difficult to translate a long text</mark> (such as, translate multiple sentences at once) unlike human translation. (<mark>Because a single context will not be enough to represent entire text, when the text is so long.</mark>)</p>
<p><strong>By introducing attention architecture, this constraint can be relaxed.</strong><br></p>
<p>The <strong>attention</strong> is more elaborative and widely used architecture in today‚Äôs NLP, and a lot of tasks (such as, machine translation, smart reply, etc) are researched by adding attention mechanism and worked well today.</p>
<p><strong>The overview outline of attention architecture is shown as follows.<br></strong></p>
<p>In this network, the context <code class="docutils literal notranslate"><span class="pre">c</span></code> is computed and obtained in attention layer (<code class="docutils literal notranslate"><span class="pre">attend</span></code> in the following diagram) on decoder, and the different context is then used in each units in the sequence for decoding. (In the following diagram, each <code class="docutils literal notranslate"><span class="pre">attend</span></code> layer is the same network and then shares the weight‚Äôs parameters.)</p>
<p><img alt="encoder-decoder with attention architecture" src="../../../../_images/encoder_decoder_attention.png" /></p>
<p>üëâ Within attention layer, it uses <strong>previous state</strong> and encoder‚Äôs outputs (<strong>not only final output, but outputs in all units</strong>), and it generates <span class="math notranslate nohighlight">\(\{ \alpha_j^i \}\;(i=1,\ldots,n)\)</span>, in which <span class="math notranslate nohighlight">\(\sum_i \alpha_j^i = 1\)</span>, with dense net (FCNet) and softmax activation, where <span class="math notranslate nohighlight">\(n\)</span> is the number of encoder‚Äôs outputs and <span class="math notranslate nohighlight">\(j\)</span> is time step in sequence. (See the following diagram.)<br></p>
<p>üëâ To say in abstraction, <span class="math notranslate nohighlight">\(\{ \alpha_j^i \}\;(i=1,\ldots,n)\)</span> means an alignment‚Äôs weight at j-th time step for each source sequence outputs, <span class="math notranslate nohighlight">\(o_1^{\prime}, o_2^{\prime}, \ldots, o_n^{\prime}\)</span>.<br></p>
<p>(This <span class="math notranslate nohighlight">\(\{ \alpha_j^i \}\;(i=1,\ldots,n)\)</span> is then called <strong>attention weights</strong>.)</p>
<blockquote>
<div><p>Note : The softmax function is often used for normalizing outputs (sum to one) in neural networks. See <a class="reference external" href="https://tsmatz.wordpress.com/2017/08/30/regression-in-machine-learning-math-for-beginners/">here</a> for softmax function.</p>
</div></blockquote>
<p>And it finally generates context <span class="math notranslate nohighlight">\(c_j\)</span> at j-th time step by <span class="math notranslate nohighlight">\( c_j = \sum_i^n \alpha_j^i \cdot o_i^{\prime} \)</span>.</p>
<p><img alt="soft attention architecture" src="../../../../_images/soft_attention.png" /></p>
<div class="alert alert-info">
<p>üí° <strong>Note</strong> : This architecture is called <strong>soft attention</strong>, which is the first attention introduced in the context of sequence-to-sequence generation. (See Bahdanau et al.)<br></p>
<p>There exist a lot of variants in attention architecture. See the <strong>Transformers</strong> section for famous scaled dot-product attention (and self-attention) in transformer.</p>
</div>
<p>With this network, it can focus on specific components in source sequence.<br></p>
<p>For instance, in case of the following <strong>French-to-English machine translation</strong>, the 3rd units in sequence (‚Äúdon‚Äôt‚Äù in English) <strong>will strongly focus on 3rd and 5th components</strong> in original sequence (French), because the <strong>word ‚Äúdon‚Äôt‚Äù will be strongly related to ‚Äúne‚Äù and ‚Äúpas‚Äù in French.</strong></p>
<p>On the other hand, the components <strong>‚Äúje‚Äù and ‚Äúcomprends‚Äù in French are <strong>weakly</strong> referred</strong>, because it‚Äôs not directly related to ‚Äúdon‚Äôt‚Äù in English, but it‚Äôs used only for determining not ‚Äúdoesn‚Äôt‚Äù or not ‚Äúisn‚Äôt‚Äù.<br>
As a result, the attention weights <span class="math notranslate nohighlight">\(\{ \alpha_j^i \}\;(i=1,\ldots,n)\)</span> will be larger for the source components ‚Äúne‚Äù and ‚Äúpas‚Äù, and will be smaller for the source components ‚Äúje‚Äù and ‚Äúcomprends‚Äù.</p>
<p><img alt="attend in machine translation" src="../../../../_images/attend_image.png" /></p>
<section id="install-required-packages">
<h2>Install required packages<a class="headerlink" href="#install-required-packages" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Note : Currently torch 1.13.1 for cuda 11.4 has bugs (in which we can‚Äôt run <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> with <code class="docutils literal notranslate"><span class="pre">out_features=1</span></code>) and we then use cuda 11.8 here.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">torch</span><span class="o">==</span><span class="m">2</span>.3.0<span class="w"> </span><span class="nv">torchtext</span><span class="o">==</span><span class="m">0</span>.18.0<span class="w"> </span>--extra-index-url<span class="w"> </span>https://download.pytorch.org/whl/cu118
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>numpy<span class="w"> </span>nltk
</pre></div>
</div>
</div>
</div>
</section>
<section id="prepare-data">
<h2>Prepare data<a class="headerlink" href="#prepare-data" title="Link to this heading">#</a></h2>
<p>In this example, we will use Engligh-French dataset by <a class="reference external" href="https://www.manythings.org/anki/">Anki</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>wget<span class="w"> </span>http://www.manythings.org/anki/fra-eng.zip
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--2023-02-17 13:37:35--  http://www.manythings.org/anki/fra-eng.zip
Resolving www.manythings.org (www.manythings.org)... 173.254.30.110
Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 6720195 (6.4M) [application/zip]
Saving to: ‚Äòfra-eng.zip‚Äô

fra-eng.zip         100%[===================&gt;]   6.41M   568KB/s    in 15s     

2023-02-17 13:37:51 (429 KB/s) - ‚Äòfra-eng.zip‚Äô saved [6720195/6720195]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>unzip<span class="w"> </span>fra-eng.zip<span class="w"> </span>-d<span class="w"> </span>fra-eng
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Archive:  fra-eng.zip
  inflating: fra-eng/_about.txt      
  inflating: fra-eng/fra.txt         
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>head<span class="w"> </span>-n<span class="w"> </span><span class="m">5</span><span class="w"> </span>fra-eng/fra.txt
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Go.	Va !	CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) &amp; #1158250 (Wittydev)
Go.	Marche.	CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) &amp; #8090732 (Micsmithel)
Go.	En route !	CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) &amp; #8267435 (felix63)
Go.	Bouge !	CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) &amp; #9022935 (Micsmithel)
Hi.	Salut !	CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) &amp; #509819 (Aiji)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>wc<span class="w"> </span>-l<span class="w"> </span>fra-eng/fra.txt
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>197463 fra-eng/fra.txt
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">pathobj</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;fra-eng/fra.txt&quot;</span><span class="p">)</span>
<span class="n">text_all</span> <span class="o">=</span> <span class="n">pathobj</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span><span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">text_all</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_data</span><span class="p">)[:,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]]</span>
<span class="c1"># print first row</span>
<span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Va !&#39;, &#39;Go.&#39;], dtype=&#39;&lt;U349&#39;)
</pre></div>
</div>
</div>
</div>
<p>In this training set, text length in the latter part is longer (and includes multiple sentences) than the former part.<br></p>
<p>Therefore we will shuffle entire data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Dis toujours la v√©rit√©.&#39;, &#39;Always tell the truth.&#39;], dtype=&#39;&lt;U349&#39;)
</pre></div>
</div>
</div>
</div>
<p>When data consists of multiple sentences, it converts to a single sentence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">nltk.data</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;punkt&quot;</span><span class="p">)</span>
<span class="n">tokenizer_en</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;tokenizers/punkt/english.pickle&quot;</span><span class="p">)</span>
<span class="n">tokenizer_fr</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;tokenizers/punkt/french.pickle&quot;</span><span class="p">)</span>
<span class="n">fr_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">en_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">tokenizer_fr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">tokenizer_en</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">):</span>
        <span class="n">fr_list</span> <span class="o">+=</span> <span class="n">x1</span>
        <span class="n">en_list</span> <span class="o">+=</span> <span class="n">x2</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">fr_list</span><span class="p">,</span> <span class="n">en_list</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package punkt to /home/tsmatz/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
</pre></div>
</div>
</div>
</div>
<p>To get the better performance (accuracy), I standarize the input text as follows.</p>
<ul class="simple">
<li><p>Make all words to lowercase in order to reduce words</p></li>
<li><p>Make ‚Äú-‚Äù (hyphen) to space</p></li>
<li><p>Remove all punctuation except ‚Äú ‚Äô ‚Äú (e.g, Ken‚Äôs bag, ces‚Äôt, ‚Ä¶)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">string</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">char</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">char</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&#39;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span>
    <span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">char</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="s2">&quot;¬´¬ª&quot;</span><span class="p">:</span>
    <span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">char</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">char</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="c1"># print first row</span>
<span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;dis toujours la v√©rit√©&#39;, &#39;always tell the truth&#39;], dtype=&#39;&lt;U250&#39;)
</pre></div>
</div>
</div>
</div>
<p>Add <code class="docutils literal notranslate"><span class="pre">&lt;start&gt;</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;end&gt;</span></code> tokens in string.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s2">&quot;&lt;start&gt;&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s2">&quot;&lt;end&gt;&quot;</span><span class="p">]),</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s2">&quot;&lt;start&gt;&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;&lt;end&gt;&quot;</span><span class="p">])]</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">])</span>
<span class="c1"># print first row</span>
<span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;&lt;start&gt; dis toujours la v√©rit√© &lt;end&gt;&#39;,
       &#39;&lt;start&gt; always tell the truth &lt;end&gt;&#39;], dtype=&#39;&lt;U264&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="generate-sequence-inputs">
<h2>Generate sequence inputs<a class="headerlink" href="#generate-sequence-inputs" title="Link to this heading">#</a></h2>
<p>We will generate the sequence of word‚Äôs indices (i.e, tokenize) from text.</p>
<p><img alt="Index vectorize" src="../../../../_images/index_vectorize2.png" /></p>
<p>First we create a list of vocabulary (<code class="docutils literal notranslate"><span class="pre">vocab</span></code>) for both source text (French) and target text (English) respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchtext.data.utils</span> <span class="kn">import</span> <span class="n">get_tokenizer</span>
<span class="kn">from</span> <span class="nn">torchtext.vocab</span> <span class="kn">import</span> <span class="n">build_vocab_from_iterator</span>

<span class="n">max_word</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="c1"># create space-split tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_tokenizer</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># define tokenization function</span>
<span class="k">def</span> <span class="nf">yield_tokens</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">tokens</span>

<span class="c1"># build vocabulary list for French</span>
<span class="n">vocab_fr</span> <span class="o">=</span> <span class="n">build_vocab_from_iterator</span><span class="p">(</span>
    <span class="n">yield_tokens</span><span class="p">(</span><span class="n">train_data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span>
    <span class="n">specials</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">],</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_word</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">vocab_fr</span><span class="o">.</span><span class="n">set_default_index</span><span class="p">(</span><span class="n">vocab_fr</span><span class="p">[</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">])</span>

<span class="c1"># build vocabulary list for English</span>
<span class="n">vocab_en</span> <span class="o">=</span> <span class="n">build_vocab_from_iterator</span><span class="p">(</span>
    <span class="n">yield_tokens</span><span class="p">(</span><span class="n">train_data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span>
    <span class="n">specials</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">],</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_word</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">vocab_en</span><span class="o">.</span><span class="n">set_default_index</span><span class="p">(</span><span class="n">vocab_en</span><span class="p">[</span><span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>The generated token index is <code class="docutils literal notranslate"><span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">...</span> <span class="pre">,</span> <span class="pre">vocab_size</span> <span class="pre">-</span> <span class="pre">1</span></code>.<br></p>
<p>Now I set <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> as a token id in padded positions for both French and English respctively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pad_index_fr</span> <span class="o">=</span> <span class="n">vocab_fr</span><span class="o">.</span><span class="fm">__len__</span><span class="p">()</span>
<span class="n">vocab_fr</span><span class="o">.</span><span class="n">append_token</span><span class="p">(</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">)</span>

<span class="n">pad_index_en</span> <span class="o">=</span> <span class="n">vocab_en</span><span class="o">.</span><span class="fm">__len__</span><span class="p">()</span>
<span class="n">vocab_en</span><span class="o">.</span><span class="n">append_token</span><span class="p">(</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Get list for both index-to-word and word-to-index.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">itos_fr</span> <span class="o">=</span> <span class="n">vocab_fr</span><span class="o">.</span><span class="n">get_itos</span><span class="p">()</span>
<span class="n">stoi_fr</span> <span class="o">=</span> <span class="n">vocab_fr</span><span class="o">.</span><span class="n">get_stoi</span><span class="p">()</span>

<span class="n">itos_en</span> <span class="o">=</span> <span class="n">vocab_en</span><span class="o">.</span><span class="n">get_itos</span><span class="p">()</span>
<span class="n">stoi_en</span> <span class="o">=</span> <span class="n">vocab_en</span><span class="o">.</span><span class="n">get_stoi</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># test</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The number of token index in French (source) is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">vocab_fr</span><span class="o">.</span><span class="fm">__len__</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The padded index in French (source) is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">stoi_fr</span><span class="p">[</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The number of token index in English (target) is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">vocab_en</span><span class="o">.</span><span class="fm">__len__</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The padded index in English (target) is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">stoi_en</span><span class="p">[</span><span class="s2">&quot;&lt;pad&gt;&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The number of token index in French (source) is 10001.
The padded index in French (source) is 10000.
The number of token index in English (target) is 10001.
The padded index in English (target) is 10000.
</pre></div>
</div>
</div>
</div>
<p>Now we build a collator function, which is used for pre-processing in data loader.</p>
<p>In this collator,</p>
<p>üëâ First we create a list of word‚Äôs indices for source (French) and target (English) respectively as follows.</p>
<p><code class="docutils literal notranslate"><span class="pre">&lt;start&gt;</span> <span class="pre">this</span> <span class="pre">is</span> <span class="pre">pen</span> <span class="pre">&lt;end&gt;</span></code> ‚Äì&gt; <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">7,</span> <span class="pre">5,</span> <span class="pre">14,</span> <span class="pre">1]</span></code></p>
<p>üëâ For target (English) sequence, we separate into features (x) and labels (y).<br>
In this task, we predict the next word in target (English) sequence using the current word‚Äôs sequence (English) and the encoded context of source (French).<br>
We then separate target sequence into the sequence iteself (x) and the following label (y).</p>
<p><u>before</u> :</p>
<p><code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">7,</span> <span class="pre">5,</span> <span class="pre">14,</span> <span class="pre">1]</span></code></p>
<p><u>after</u> :</p>
<p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">:</span> <span class="pre">[2,</span> <span class="pre">7,</span> <span class="pre">5,</span> <span class="pre">14,</span> <span class="pre">1]</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">:</span> <span class="pre">[7,</span> <span class="pre">5,</span> <span class="pre">14,</span> <span class="pre">1,</span> <span class="pre">-100]</span></code></p>
<div class="alert alert-info alert-block">
<p>üí° <strong>Note</strong> : Here I set -100 as an unknown label id, because PyTorch cross-entropy function (<strong>torch.nn.functional.cross_entropy()</strong>) has a property <code class="docutils literal notranslate"><span class="pre">ignore_index</span></code> which default value is -100.</p>
</div>
<p>üëâ Finally we pad the inputs (for both source and target) as follows.<br>
The padded index in features is <code class="docutils literal notranslate"><span class="pre">pad_index</span></code> and the padded index in label is -100. (See above note.)</p>
<p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">:</span> <span class="pre">[2,</span> <span class="pre">7,</span> <span class="pre">5,</span> <span class="pre">14,</span> <span class="pre">1,</span> <span class="pre">N,</span> <span class="pre">...</span> <span class="pre">,</span> <span class="pre">N]</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">:</span> <span class="pre">[7,</span> <span class="pre">5,</span> <span class="pre">14,</span> <span class="pre">1,</span> <span class="pre">-100,</span> <span class="pre">-100,</span> <span class="pre">...</span> <span class="pre">,</span> <span class="pre">-100]</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">seq_len_fr</span> <span class="o">=</span> <span class="mi">45</span>
<span class="n">seq_len_en</span> <span class="o">=</span> <span class="mi">38</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">collate_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">label_list</span><span class="p">,</span> <span class="n">feature_source_list</span><span class="p">,</span> <span class="n">feature_target_list</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">text_fr</span><span class="p">,</span> <span class="n">text_en</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
        <span class="c1"># (1) tokenize to a list of word&#39;s indices</span>
        <span class="n">tokens_fr</span> <span class="o">=</span> <span class="n">vocab_fr</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text_fr</span><span class="p">))</span>
        <span class="n">tokens_en</span> <span class="o">=</span> <span class="n">vocab_en</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text_en</span><span class="p">))</span>
        <span class="c1"># (2) separate into features and labels in target tokens (English)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">tokens_en</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">)</span>
        <span class="c1"># (3) limit length to seq_len and pad sequence</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">seq_len_en</span><span class="p">]</span>
        <span class="n">tokens_fr</span> <span class="o">=</span> <span class="n">tokens_fr</span><span class="p">[:</span><span class="n">seq_len_fr</span><span class="p">]</span>
        <span class="n">tokens_en</span> <span class="o">=</span> <span class="n">tokens_en</span><span class="p">[:</span><span class="n">seq_len_en</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">seq_len_en</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="n">tokens_fr</span> <span class="o">+=</span> <span class="p">[</span><span class="n">pad_index_fr</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">seq_len_fr</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens_fr</span><span class="p">))</span>
        <span class="n">tokens_en</span> <span class="o">+=</span> <span class="p">[</span><span class="n">pad_index_en</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">seq_len_en</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens_en</span><span class="p">))</span>
        <span class="c1"># add to list</span>
        <span class="n">label_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">feature_source_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens_fr</span><span class="p">)</span>
        <span class="n">feature_target_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens_en</span><span class="p">)</span>
    <span class="c1"># convert to tensor</span>
    <span class="n">label_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">label_list</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">feature_source_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">feature_source_list</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">feature_target_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">feature_target_list</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">label_list</span><span class="p">,</span> <span class="n">feature_source_list</span><span class="p">,</span> <span class="n">feature_target_list</span>

<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">train_data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">train_data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_batch</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># test</span>
<span class="k">for</span> <span class="n">labels</span><span class="p">,</span> <span class="n">sources</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="k">break</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;label shape in batch : </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;feature source shape in batch : </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sources</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;feature target shape in batch : </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;***** label sample *****&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;***** features (source) sample *****&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sources</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;***** features (target) sample *****&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>label shape in batch : torch.Size([64, 38])
feature source shape in batch : torch.Size([64, 45])
feature target shape in batch : torch.Size([64, 38])
***** label sample *****
tensor([   3,  450,  112,    1, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
        -100, -100], device=&#39;cuda:0&#39;)
***** features (source) sample *****
tensor([    2,    23,   624,    11,   103,     1, 10000, 10000, 10000, 10000,
        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,
        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,
        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,
        10000, 10000, 10000, 10000, 10000], device=&#39;cuda:0&#39;)
***** features (target) sample *****
tensor([    2,     3,   450,   112,     1, 10000, 10000, 10000, 10000, 10000,
        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,
        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,
        10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000],
       device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="build-network">
<h2>Build Network<a class="headerlink" href="#build-network" title="Link to this heading">#</a></h2>
<p>Now we build an attention model in encoder-decoder architecture as follows.</p>
<ul class="simple">
<li><p>Outputs (not only final output, but all outputs in all units) in RNN for source French text are generated in encoder.</p></li>
<li><p>Encoder‚Äôs outputs are used in attention architecture and the result is passed into unit in decoder‚Äôs RNN.</p></li>
<li><p>Each RNN output in decoder is passed into dense (FCNet) layer and generate the sequence of next words.</p></li>
<li><p>Calculate loss between predicted next words and the true values of next words, and then proceed to optimize neural networks.</p></li>
</ul>
<p><img alt="the trainer architecture of machine translation" src="../../../../_images/machine_translation2.png" /></p>
<p>First, we will build encoder model.<br></p>
<p>See the <a class="reference internal" href="#./06_language_model_rnn.ipynb"><span class="xref myst">previous examples</span></a> for details about RNN inputs and outputs in PyTorch. (Here I also use packed sequence, because I want to process appropriate time-steps in each sequence.)</p>
<p>Unlike <a class="reference internal" href="#./07_encoder_decoder.ipynb"><span class="xref myst">previous example</span></a> (vanilla encoder-decoder example), all outputs in all units are used in the decoder, and the encoder should then return all outputs (not only the final output).</p>
<p><img alt="all outputs in encoder" src="../../../../_images/encoder_all.png" /></p>
<p>Note that the size of the following <code class="docutils literal notranslate"><span class="pre">masks</span></code> output is <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len)</span></code>, in which its element‚Äôs value is 0 when it‚Äôs a padded position, and otherwise 1. (This <code class="docutils literal notranslate"><span class="pre">masks</span></code> will then be used in the following softmax operation in decoder side.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">rnn_units</span> <span class="o">=</span> <span class="mi">1024</span>

<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">rnn_units</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
            <span class="n">vocab_size</span><span class="p">,</span>
            <span class="n">embedding_dim</span><span class="p">,</span>
            <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">rnn_units</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># embedding</span>
        <span class="c1">#   --&gt; (batch_size, seq_len, embedding_dim)</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="c1"># build &quot;lengths&quot; property to pack inputs (see previous example)</span>
        <span class="n">masks</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">masks</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># pack inputs for RNN (see previous example)</span>
        <span class="n">packed_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pack_padded_sequence</span><span class="p">(</span>
            <span class="n">outs</span><span class="p">,</span>
            <span class="n">lengths</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">enforce_sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># apply RNN</span>
        <span class="n">packed_outs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">packed_inputs</span><span class="p">)</span>
        <span class="c1"># unpack results</span>
        <span class="c1">#   --&gt; (batch_size, seq_len, rnn_units)</span>
        <span class="n">outs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_packed_sequence</span><span class="p">(</span>
            <span class="n">packed_outs</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">padding_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
            <span class="n">total_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">outs</span><span class="p">,</span> <span class="n">masks</span>

<span class="n">enc_model</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_fr</span><span class="o">.</span><span class="fm">__len__</span><span class="p">(),</span>
    <span class="n">seq_len</span><span class="o">=</span><span class="n">seq_len_fr</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
    <span class="n">rnn_units</span><span class="o">=</span><span class="n">rnn_units</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=</span><span class="n">pad_index_fr</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Now we build decoder with attention architecture as follows.</strong></p>
<p><img alt="decoder with attention" src="../../../../_images/decoder_attention.png" /></p>
<p>üëâ In each time-steps in target sequence, the state in previous step is used in computation of attention layer, and it repeats this process until the end of sequence. (See the following <code class="docutils literal notranslate"><span class="pre">for</span></code> loop.)</p>
<p>üëâ In each steps, first, the previous state and encoder‚Äôs outputs are concatenated, and the results are passed into dense network (FCNet).<br></p>
<p>üëâ By applying softmax function for this output, the attention weights <span class="math notranslate nohighlight">\(\alpha\)</span> (<code class="docutils literal notranslate"><span class="pre">alpha</span></code> in the following code) at <code class="docutils literal notranslate"><span class="pre">j</span></code>-th step are obtained. The context <code class="docutils literal notranslate"><span class="pre">c</span></code> at <code class="docutils literal notranslate"><span class="pre">j</span></code>-th step is then generated by <span class="math notranslate nohighlight">\(\sum_i \alpha_i o_i^{\prime}\)</span> where <span class="math notranslate nohighlight">\(o_i^{\prime}\)</span> is i-th element in encoder‚Äôs outputs.<br></p>
<p>In the following code, the padded elements in the softmax operation will be ignored (masked), because <span class="math notranslate nohighlight">\(e^{-inf} = 0\)</span>.</p>
<p>üëâ Once we get the context <code class="docutils literal notranslate"><span class="pre">c</span></code>, the subsequent steps are the same as <a class="reference internal" href="#./07_encoder_decoder.ipynb"><span class="xref myst">previous example</span></a>. (See previous example for details.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">DecoderWithAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">rnn_units</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">,</span> <span class="n">hidden_dim1</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">hidden_dim2</span><span class="o">=</span><span class="mi">1024</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_units</span> <span class="o">=</span> <span class="n">rnn_units</span>

        <span class="c1"># Below are used in attention layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dense1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">rnn_units</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dense2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Below are used in other parts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
            <span class="n">vocab_size</span><span class="p">,</span>
            <span class="n">embedding_dim</span><span class="p">,</span>
            <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnncell</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRUCell</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="n">rnn_units</span> <span class="o">+</span> <span class="n">embedding_dim</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">rnn_units</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dense1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">rnn_units</span><span class="p">,</span> <span class="n">hidden_dim2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dense2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim2</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_masks</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_states</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1">#</span>
        <span class="c1"># get size</span>
        <span class="c1">#</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dec_seq_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">enc_seq_size</span> <span class="o">=</span> <span class="n">enc_outputs</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1">#</span>
        <span class="c1"># set initial states</span>
        <span class="c1">#</span>

        <span class="k">if</span> <span class="n">states</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">current_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_units</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current_states</span> <span class="o">=</span> <span class="n">states</span>

        <span class="c1"># loop target sequence</span>
        <span class="c1">#   [Note] Here I loop in all time-steps, but please filter</span>
        <span class="c1">#   for saving resource&#39;s consumption.</span>
        <span class="c1">#   (Sort batch, run by filtering, and turn into original position.)</span>
        <span class="n">rnn_outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dec_seq_size</span><span class="p">):</span>

            <span class="c1">#</span>
            <span class="c1"># process attention</span>
            <span class="c1">#</span>

            <span class="c1">#   --&gt; (batch_size, 1, rnn_units)</span>
            <span class="n">current_states_reshaped</span> <span class="o">=</span> <span class="n">current_states</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1">#   --&gt; (batch_size, enc_seq_size, rnn_units)</span>
            <span class="n">current_states_reshaped</span> <span class="o">=</span> <span class="n">current_states_reshaped</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">enc_seq_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># concat</span>
            <span class="c1">#   --&gt; (batch_size, enc_seq_size, rnn_units * 2)</span>
            <span class="n">enc_and_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">current_states_reshaped</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># apply dense</span>
            <span class="c1">#   --&gt; (batch_size, enc_seq_size, 1)</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_dense1</span><span class="p">(</span><span class="n">enc_and_states</span><span class="p">)</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_dense2</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
            <span class="c1">#   --&gt; (batch_size, enc_seq_size)</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="c1"># apply masked softmax</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">enc_masks</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># get context</span>
            <span class="c1">#   --&gt; (batch_size, rnn_units)</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bs,bsu-&gt;bu&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">)</span>

            <span class="c1">#</span>
            <span class="c1"># process rnn</span>
            <span class="c1">#</span>

            <span class="c1"># embedding</span>
            <span class="c1">#   --&gt; (batch_size, embedding_dim)</span>
            <span class="n">emb_j</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:,</span><span class="n">j</span><span class="p">])</span>
            <span class="c1"># concat</span>
            <span class="c1">#   --&gt; (batch_size, rnn_units + embedding_dim)</span>
            <span class="n">input_j</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">c</span><span class="p">,</span> <span class="n">emb_j</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># apply rnn (proceed to the next state)</span>
            <span class="n">current_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnncell</span><span class="p">(</span><span class="n">input_j</span><span class="p">,</span> <span class="n">current_states</span><span class="p">)</span>
            <span class="c1"># append state</span>
            <span class="n">rnn_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_states</span><span class="p">)</span>

        <span class="c1">#</span>
        <span class="c1"># process outputs</span>
        <span class="c1">#</span>

        <span class="c1"># get output state&#39;s tensor</span>
        <span class="c1">#   --&gt; (batch_size, dec_seq_size, rnn_units)</span>
        <span class="n">rnn_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># apply dense</span>
        <span class="c1">#   --&gt; (batch_size, dec_seq_size, vocab_size)</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dense1</span><span class="p">(</span><span class="n">rnn_outputs</span><span class="p">)</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dense2</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>

        <span class="c1"># return results</span>
        <span class="k">if</span> <span class="n">return_states</span><span class="p">:</span>
            <span class="c1"># set 0.0 in padded position</span>
            <span class="n">masks</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
            <span class="n">masks</span> <span class="o">=</span> <span class="n">masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">masks</span> <span class="o">=</span> <span class="n">masks</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_units</span><span class="p">)</span>
            <span class="n">rnn_outputs</span> <span class="o">=</span> <span class="n">rnn_outputs</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">masks</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">rnn_outputs</span>  <span class="c1"># This is used in prediction</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">logits</span>               <span class="c1"># This is used in training</span>

<span class="n">dec_model</span> <span class="o">=</span> <span class="n">DecoderWithAttention</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_en</span><span class="o">.</span><span class="fm">__len__</span><span class="p">(),</span>
    <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
    <span class="n">rnn_units</span><span class="o">=</span><span class="n">rnn_units</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="o">=</span><span class="n">pad_index_en</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train">
<h2>Train<a class="headerlink" href="#train" title="Link to this heading">#</a></h2>
<p>Now we put it all together and run training.</p>
<p>The loss on label id=-100 is ignored in <code class="docutils literal notranslate"><span class="pre">cross_entropy()</span></code> function. The padded position and the end of sequence will then be ignored in optimization.</p>
<blockquote>
<div><p>Note : Because the default value of  <code class="docutils literal notranslate"><span class="pre">ignore_index</span></code> property in <code class="docutils literal notranslate"><span class="pre">cross_entropy()</span></code> function is -100. (You can change this default value.)</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">all_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">enc_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">dec_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">all_params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">labels</span><span class="p">,</span> <span class="n">sources</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="c1"># optimize</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_masks</span> <span class="o">=</span> <span class="n">enc_model</span><span class="p">(</span><span class="n">sources</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">dec_model</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_masks</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># calculate accuracy</span>
        <span class="n">pred_labels</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">num_correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_labels</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">num_total</span> <span class="o">=</span> <span class="p">(</span><span class="n">labels</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">num_correct</span> <span class="o">/</span> <span class="n">num_total</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">{}</span><span class="s2"> - loss: </span><span class="si">{:2.4f}</span><span class="s2"> - accuracy: </span><span class="si">{:2.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">accuracy</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\r</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1 - loss: 1.8736 - accuracy: 0.6900
Epoch 2 - loss: 1.8001 - accuracy: 0.6667
Epoch 3 - loss: 0.7754 - accuracy: 0.8667
Epoch 4 - loss: 0.1816 - accuracy: 0.9286
Epoch 5 - loss: 0.7531 - accuracy: 0.8333
</pre></div>
</div>
</div>
</div>
</section>
<section id="translate-text">
<h2>Translate Text<a class="headerlink" href="#translate-text" title="Link to this heading">#</a></h2>
<p><mark>Now translate French text to English text with trained model</mark>. (All these sentences are not in training set.)</p>
<p>Here I simply translate several brief sentences, but the metrics to evaluate text-generation task will not be so easy. (Because simply checking an exact match to a reference text is not optimal.)<br></p>
<p>To eveluate the trained model, use some common metrics available in text generation, such as, BLEU or ROUGE.</p>
<blockquote>
<div><p>Note : Here I use greedy search and this will sometimes lead to wrong sequence. For drawbacks and solutions, see note in <a class="reference internal" href="#./05_language_model_basic.ipynb"><span class="xref myst">this example</span></a>.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">end_index_en</span> <span class="o">=</span> <span class="n">stoi_en</span><span class="p">[</span><span class="s2">&quot;&lt;end&gt;&quot;</span><span class="p">]</span>
<span class="n">max_output</span> <span class="o">=</span> <span class="mi">128</span>

<span class="k">def</span> <span class="nf">translate</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
    <span class="c1"># preprocess inputs</span>
    <span class="n">text_fr</span> <span class="o">=</span> <span class="n">sentence</span>
    <span class="n">text_fr</span> <span class="o">=</span> <span class="n">text_fr</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">text_fr</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s2">&quot;&lt;start&gt;&quot;</span><span class="p">,</span> <span class="n">text_fr</span><span class="p">,</span> <span class="s2">&quot;&lt;end&gt;&quot;</span><span class="p">])</span>
    <span class="n">text_en</span> <span class="o">=</span> <span class="s2">&quot;&lt;start&gt;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">tokens_fr</span><span class="p">,</span> <span class="n">tokens_en</span> <span class="o">=</span> <span class="n">collate_batch</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">([</span><span class="n">text_fr</span><span class="p">],</span> <span class="p">[</span><span class="n">text_en</span><span class="p">])))</span>

    <span class="c1"># process encoder</span>
    <span class="n">enc_outputs</span><span class="p">,</span> <span class="n">enc_masks</span> <span class="o">=</span> <span class="n">enc_model</span><span class="p">(</span><span class="n">tokens_fr</span><span class="p">)</span>

    <span class="c1"># process decoder</span>
    <span class="n">final_state</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">loop</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_output</span><span class="p">):</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="n">dec_model</span><span class="p">(</span>
            <span class="n">tokens_en</span><span class="p">,</span>
            <span class="n">enc_outputs</span><span class="p">,</span>
            <span class="n">enc_masks</span><span class="p">,</span>
            <span class="n">states</span><span class="o">=</span><span class="n">final_state</span><span class="p">,</span>
            <span class="n">return_states</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">final_state</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">pred_idx_en</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
        <span class="n">next_word_en</span> <span class="o">=</span> <span class="n">itos_en</span><span class="p">[</span><span class="n">pred_idx_en</span><span class="p">]</span>
        <span class="n">text_en</span> <span class="o">+=</span> <span class="s2">&quot; &quot;</span>
        <span class="n">text_en</span> <span class="o">+=</span> <span class="n">next_word_en</span>
        <span class="k">if</span> <span class="n">pred_idx_en</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">end_index_en</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">tokens_en</span> <span class="o">=</span> <span class="n">collate_batch</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">([</span><span class="s2">&quot;&lt;end&gt;&quot;</span><span class="p">],</span> <span class="p">[</span><span class="n">next_word_en</span><span class="p">])))</span>
    <span class="k">return</span> <span class="n">text_en</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;j&#39;aime la guitare&quot;</span><span class="p">))</span> <span class="c1"># i like guitar</span>
<span class="nb">print</span><span class="p">(</span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;il vit au japon&quot;</span><span class="p">))</span> <span class="c1"># he lives in Japan</span>
<span class="nb">print</span><span class="p">(</span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;ce stylo est utilis√© par lui&quot;</span><span class="p">))</span> <span class="c1"># this pen is used by him</span>
<span class="nb">print</span><span class="p">(</span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;c&#39;est ma chanson pr√©f√©r√©e&quot;</span><span class="p">))</span> <span class="c1"># that&#39;s my favorite song</span>
<span class="nb">print</span><span class="p">(</span><span class="n">translate</span><span class="p">(</span><span class="s2">&quot;il conduit une voiture et va √† new york&quot;</span><span class="p">))</span> <span class="c1"># he drives a car and goes to new york</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;start&gt; i like the guitar &lt;end&gt;
&lt;start&gt; he lives in japan &lt;end&gt;
&lt;start&gt; this pen is used to him &lt;end&gt;
&lt;start&gt; that&#39;s my favorite song &lt;end&gt;
&lt;start&gt; he drives a car and goes to new york &lt;end&gt;
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/ai/nlp/concepts"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="007_encoder_decoder.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Encoder-Decoder Architecture</p>
      </div>
    </a>
    <a class="right-next"
       href="009_transformer.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Transformer</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#install-required-packages">Install required packages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-data">Prepare data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-sequence-inputs">Generate sequence inputs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-network">Build Network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train">Train</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#translate-text">Translate Text</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Anukool Chaturvedi
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div class="social-icons">
    <a href="https://twitter.com/chaturanukool" target="_blank"><i class="fab fa-twitter"></i></a>
    <a href="https://linkedin.com/in/anukool-chaturvedi" target="_blank"><i class="fab fa-linkedin"></i></a>
    <a href="https://github.com/anukchat" target="_blank"><i class="fab fa-github"></i></a>
    <a href="mailto:chaturvedianukool@gmail.com"><i class="fas fa-envelope"></i></a>
    </p>
</div>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>