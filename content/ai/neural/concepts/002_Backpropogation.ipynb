{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "We got a brief explaination of what a Neural network is, now let's understand how such networks are trained.\n",
    "\n",
    "Letâ€™s start by understanding how neural networks *learn* through something called **Backpropagation**.\n",
    "\n",
    "At the heart of AI, backpropagation is an important algorithm that helps a neural network learn from its mistakes and get better at making predictions. \n",
    "\n",
    "Imagine teaching a student by telling them how close their answers are to the correct one, and then giving them feedback to improve their next guess. This is exactly what backpropagation does!\n",
    "\n",
    "**Why do we need it ?**\n",
    "\n",
    "A neural network is like a series of math equations. It takes an input (like an image of a dog), runs some calculations, and then produces an output (like saying whether itâ€™s a dog or a cat). **But the first time the network makes a guess**, itâ€™s usually not very accurate. \n",
    "\n",
    "Backpropagation helps the network learn from its mistake by adjusting its internal settings (called **weights** and **biases**) to improve the guess next time.\n",
    "\n",
    "Letâ€™s break this down step by step:\n",
    "\n",
    "1. **Forward Propagation â€“ Making a Guess**\n",
    "   - Imagine youâ€™re solving a math problem: You take the input (numbers), do some calculations, and come up with an answer.\n",
    "   - In a neural network, the input is passed through layers, where each layer applies some math (a weighted sum + activation) to get a result.\n",
    "   - This result is compared to the correct answer (the true label), and we calculate how *wrong* the networkâ€™s guess was. This is called the **loss** or **error**.\n",
    "\n",
    "2. **Error â€“ How Far Off Was the Guess?**\n",
    "   - The error tells us how bad the guess was. Now, we need to figure out which parts of the network are responsible for that error.\n",
    "   - But instead of saying â€œYouâ€™re wrong!â€ to the network, we give it specific feedback, like â€œThis part of your guess needs fixing more than that part.â€\n",
    "\n",
    "3. **Backward Propagation â€“ Learning from the Mistake**\n",
    "   - This is where **derivatives** come in! Remember from high school calculus, a derivative tells you **how much one thing changes in response to another**. For example, the derivative of distance with respect to time gives you speed (how fast distance changes as time changes).\n",
    "   - In backpropagation, weâ€™re asking: **\"How much does the error change when we change the weights and biases?**\"\n",
    "   - Using the **chain rule** from calculus, we calculate how changing each weight in the network would affect the error. Think of it like tracing your mistake backward through each math step.\n",
    "\n",
    "4. **Derivatives and Gradients â€“ Finding What to Fix**\n",
    "   - The derivative we calculated tells us how much each weight contributed to the error. This gives us a **gradient**â€”a direction to move in to reduce the error.\n",
    "   - Imagine youâ€™re hiking in the mountains and want to reach the lowest point (the errorâ€™s minimum). The gradient points you in the direction of the steepest slope downward, telling you which way to go to reduce the error the fastest.\n",
    "\n",
    "5. **Updating Weights â€“ Fixing the Mistake**\n",
    "   - Once we know which weights are responsible for the error, we adjust them slightly in the opposite direction (down the slope) to reduce the error. This process is called **gradient descent**.\n",
    "   - The size of these adjustments is controlled by a **learning rate**â€”if the steps are too big, we might overshoot; if too small, learning is slow.\n",
    "\n",
    "6. **Repetition â€“ Practice Makes Perfect**\n",
    "   - This process of guessing (forward propagation), learning from mistakes (backpropagation), and updating weights continues for many cycles. Over time, the networkâ€™s guesses improve, and the error gets smaller and smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So, in summary, the learning process is simply adjusting the weights and biases that's it! The Neural Netowork does this by a process called Backpropagation. The steps are as follows:\n",
    "1. Randomly **initialise weights**\n",
    "2. __Forward Pass__: Predict a value using an activation function. \n",
    "3. See how bad you're performing using loss function (compare predicted value with actual value). \n",
    "4. __Backward Pass__: Backpropagate the error. That is, tell your network that it's wrong, and also tell what direction it's supposed to go in order to reduce the error. This step updates the weights (here's where the network learns!)\n",
    "5. Repeat steps 2 & 3 until the error is reasonably small or for a specified number of iterations. \n",
    "\n",
    "Step 3 is the most important step. We'll mathematically derive the equation for updating the values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization\n",
    "\n",
    "Before starting forward propagation we need to initialize Theta parameters. We can not assign zero to all thetas since this would make our network useless because every neuron of the layer will learn the same as its siblings. In other word we need to **break the symmetry**. In order to do so we need to initialize thetas to some small random initial values:\n",
    "\n",
    "![theta-init](images/theta-init.svg)\n",
    "\n",
    "> Though there are some algorithms which provides the initialization strategies to make the model learn faster (Ex: **Xavier Initialization**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward (or Feedforward) Propagation\n",
    "\n",
    "Forward propagation is an interactive process of calculating activations for each layer starting from the input layer and going to the output layer.\n",
    "\n",
    "For the simple network mentioned in a previous section above we're able to calculate activations for second layer based on the input layer and our network parameters:\n",
    "\n",
    "![a-1-2](images/a-1-2.svg)\n",
    "\n",
    "![a-2-2](images/a-2-2.svg)\n",
    "\n",
    "![a-3-2](images/a-3-2.svg)\n",
    "\n",
    "The output layer activation will be calculated based on the hidden layer activations:\n",
    "\n",
    "![h-Theta-example](images/h-Theta-example.svg)\n",
    "\n",
    "Where _g()_ function is activation functions that may be a sigmoid:\n",
    "\n",
    "![sigmoid](images/sigmoid.svg)\n",
    "\n",
    "![Sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n",
    "\n",
    "### Vectorized Implementation of Forward Propagation\n",
    "\n",
    "Now let's convert previous calculations into more concise vectorized form. Because vectorisation helps us to do calculations faster (compared to for loops)\n",
    "\n",
    "![neuron x](images/neuron-x.svg)\n",
    "\n",
    "To simplify previous activation equations let's introduce a _z_ variable:\n",
    "\n",
    "![z-1](images/z-1.svg)\n",
    "\n",
    "![z-2](images/z-2.svg)\n",
    "\n",
    "![z-3](images/z-3.svg)\n",
    "\n",
    "![z-matrix](images/z-matrix.svg)\n",
    "\n",
    "> Don't forget to add bias units (activations) before propagating to the next layer.\n",
    "> ![a-bias](images/a-bias.svg)\n",
    "\n",
    "![z-3-vectorize](images/z-3-vectorized.svg)\n",
    "\n",
    "![h-Theta-vectorized](images/h-Theta-vectorized.svg)\n",
    "\n",
    "### Forward Propagation Example\n",
    "\n",
    "Let's take the following network architecture with 4 layers (input layer, 2 hidden layers and output layer) as an example:\n",
    "\n",
    "![multi-class-network](images/multi-class-network.drawio.svg)\n",
    "\n",
    "In this case the forward propagation steps would look like the following:\n",
    "\n",
    "![forward-propagation-example](images/forward-propagation-example.svg)\n",
    "\n",
    "> Larger the architecture larger the number of layers, more the number of layers, more patterns can be learned by the model from your data . Intermediate layers are called as hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "Cost function is used to check how bad you're network is performing. One simple way to do that is subtract the predicted value and the actual value. For instance, if the actual value is 45 and your network is predicting 30, you can extrapolate you network is off by 15. \n",
    "\n",
    "In practise we don't use the simple subtraction, we instead **square**. Why do we square it? \n",
    "\n",
    "Well the square function has better mathematical properties (like it's a convex function and it's differentiable) which makes it easier for us to calculate the gradient.  \n",
    "\n",
    "\n",
    "$$ \\tag 1\n",
    "MSE=\\frac{1}{2m}\\sum_{i=0}^m\\left(y^{\\left(i\\right)} - h_{\\theta}\\left(x^{\\left(i\\right)}\\right)\\right)^2\n",
    "$$\n",
    "\n",
    "Which is simply the sum of the squared difference between the obeserved value and predicted value. \n",
    "\n",
    "In the above equation\n",
    "1. MSE= Mean Square Error\n",
    "1. m = number of examples\n",
    "2. $h(\\theta)$ = activation function\n",
    "3. $x^{(i)}$ = the ith sample in the dataset\n",
    "4. $y^{(i)}$ = output of ith sample in the dataset\n",
    "\n",
    "To calculate the cost function $J(\\theta)$, we will perform derivation of the loss function with respect to the weights. \n",
    "\n",
    "We'll be using sigmoid activation function which is simplest to illustrate while deriving Backpropagation. \n",
    "\n",
    "Derivation of the sigmoid function looks like this:\n",
    "\n",
    "$$ \\tag 2\n",
    "\\frac{\\partial}{\\partial x}\\sigma\\left(x\\right)=\\sigma\\left(x\\right)\\cdot\\left(1-\\sigma\\left(x\\right)\\right)\n",
    "$$\n",
    "\n",
    "We'll denote the above equation by $\\sigma'\\left(x\\right)$\n",
    "\n",
    "After doing derivation w.r.t to the weights, the cost function for the neuron network would similar to the logistic regression cost function.\n",
    "\n",
    "![cost-function](images/cost-function.svg)\n",
    "\n",
    "![h-Theta](images/h-Theta.svg)\n",
    "\n",
    "![h-Theta-i](images/h-Theta-i.svg)\n",
    "\n",
    "Here, $\\frac{\\lambda}{2m}$ is a L2 regularization (we will know about this in Optimazation techniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "We have defined Forward propogation, Cost function (to calculate the loss w.r.t to each weight of the network), Now is the time to propogate the loss backward to every model weight and update them, so that we can reach closer to our end goal.\n",
    "\n",
    "### Gradient Computation\n",
    "\n",
    "Backpropagation algorithm has the same purpose as gradient descent for linear or logistic regression - **it corrects the values of thetas to minimize a cost function.**\n",
    "\n",
    "In other words we need to be able to calculate partial derivative of cost function for each theta.\n",
    "\n",
    "![J-partial](images/J-partial.svg)\n",
    "\n",
    "![multi-class-network](images/multi-class-network.drawio.svg)\n",
    "\n",
    "Let's assume that:\n",
    "\n",
    "![delta-j-l](images/delta-j-l.svg) - \"error\" of node _j_ in layer _l_.\n",
    "\n",
    "For each output unit (layer _L = 4_):\n",
    "\n",
    "![delta-4](images/delta-4.svg)\n",
    "\n",
    "Or in vectorized form:\n",
    "\n",
    "![delta-4-vectorized](images/delta-4-vectorized.svg)\n",
    "\n",
    "![delta-3-2](images/delta-3-2.svg)\n",
    "\n",
    "![sigmoid-gradient](images/sigmoid-gradient.svg) - sigmoid gradient.\n",
    "\n",
    "![sigmoid-gradient-2](images/sigmoid-gradient-2.svg)\n",
    "\n",
    "Now we may calculate the gradient step:\n",
    "\n",
    "![J-partial-detailed](images/J-partial-detailed.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Algorithm\n",
    "\n",
    "We now know how to compute the gradients per weight (parameter), our **Goal** is to minimize the cost function $J(\\theta)$ to find the best parameters $\\theta$ that fit our model to the data.\n",
    "\n",
    "To achieve that, we need to follow gradient update steps iteratively till we reach convergance (Where difference between actual and expected is reduced to minimum)\n",
    "\n",
    "**Repeat until convergence**:\n",
    "  - For each iteration, update the parameters using the following rule:\n",
    "    $$\n",
    "    \\theta_j := \\theta_j - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$$\n",
    "  - $( \\theta_j )$ is updated for each parameter in the model.\n",
    "  \n",
    "**Stop when**:\n",
    "  - The change in $\\theta_j$ between consecutive iterations **is very small**, indicating **convergence** (i.e., the cost function $J(\\theta)$ has reached a minimum).\n",
    "\n",
    "Where\n",
    "- **$\\theta_j$**:  Represents the $j$-th parameter (or weight) of the model. The model has multiple parameters ($\\theta_0, \\theta_1, \\dots \\theta_n$) depending on the number of features in the input data.\n",
    "  \n",
    "- **$\\alpha$** (Learning Rate):  \n",
    "  - The learning rate controls how big of a step we take in the direction of the gradient.  \n",
    "  - **Small $\\alpha$**: Slow learning (takes many iterations to converge).  \n",
    "  - **Large $\\alpha$**: Can lead to overshooting and potentially not converge at all.  \n",
    "  - Typical values: $\\alpha$ is often set between 0.01 and 0.1.\n",
    "\n",
    "- **$\\frac{\\partial}{\\partial \\theta_j} J(\\theta)$**:  \n",
    "  - This is the partial derivative (gradient) of the cost function $J(\\theta)$ with respect to the parameter $\\theta_j$.  \n",
    "  - The gradient tells us the slope of the cost function at the current parameter values.  \n",
    "  - If the slope is positive, $\\theta_j$ is decreased; if the slope is negative, $\\theta_j$ is increased. This is because we want to move towards the minimum of the cost function.\n",
    "\n",
    "- **Cost Function $J(\\theta)$**:  \n",
    "  - The cost function measures how far off the model's predictions are from the actual values in the training data.  \n",
    "  - For linear regression, it's usually defined as:\n",
    "    $$\n",
    "    J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
    "    $$\n",
    "  - Where:\n",
    "    - $m$ is the number of training examples.\n",
    "    - $h_\\theta(x^{(i)})$ is the predicted value (hypothesis function).\n",
    "    - $y^{(i)}$ is the actual value.\n",
    "\n",
    "- **$m$** (Number of training examples): $m$ represents the number of examples in the training dataset.\n",
    "\n",
    "- At each step of gradient descent, the parameters $\\theta_j$ are updated in the opposite direction of the gradient of the cost function. This ensures we are moving toward the minimum value of the cost function.\n",
    "  \n",
    "- The learning rate $\\alpha$ controls the size of these updates.\n",
    "  \n",
    "\n",
    "For each $j = 0, 1, \\dots, n$, repeat the following update until convergence:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $h_\\theta(x^{(i)}) = \\theta_0 + \\theta_1 x_1^{(i)} + \\theta_2 x_2^{(i)} + \\dots + \\theta_n x_n^{(i)}$\n",
    "  - This is the model's prediction (the hypothesis function).\n",
    "- $x_j^{(i)}$ is the value of the $j$-th feature for the $i$-th training example.\n",
    "\n",
    "- The algorithm stops when:\n",
    "  - The change in $\\theta_j$ between consecutive iterations is very small (convergence).\n",
    "  - The maximum number of iterations is reached (sometimes used as a safety measure).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate ($\\alpha$)\n",
    "\n",
    "The learning rate is a hyperparameter that controls the step size at each iteration while moving toward a minimum of a loss function. \n",
    "\n",
    "- **Too High**: If the learning rate is too high, the model may converge too quickly to a suboptimal solution or even diverge, failing to find the best minimum of the loss function.\n",
    "- **Too Low**: If the learning rate is too low, the model will converge very slowly, requiring more iterations and computational resources to reach an optimal solution. It might also get stuck in local minima or plateau areas.\n",
    "\n",
    "Let's understand it through a visualization\n",
    "\n",
    "![learning rate](images/learning-rate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see yourself, how different values of learning rate impacts the gradient behaviour, try below visualization\n",
    "\n",
    "1. Add a large learning rate of say 2 and keep clicking on **Next iteration** to check how loss reduces or it gets divergent after some time\n",
    "2. Add a very small learning rate of 0.001 and keep clicking on **Next iteration** to check how much time it takes to converge\n",
    "3. Add a medium learning rate of say 0.1 and check how much time it takes to converge\n",
    "\n",
    "<iframe src=\"https://uclaacm.github.io/gradient-descent-visualiser/#playground\" width=\"100%\" height=\"600\"></iframe>\n",
    "\n",
    "\n",
    "> Notice how the loss reduces and as we approach closer to the minimum, the number of iteration increases for convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info' role='alert'>\n",
    "ðŸ”Ž <strong>Hyperparameters</strong> in machine learning are settings that are chosen before the model training process begins and control the learning behavior of the model. \n",
    "\n",
    "They differ from model parameters (like weights) as they are not learned from the data. Proper tuning of hyperparameters is critical for optimizing model performance.\n",
    "\n",
    "Some of them are:\n",
    "\n",
    "1. **Learning Rate**:\n",
    "2. **Batch Size:**\n",
    "3. **Number of Epochs:**\n",
    "4. **Number of Layers and Neurons:**\n",
    "5. **Dropout Rate:**\n",
    "6. **Optimizer:**\n",
    "7. **Weight Initialization:**\n",
    "8. many more...\n",
    "   \n",
    "These hyperparameters can significantly impact the performance of a neural network and are typically tuned through experimentation or automated search methods like grid search or random search.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'X', 'y'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "data = loadmat(\"machine_learning_andrewng/ex4data1.mat\")\n",
    "weights = loadmat(\"machine_learning_andrewng/ex3weights.mat\")\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "#one-hot encoding the y values\n",
    "y = pd.get_dummies(y.ravel()).values\n",
    "theta1_loaded = weights[\"Theta1\"]\n",
    "theta2_loaded = weights[\"Theta2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12d6ad750>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[900].reshape(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation we will use MNIST dataset, which is a set of handwritten number images from 1-10, our task is correctly detect these hand written digits\n",
    "\n",
    "The image size is 20x20 pixels, which we have already flattened in the dataset to (400,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from time import time\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"Creates Neural Network for MNIST dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=25, output_size=10, lambda_=0):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with:\n",
    "        - hidden_size: Number of neurons in the hidden layer\n",
    "        - output_size: Number of output neurons (MNIST has 10 digits, so 10 outputs)\n",
    "        - lambda_: Regularization parameter to prevent overfitting\n",
    "        \"\"\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.thetas = None  # Placeholder for network parameters (weights)\n",
    "        self.lambda_ = lambda_  # Regularization coefficient\n",
    "        self.X = None  # Placeholder for input data\n",
    "        self.y = None  # Placeholder for target data\n",
    "        self.input_size = 0  # Input layer size will be set based on data\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"Activation function: Sigmoid used to introduce non-linearity\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def flatten(arr1, arr2):\n",
    "        \"\"\"Flatten two matrices into a single vector (for optimization purposes)\"\"\"\n",
    "        return np.r_[arr1.flatten(), arr2.flatten()]\n",
    "    \n",
    "    def set_params(self, *thetas):\n",
    "        \"\"\"Set thetas (weights) by flattening and assigning them\"\"\"\n",
    "        self.thetas = self.flatten(*thetas)\n",
    "    \n",
    "    def unflatten(self, arr):\n",
    "        \"\"\"\n",
    "        Reshape the flattened thetas vector back into two separate matrices.\n",
    "        theta1 corresponds to input-to-hidden weights.\n",
    "        theta2 corresponds to hidden-to-output weights.\n",
    "        \"\"\"\n",
    "        theta1 = arr[:self.hidden_size * (self.input_size + 1)]\n",
    "        theta1 = theta1.reshape(self.hidden_size, self.input_size + 1)\n",
    "        theta2 = arr[self.hidden_size * (self.input_size + 1):]\n",
    "        theta2 = theta2.reshape(self.output_size, self.hidden_size + 1)\n",
    "        return theta1, theta2\n",
    "    \n",
    "    def init_random_thetas(self, epsilon=0.12):\n",
    "        \"\"\"\n",
    "        Initialize random values for weights (theta1 and theta2) with a small range to avoid symmetry.\n",
    "        The weights are initialized within [-epsilon, epsilon].\n",
    "        \"\"\"\n",
    "        theta1 = np.random.rand(self.hidden_size, self.input_size + 1) * 2 * epsilon - epsilon\n",
    "        theta2 = np.random.rand(self.output_size, self.hidden_size + 1) * 2 * epsilon - epsilon\n",
    "        return self.flatten(theta1, theta2)\n",
    "\n",
    "    def sigmoid_prime(self, z):\n",
    "        \"\"\"Derivative of the sigmoid function, used in backpropagation\"\"\"\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "    def cross_entropy(self, thetas=None):\n",
    "        \"\"\"\n",
    "        Compute the cross-entropy loss, including regularization.\n",
    "        Loss measures the difference between predicted and actual values.\n",
    "        \"\"\"\n",
    "        if thetas is None:\n",
    "            theta1, theta2 = self.unflatten(self.thetas)\n",
    "        else:\n",
    "            theta1, theta2 = self.unflatten(thetas)\n",
    "\n",
    "        m = self.X.shape[0]  # Number of training examples\n",
    "        \n",
    "        # Perform a forward pass to calculate the predicted values\n",
    "        y_pred = self.forward_pass(thetas)\n",
    "        \n",
    "        # Calculate the loss: negative log-likelihood of the true values\n",
    "        positive_loss = np.sum(np.multiply(self.y, np.log(y_pred)).flatten())\n",
    "        negative_loss = np.sum(np.multiply((1 - self.y), np.log(1 - y_pred)).flatten())\n",
    "        \n",
    "        # Add regularization term to prevent overfitting\n",
    "        regularization = (self.lambda_ / (2 * m)) * (np.sum(theta1.flatten() ** 2) + np.sum(theta2.flatten() ** 2))\n",
    "        \n",
    "        # Final cost function with regularization\n",
    "        J = - (1 / m) * (positive_loss + negative_loss) + regularization\n",
    "        return J\n",
    "\n",
    "    def forward_pass(self, thetas=None, elaborate=False):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "        Calculate activations for each layer and return the output.\n",
    "        \"\"\"\n",
    "        if thetas is None:\n",
    "            theta1, theta2 = self.unflatten(self.thetas)\n",
    "        else:\n",
    "            theta1, theta2 = self.unflatten(thetas)\n",
    "        \n",
    "        # Add bias to input data (a1)\n",
    "        a1 = np.c_[np.ones(self.X.shape[0]), self.X]\n",
    "        \n",
    "        # Hidden layer (z2, a2)\n",
    "        z2 = theta1.dot(a1.T)  # Compute z2 (hidden layer input)\n",
    "        a2 = self.sigmoid(z2.T)  # Apply activation function (sigmoid) to get a2\n",
    "        \n",
    "        # Add bias to hidden layer activations\n",
    "        a2 = np.c_[np.ones(a2.shape[0]), a2]\n",
    "        \n",
    "        # Output layer (z3, a3)\n",
    "        z3 = theta2.dot(a2.T)\n",
    "        a3 = self.sigmoid(z3.T)\n",
    "        \n",
    "        # If requested, return intermediate activations for backpropagation\n",
    "        if elaborate:\n",
    "            return (a1, a2, a3), (z2, z3)\n",
    "        return a3\n",
    "\n",
    "    def backward_pass(self, thetas=None):\n",
    "        \"\"\"\n",
    "        Perform a backward pass to compute gradients for weight updates.\n",
    "        This implements the backpropagation algorithm.\n",
    "        \"\"\"\n",
    "        if thetas is None:\n",
    "            theta1, theta2 = self.unflatten(self.thetas)\n",
    "        else:\n",
    "            theta1, theta2 = self.unflatten(thetas)\n",
    "\n",
    "        # Forward pass to get all activations\n",
    "        (a1, a2, y_pred), (z2, z3) = self.forward_pass(thetas, elaborate=True)\n",
    "\n",
    "        # Compute the error in the output layer\n",
    "        delta3 = np.multiply((y_pred - self.y), self.sigmoid_prime(z3.T))\n",
    "\n",
    "        # Gradient for theta2\n",
    "        theta2_grad = a2.T.dot(delta3)\n",
    "        theta2_grad = theta2_grad.T  # Ensure shape matches theta2\n",
    "\n",
    "        # Compute the error in the hidden layer\n",
    "        delta2 = np.multiply(delta3.dot(theta2[:, 1:]), self.sigmoid_prime(z2.T))\n",
    "\n",
    "        # Gradient for theta1\n",
    "        theta1_grad = a1.T.dot(delta2)\n",
    "        theta1_grad = theta1_grad.T\n",
    "\n",
    "        return self.flatten(theta1_grad, theta2_grad)\n",
    "\n",
    "    def gradient_descent(self, X, y, n_epochs=1000, alpha=0.001):\n",
    "        \"\"\"\n",
    "        Perform gradient descent optimization to minimize the cost function.\n",
    "        \"\"\"\n",
    "        # Initialize random weights (thetas)\n",
    "        self.thetas = self.init_random_thetas()\n",
    "        theta1, theta2 = self.unflatten(self.thetas)\n",
    "\n",
    "        # Loop over epochs to update thetas using gradients\n",
    "        for i in range(1, n_epochs + 1):\n",
    "            # Compute cost for current thetas\n",
    "            cost = self.cross_entropy()\n",
    "            print(\"\\rIteration: {0} Cost: {1}\".format(i, cost), end=\"\")\n",
    "            \n",
    "            # Compute gradients\n",
    "            theta1_grad, theta2_grad = self.unflatten(self.backward_pass())\n",
    "\n",
    "            # Update weights (thetas) using gradients\n",
    "            theta1 = theta1 - alpha * theta1_grad\n",
    "            theta2 = theta2 - alpha * theta2_grad\n",
    "\n",
    "            # Flatten the updated weights and set them as the current parameters\n",
    "            self.thetas = self.flatten(theta1, theta2)\n",
    "        print()\n",
    "\n",
    "    def fmin_unc(self, X, y, **params):\n",
    "        \"\"\"\n",
    "        Use scipy's minimize function to optimize the weights using unconstrained optimization.\n",
    "        \"\"\"\n",
    "        self.thetas = self.init_random_thetas()\n",
    "        res = minimize(self.cross_entropy, self.thetas, jac=self.backward_pass,\n",
    "                       method=\"tnc\", options=params)\n",
    "        print(res)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model with training data (X) and labels (y).\n",
    "        It sets the input size based on the training data and ensures\n",
    "        that the number of output neurons matches the number of labels.\n",
    "        \"\"\"\n",
    "        if y.shape[1] != self.output_size:\n",
    "            raise ValueError(\"Number of columns in y ({0}) are != to number of output neurons ({1})\"\n",
    "                             .format(y.shape[1], self.output_size))\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.input_size = X.shape[1]\n",
    "    \n",
    "    def train(self, method=\"gradient_descent\", **params):\n",
    "        \"\"\"\n",
    "        Train the model using either gradient descent or unconstrained optimization.\n",
    "        It measures the training time.\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if method == \"gradient_descent\":\n",
    "            self.gradient_descent(X, y, **params)\n",
    "        else:\n",
    "            self.fmin_unc(X, y, **params)\n",
    "        print(\"Training time: {0:.2f} secs\".format(time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using Gradient Descent\n",
      "Iteration: 1000 Cost: 0.4521438114818742\n",
      "Training time: 6.80 secs\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(hidden_size=25, output_size=10)\n",
    "nn.fit(X, y)\n",
    "print(\"Training using Gradient Descent\")\n",
    "nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using Newton's Conjugate Gradient\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2g/fd5fqwvd6x5cd9_2fspz1xh40000gp/T/ipykernel_81976/4005965860.py:177: OptimizeWarning: Unknown solver options: maxiter\n",
      "  res = minimize(self.cross_entropy, self.thetas, jac=self.backward_pass,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " message: Converged (|f_n-f_(n-1)| ~= 0)\n",
      " success: True\n",
      "  status: 1\n",
      "     fun: 0.44474797884435985\n",
      "       x: [-1.580e+00  1.086e-01 ... -3.269e+00  2.960e+00]\n",
      "     nit: 25\n",
      "     jac: [ 2.042e+00  0.000e+00 ... -5.364e-02 -1.743e-01]\n",
      "    nfev: 342\n",
      "Training time: 2.42 secs\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(hidden_size=25, output_size=10)\n",
    "nn.fit(X, y)\n",
    "print(\"Training using Newton's Conjugate Gradient\")\n",
    "nn.train(method=\"newton\", maxiter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.2876291651613189)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(hidden_size=25, output_size=10)\n",
    "nn.fit(X, y)\n",
    "nn.set_params(theta1_loaded, theta2_loaded)\n",
    "nn.cross_entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Machine Learning on Coursera](https://www.coursera.org/learn/machine-learning)\n",
    "- [But what is a Neural Network? By 3Blue1Brown](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "- [Neural Network on Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "- [TensorFlow Neural Network Playground](https://playground.tensorflow.org/)\n",
    "- [Deep Learning by Carnegie Mellon University](https://insights.sei.cmu.edu/sei_blog/2018/02/deep-learning-going-deeper-toward-meaningful-patterns-in-complex-data.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
