{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropogation algorithm is used to train a neural network by adjusting the weights and biases to minimize the cost function. It is an iterative process that involves forward propagation and backward propagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does a Neural Network learn? \n",
    "The learning process is simply adjusting the weights and biases that's it! The Neural Netowork does this by a process called Backpropagation. The steps are as follows:\n",
    "1. Randomly **initialise weights**\n",
    "2. __Forward Pass__: Predict a value using an activation function. \n",
    "3. See how bad you're performing using loss function (compare predicted value with actual value). \n",
    "4. __Backward Pass__: Backpropagate the error. That is, tell your network that it's wrong, and also tell what direction it's supposed to go in order to reduce the error. This step updates the weights (here's where the network learns!)\n",
    "5. Repeat steps 2 & 3 until the error is reasonably small or for a specified number of iterations. \n",
    "\n",
    "Step 3 is the most important step. We'll mathematically derive the equation for updating the values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization\n",
    "\n",
    "Before starting forward propagation we need to initialize Theta parameters. We can not assign zero to all thetas since this would make our network useless because every neuron of the layer will learn the same as its siblings. In other word we need to **break the symmetry**. In order to do so we need to initialize thetas to some small random initial values:\n",
    "\n",
    "![theta-init](../images/theta-init.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward (or Feedforward) Propagation\n",
    "\n",
    "Forward propagation is an interactive process of calculating activations for each layer starting from the input layer and going to the output layer.\n",
    "\n",
    "For the simple network mentioned in a previous section above we're able to calculate activations for second layer based on the input layer and our network parameters:\n",
    "\n",
    "![a-1-2](../images/a-1-2.svg)\n",
    "\n",
    "![a-2-2](../images/a-2-2.svg)\n",
    "\n",
    "![a-3-2](../images/a-3-2.svg)\n",
    "\n",
    "The output layer activation will be calculated based on the hidden layer activations:\n",
    "\n",
    "![h-Theta-example](../images/h-Theta-example.svg)\n",
    "\n",
    "Where _g()_ function may be a sigmoid:\n",
    "\n",
    "![sigmoid](../images/sigmoid.svg)\n",
    "\n",
    "![Sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n",
    "\n",
    "### Vectorized Implementation of Forward Propagation\n",
    "\n",
    "Now let's convert previous calculations into more concise vectorized form.\n",
    "\n",
    "![neuron x](../images/neuron-x.svg)\n",
    "\n",
    "To simplify previous activation equations let's introduce a _z_ variable:\n",
    "\n",
    "![z-1](../images/z-1.svg)\n",
    "\n",
    "![z-2](../images/z-2.svg)\n",
    "\n",
    "![z-3](../images/z-3.svg)\n",
    "\n",
    "![z-matrix](../images/z-matrix.svg)\n",
    "\n",
    "> Don't forget to add bias units (activations) before propagating to the next layer.\n",
    "> ![a-bias](../images/a-bias.svg)\n",
    "\n",
    "![z-3-vectorize](../images/z-3-vectorized.svg)\n",
    "\n",
    "![h-Theta-vectorized](../images/h-Theta-vectorized.svg)\n",
    "\n",
    "### Forward Propagation Example\n",
    "\n",
    "Let's take the following network architecture with 4 layers (input layer, 2 hidden layers and output layer) as an example:\n",
    "\n",
    "![multi-class-network](../images/multi-class-network.drawio.svg)\n",
    "\n",
    "In this case the forward propagation steps would look like the following:\n",
    "\n",
    "![forward-propagation-example](../images/forward-propagation-example.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "Cost function is used to check how bad you're network is performing. One simple way to do that is subtract the predicted value and the actual value. For instance, if the actual value is 45 and your network is predicting 30, you can extrapolate you network is off by 15. \n",
    "\n",
    "In practise we don't use the simple subtraction, we instead **square**. Why do we square it? \n",
    "\n",
    "Well the square function has better mathematical properties (like it's a convex function and it's differentiable) which makes it easier for us to calculate the gradient.  \n",
    "\n",
    "To calculate the loss function, we will perform derivation of the loss function with respect to the weights. \n",
    "\n",
    "$$ \\tag 1\n",
    "MSE=\\sum_{i=0}^m\\left(y^{\\left(i\\right)} - h_{\\theta}\\left(x^{\\left(i\\right)}\\right)\\right)^2\n",
    "$$\n",
    "\n",
    "Which is simply the sum of the squared difference between the obeserved value and predicted value. \n",
    "\n",
    "In the above equation\n",
    "1. m = number of examples\n",
    "2. $h(\\theta)$ = activation function\n",
    "3. $x^{(i)}$ = the ith sample in the dataset\n",
    "4. $y^{(i)}$ = output of ith sample in the dataset\n",
    "\n",
    "We'll be using sigmoid activation function which is simplest to illustrate while deriving Backpropagation. \n",
    "\n",
    "Derivation of the sigmoid function looks like this:\n",
    "\n",
    "$$ \\tag 2\n",
    "\\frac{\\partial}{\\partial x}\\sigma\\left(x\\right)=\\sigma\\left(x\\right)\\cdot\\left(1-\\sigma\\left(x\\right)\\right)\n",
    "$$\n",
    "\n",
    "We'll denote the above equation by $\\sigma'\\left(x\\right)$\n",
    "\n",
    "\n",
    "The cost function for the neuron network is quite similar to the logistic regression cost function.\n",
    "\n",
    "\n",
    "![cost-function](../images/cost-function.svg)\n",
    "\n",
    "![h-Theta](../images/h-Theta.svg)\n",
    "\n",
    "![h-Theta-i](../images/h-Theta-i.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Backpropagation\n",
    "\n",
    "### Gradient Computation\n",
    "\n",
    "Backpropagation algorithm has the same purpose as gradient descent for linear or logistic regression - it corrects the values of thetas to minimize a cost function.\n",
    "\n",
    "In other words we need to be able to calculate partial derivative of cost function for each theta.\n",
    "\n",
    "![J-partial](../images/J-partial.svg)\n",
    "\n",
    "![multi-class-network](../images/multi-class-network.drawio.svg)\n",
    "\n",
    "Let's assume that:\n",
    "\n",
    "![delta-j-l](../images/delta-j-l.svg) - \"error\" of node _j_ in layer _l_.\n",
    "\n",
    "For each output unit (layer _L = 4_):\n",
    "\n",
    "![delta-4](../images/delta-4.svg)\n",
    "\n",
    "Or in vectorized form:\n",
    "\n",
    "![delta-4-vectorized](../images/delta-4-vectorized.svg)\n",
    "\n",
    "![delta-3-2](../images/delta-3-2.svg)\n",
    "\n",
    "![sigmoid-gradient](../images/sigmoid-gradient.svg) - sigmoid gradient.\n",
    "\n",
    "![sigmoid-gradient-2](../images/sigmoid-gradient-2.svg)\n",
    "\n",
    "Now we may calculate the gradient step:\n",
    "\n",
    "![J-partial-detailed](../images/J-partial-detailed.svg)\n",
    "\n",
    "### Backpropagation Algorithm\n",
    "\n",
    "For training set\n",
    "\n",
    "![training-set](../images/training-set.svg)\n",
    "\n",
    "We need to set:\n",
    "\n",
    "![Delta](../images/Delta.svg)\n",
    "\n",
    "![backpropagation](../images/backpropagation.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'X', 'y'])\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'Theta1', 'Theta2'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "data = loadmat(\"machine_learning_andrewng/ex4data1.mat\")\n",
    "print(data.keys())\n",
    "print(weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "#one-hot encoding the y values\n",
    "y = pd.get_dummies(y.ravel()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from time import time\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"Creates Neural Network for MNIST dataset.\"\"\"\n",
    "    def __init__(self, hidden_size=25, output_size=10, lambda_=0):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.thetas = None\n",
    "        self.lambda_ = lambda_\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.input_size = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten(arr1, arr2):\n",
    "        return np.r_[arr1.flatten(), arr2.flatten()]\n",
    "    \n",
    "    def set_params(self, *thetas):\n",
    "        self.thetas = self.flatten(*thetas)\n",
    "\n",
    "    def unflatten(self, arr):\n",
    "        theta1 = arr[:self.hidden_size * (self.input_size + 1)]\n",
    "        theta1 = theta1.reshape(self.hidden_size, self.input_size + 1)\n",
    "        theta2 = arr[self.hidden_size * (self.input_size + 1):]\n",
    "        theta2 = theta2.reshape(self.output_size, self.hidden_size + 1)\n",
    "        return theta1, theta2\n",
    "\n",
    "    def init_random_thetas(self, epsilon=0.12):\n",
    "        theta1 = np.random.rand(self.hidden_size, self.input_size + 1) \\\n",
    "                 * 2 * epsilon - epsilon\n",
    "        theta2 = np.random.rand(self.output_size, self.hidden_size + 1) \\\n",
    "                 * 2 * epsilon - epsilon\n",
    "        return self.flatten(theta1, theta2)\n",
    "\n",
    "    def sigmoid_prime(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "    #loss function\n",
    "    def cross_entropy(self, thetas=None):\n",
    "        if thetas is None:\n",
    "            theta1, theta2 = self.unflatten(self.thetas)\n",
    "        else:\n",
    "            theta1, theta2 = self.unflatten(thetas)\n",
    "        m = self.X.shape[0]\n",
    "        y_pred = self.forward_pass(thetas)\n",
    "        positive_loss = np.sum(np.multiply(self.y, np.log(y_pred)).flatten())\n",
    "        negative_loss = np.sum(np.multiply((1 - self.y), np.log(1 - y_pred))\n",
    "                               .flatten())\n",
    "        regularization = (self.lambda_ / (2 * m)) \\\n",
    "                         * (np.sum(theta1.flatten() ** 2)\n",
    "                            + np.sum(theta2.flatten() ** 2))\n",
    "        J = - (1 / m) * (positive_loss + negative_loss) + regularization\n",
    "        return J\n",
    "\n",
    "    def forward_pass(self, thetas=None, elaborate=False):\n",
    "        if thetas is None:\n",
    "            theta1, theta2 = self.unflatten(self.thetas)\n",
    "        else:\n",
    "            theta1, theta2 = self.unflatten(thetas)\n",
    "        a1 = np.c_[np.ones(self.X.shape[0]), self.X]\n",
    "        z2 = theta1.dot(a1.T)                   # 25x401 * 401x5000 = 25x5000\n",
    "        a2 = self.sigmoid(z2.T)                 # 5000x25\n",
    "        a2 = np.c_[np.ones(a2.shape[0]), a2]    # 5000x26\n",
    "        z3 = theta2.dot(a2.T)                   # 10x26 * 26x5000 = 10x5000\n",
    "        a3 = self.sigmoid(z3.T)                 # 5000x10\n",
    "        if elaborate:\n",
    "            return (a1, a2, a3), (z2, z3)\n",
    "        return a3\n",
    "\n",
    "    def backward_pass(self, thetas=None):\n",
    "        if thetas is None:\n",
    "            theta1, theta2 = self.unflatten(self.thetas)\n",
    "        else:\n",
    "            theta1, theta2 = self.unflatten(thetas)\n",
    "        (a1, a2, y_pred), (z2, z3) = self.forward_pass(thetas, elaborate=True)\n",
    "        delta3 = np.multiply((y_pred - self.y), self.sigmoid_prime(z3.T))\n",
    "        theta2_grad = a2.T.dot(delta3)\n",
    "        theta2_grad = theta2_grad.T\n",
    "        # theta2_grad.shape is now same as theta2.shape\n",
    "        delta2 = np.multiply(delta3.dot(theta2[:, 1:]), self.sigmoid_prime(z2.T))\n",
    "        theta1_grad = a1.T.dot(delta2)\n",
    "        theta1_grad = theta1_grad.T\n",
    "        return self.flatten(theta1_grad, theta2_grad)\n",
    "\n",
    "    def gradient_descent(self, X, y, n_epochs=1000, alpha=0.001):\n",
    "        self.thetas = self.init_random_thetas()\n",
    "        theta1, theta2 = self.unflatten(self.thetas)\n",
    "        \n",
    "        for i in range(1, n_epochs+1):\n",
    "            cost = self.cross_entropy()\n",
    "            print(\"\\rIteration: {0} Cost: {1}\".format(i, cost), end=\"\")\n",
    "            theta1_grad, theta2_grad = self.unflatten(self.backward_pass())\n",
    "            theta1 = theta1 - alpha * theta1_grad\n",
    "            theta2 = theta2 - alpha * theta2_grad\n",
    "            self.thetas = self.flatten(theta1, theta2)\n",
    "        print()\n",
    "        \n",
    "    def fmin_unc(self, X, y, **params):\n",
    "        self.thetas = self.init_random_thetas()\n",
    "        res = minimize(self.cross_entropy, self.thetas, jac=self.backward_pass,\n",
    "                       method=\"tnc\", options=params)\n",
    "        print(res)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if y.shape[1] != self.output_size:\n",
    "            raise ValueError(\"Number of columns in y ({0}) are != to number \"\n",
    "                             \"of output neurons ({1})\"\n",
    "                             .format(y.shape[1], self.output_size))\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.input_size = X.shape[1]\n",
    "    \n",
    "    def train(self, method=\"gradient_descent\", **params):\n",
    "        start_time = time()\n",
    "        if method == \"gradient_descent\":\n",
    "            self.gradient_descent(X, y, **params)\n",
    "        else:\n",
    "            self.fmin_unc(X, y, **params)\n",
    "        print(\"Training time: {0:.2f} secs\".format(time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using Gradient Descent\n",
      "Iteration: 1000 Cost: 0.4512534813094702\n",
      "Training time: 133.12 secs\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(hidden_size=25, output_size=10)\n",
    "nn.fit(X, y)\n",
    "print(\"Training using Gradient Descent\")\n",
    "nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using Newton's Conjugate Gradient\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7m/04ssj6n96q984_6wsnr60dg00000gn/T/ipykernel_19899/3101508393.py:108: OptimizeWarning: Unknown solver options: maxiter\n",
      "  res = minimize(self.cross_entropy, self.thetas, jac=self.backward_pass,\n",
      "/var/folders/7m/04ssj6n96q984_6wsnr60dg00000gn/T/ipykernel_19899/3101508393.py:55: RuntimeWarning: divide by zero encountered in log\n",
      "  negative_loss = np.sum(np.multiply((1 - self.y), np.log(1 - y_pred))\n",
      "/var/folders/7m/04ssj6n96q984_6wsnr60dg00000gn/T/ipykernel_19899/3101508393.py:55: RuntimeWarning: invalid value encountered in multiply\n",
      "  negative_loss = np.sum(np.multiply((1 - self.y), np.log(1 - y_pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " message: Linear search failed\n",
      " success: False\n",
      "  status: 4\n",
      "     fun: 1.1439424009572163\n",
      "       x: [-1.856e-01  1.621e-02 ... -3.700e-01 -6.315e+00]\n",
      "     nit: 25\n",
      "     jac: [-2.211e-02  0.000e+00 ... -8.103e-02  2.362e-01]\n",
      "    nfev: 542\n",
      "Training time: 47.70 secs\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(hidden_size=25, output_size=10)\n",
    "nn.fit(X, y)\n",
    "print(\"Training using Newton's Conjugate Gradient\")\n",
    "nn.train(method=\"newton\", maxiter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2876291651613189"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(hidden_size=25, output_size=10)\n",
    "nn.fit(X, y)\n",
    "nn.set_params(theta1_loaded, theta2_loaded)\n",
    "nn.cross_entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Machine Learning on Coursera](https://www.coursera.org/learn/machine-learning)\n",
    "- [But what is a Neural Network? By 3Blue1Brown](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "- [Neural Network on Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "- [TensorFlow Neural Network Playground](https://playground.tensorflow.org/)\n",
    "- [Deep Learning by Carnegie Mellon University](https://insights.sei.cmu.edu/sei_blog/2018/02/deep-learning-going-deeper-toward-meaningful-patterns-in-complex-data.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
