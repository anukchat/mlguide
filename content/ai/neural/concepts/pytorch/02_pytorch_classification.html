
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>PyTorch Neural Network Classification &#8212; Machine Learning Guide</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/intro.css?v=a1b4e56e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/ai/neural/concepts/pytorch/02_pytorch_classification';</script>
    <link rel="canonical" href="https://mlguide.in/content/ai/neural/concepts/pytorch/02_pytorch_classification.html" />
    <link rel="icon" href="../../../../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <link rel="next" title="PyTorch Computer Vision" href="03_pytorch_computer_vision.html" />
    <link rel="prev" title="PyTorch Workflow" href="01_pytorch_workflow.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../../intro_v2.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../../../_static/MLGuide_logo_nb.png" class="logo__image only-light" alt="Machine Learning Guide - Home"/>
    <script>document.write(`<img src="../../../../../_static/MLGuide_logo_nb.png" class="logo__image only-dark" alt="Machine Learning Guide - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Guide</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../python/python_toc.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../python/1_installation.html">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../python/2_syntax_and_symantics.html">Syntax &amp; Symantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../python/3_functions_and_modules.html">Functions &amp; Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../python/4_Object_Oriented.html">Object Oriented Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../python/5_Exceptions_Handling.html">Exceptions Handling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../python/6_Handling_Files.html">Handling Files</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../python/7_Datetime_Operations.html">Datetime Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../python/8_advanced.html">Advanced Concepts</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../mathematics/mathematics_toc.html">Mathematics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../mathematics/linear-algebra_vectors.html">Vectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../mathematics/linear-algebra_matrices.html">Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../mathematics/dissimilarity_measures.html">Similarity measure</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../analytics/intro_analytics.html">Data analytics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../analytics/numpy/numpy_toc.html">Numpy</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../analytics/numpy/001_Python_NumPy.html">NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../analytics/numpy/Python_Numpy_Exercises_with_hints.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../analytics/pandas/pandas_toc.html">Pandas</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../analytics/pandas/001_Python_Pandas_DataFrame.html">Pandas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../analytics/pandas/002_Pandas_HowTos.html">How To's</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../analytics/pandas/003_Pandas_Exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../analytics/matplotlib/matplotlib_toc.html">Matplotlib</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../analytics/matplotlib/001_Python_Matplotlib.html">Matplotlib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../analytics/matplotlib/003_Python_Matplotlib_Exercises.html">Exercises</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../Introduction_to_ml.html">Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../classicml/concepts/01_Introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../classicml/concepts/000_Data_Exploration.html">Exploratory Data Analysis</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../classicml/concepts/001_Data_Preparation.html">Data Preparation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../classicml/concepts/002_Regression.html">Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../classicml/concepts/003_Classification.html">Classfication</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../classicml/concepts/004_Clustering.html">Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../classicml/concepts/005_Evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../classicml/concepts/006_Advanced.html">K-Fold Cross Validation</a></li>


<li class="toctree-l2"><a class="reference internal" href="../../../classicml/concepts/007_Dimensionality_Reduction.html">Dimensionality Reduction</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../../neural_toc.html">Neural Networks</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../001_Introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../002_Backpropogation.html">Backpropogation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../003_Activations.html">Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../004_Optimization.html">Optimizations</a></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="pytorch_toc.html">Pytorch</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="00_pytorch_fundamentals.html">Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_pytorch_workflow.html">Workflow</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="03_pytorch_computer_vision.html">Computer Vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="04_pytorch_custom_datasets.html">Custom Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="06_pytorch_transfer_learning.html">Transfer Learning</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../genai/introduction.html">Generative AI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../genai/concepts/transformers/01_transformers_from_scratch.html">Transformers</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../genai/langchain/langchain_toc.html">Langchain</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../genai/langchain/intro.html">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../genai/langchain/01_LangChain_Fundamentals.html">LangChain Cookbook üë®‚Äçüç≥üë©‚Äçüç≥</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../genai/langchain/02_LangChain_Use_Cases.html">LangChain Cookbook Part 2: Use Casesüë®‚Äçüç≥üë©‚Äçüç≥</a></li>

</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../resources/blogs/blogs_toc.html">Blogs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../resources/papers/papers_toc.html">Research papers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../resources/books/books_toc.html">E-Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../resources/courses/courses_toc.html">Courses</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../intro_me.html">About me</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/anukchat/mlguide/main?urlpath=lab/tree/content/ai/neural/concepts/pytorch/02_pytorch_classification.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../../../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/anukchat/mlguide/blob/main/content/ai/neural/concepts/pytorch/02_pytorch_classification.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../../../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../../../_sources/content/ai/neural/concepts/pytorch/02_pytorch_classification.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>PyTorch Neural Network Classification</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-classification-problem">What is a classification problem?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-of-a-classification-neural-network">Architecture of a classification neural network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#make-classification-data-and-get-it-ready">Make classification data and get it ready</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-and-output-shapes">Input and output shapes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turn-data-into-tensors-and-create-train-and-test-splits">Turn data into tensors and create train and test splits</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-model">Building a model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-loss-function-and-optimizer">Setup loss function and optimizer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-model">Train model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#going-from-raw-model-outputs-to-predicted-labels-logits-prediction-probabilities-prediction-labels">Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-training-and-testing-loop">Building a training and testing loop</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#make-predictions-and-evaluate-the-model">Make predictions and evaluate the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-a-model">Improving a model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-data-to-see-if-our-model-can-model-a-straight-line">Preparing data to see if our model can model a straight line</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjusting-model-1-to-fit-a-straight-line">Adjusting <code class="docutils literal notranslate"><span class="pre">model_1</span></code> to fit a straight line</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-missing-piece-non-linearity">The missing piece: non-linearity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recreating-non-linear-data-red-and-blue-circles">Recreating non-linear data (red and blue circles)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-model-with-non-linearity">Building a model with non-linearity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-model-with-non-linearity">Training a model with non-linearity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-a-model-trained-with-non-linear-activation-functions">Evaluating a model trained with non-linear activation functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#replicating-non-linear-activation-functions">Replicating non-linear activation functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-things-together">Putting things together</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-mutli-class-classification-data">Creating mutli-class classification data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-multi-class-classification-model-in-pytorch">Building a multi-class classification model in PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model">Creating a loss function and optimizer for a multi-class PyTorch model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-prediction-probabilities-for-a-multi-class-pytorch-model">Getting prediction probabilities for a multi-class PyTorch model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-training-and-testing-loop-for-a-multi-class-pytorch-model">Creating a training and testing loop for a multi-class PyTorch model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#making-and-evaluating-predictions-with-a-pytorch-multi-class-model">Making and evaluating predictions with a PyTorch multi-class model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-classification-evaluation-metrics">More classification evaluation metrics</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="pytorch-neural-network-classification">
<h1>PyTorch Neural Network Classification<a class="headerlink" href="#pytorch-neural-network-classification" title="Link to this heading">#</a></h1>
<section id="what-is-a-classification-problem">
<h2>What is a classification problem?<a class="headerlink" href="#what-is-a-classification-problem" title="Link to this heading">#</a></h2>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Statistical_classification">classification problem</a> involves predicting whether something is one thing or another.</p>
<p>For example, you might want to:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Problem type</p></th>
<th class="head"><p>What is it?</p></th>
<th class="head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Binary classification</strong></p></td>
<td><p>Target can be one of two options, e.g. yes or no</p></td>
<td><p>Predict whether or not someone has heart disease based on their health parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Multi-class classification</strong></p></td>
<td><p>Target can be one of more than two options</p></td>
<td><p>Decide whether a photo of is of food, a person or a dog.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Multi-label classification</strong></p></td>
<td><p>Target can be assigned more than one option</p></td>
<td><p>Predict what categories should be assigned to a Wikipedia article (e.g. mathematics, science &amp; philosohpy).</p></td>
</tr>
</tbody>
</table>
</div>
<img alt="various different classification in machine learning such as binary classification, multiclass classification and multilabel classification" src="../../../../../_images/02-different-classification-problems.png" />
<p>Classification, along with regression (predicting a number, covered in <a class="reference external" href="https://www.learnpytorch.io/01_pytorch_workflow/">notebook 01</a>) is one of the most common types of machine learning problems.</p>
<p>In this notebook, we‚Äôre going to work through a couple of different classification problems with PyTorch.</p>
<p>In other words, taking a set of inputs and predicting what class those set of inputs belong to.</p>
<p>In this notebook we‚Äôre going to reiterate over the PyTorch workflow</p>
<img alt="a pytorch workflow flowchart" src="../../../../../_images/01_a_pytorch_workflow.png" />
<p>Except instead of trying to predict a straight line (predicting a number, also called a regression problem), we‚Äôll be working on a <strong>classification problem</strong>.</p>
<p>Specifically, we‚Äôre going to cover:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Topic</strong></p></th>
<th class="head"><p><strong>Contents</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Architecture of a classification neural network</strong></p></td>
<td><p>Neural networks can come in almost any shape or size, but they typically follow a similar floor plan.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Getting binary classification data ready</strong></p></td>
<td><p>Data can be almost anything but to get started we‚Äôre going to create a simple binary classification dataset.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Building a PyTorch classification model</strong></p></td>
<td><p>Here we‚Äôll create a model to learn patterns in the data, we‚Äôll also choose a <strong>loss function</strong>, <strong>optimizer</strong> and build a <strong>training loop</strong> specific to classification.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Fitting the model to data (training)</strong></p></td>
<td><p>We‚Äôve got data and a model, now let‚Äôs let the model (try to) find patterns in the (<strong>training</strong>) data.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Making predictions and evaluating a model (inference)</strong></p></td>
<td><p>Our model‚Äôs found patterns in the data, let‚Äôs compare its findings to the actual (<strong>testing</strong>) data.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Improving a model (from a model perspective)</strong></p></td>
<td><p>We‚Äôve trained an evaluated a model but it‚Äôs not working, let‚Äôs try a few things to improve it.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Non-linearity</strong></p></td>
<td><p>So far our model has only had the ability to model straight lines, what about non-linear (non-straight) lines?</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Replicating non-linear functions</strong></p></td>
<td><p>We used <strong>non-linear functions</strong> to help model non-linear data, but what do these look like?</p></td>
</tr>
<tr class="row-even"><td><p><strong>Putting it all together with multi-class classification</strong></p></td>
<td><p>Let‚Äôs put everything we‚Äôve done so far for binary classification together with a multi-class classification problem.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="architecture-of-a-classification-neural-network">
<h2>Architecture of a classification neural network<a class="headerlink" href="#architecture-of-a-classification-neural-network" title="Link to this heading">#</a></h2>
<p>Before we get into writing code, let‚Äôs look at the general architecture of a classification neural network.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Hyperparameter</strong></p></th>
<th class="head"><p><strong>Binary Classification</strong></p></th>
<th class="head"><p><strong>Multiclass classification</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Input layer shape</strong> (<code class="docutils literal notranslate"><span class="pre">in_features</span></code>)</p></td>
<td><p>Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction)</p></td>
<td><p>Same as binary classification</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Hidden layer(s)</strong></p></td>
<td><p>Problem specific, minimum = 1, maximum = unlimited</p></td>
<td><p>Same as binary classification</p></td>
</tr>
<tr class="row-even"><td><p><strong>Neurons per hidden layer</strong></p></td>
<td><p>Problem specific, generally 10 to 512</p></td>
<td><p>Same as binary classification</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Output layer shape</strong> (<code class="docutils literal notranslate"><span class="pre">out_features</span></code>)</p></td>
<td><p>1 (one class or the other)</p></td>
<td><p>1 per class (e.g. 3 for food, person or dog photo)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Hidden layer activation</strong></p></td>
<td><p>Usually <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU">ReLU</a> (rectified linear unit) but <a class="reference external" href="https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions">can be many others</a></p></td>
<td><p>Same as binary classification</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Output activation</strong></p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid</a> (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html"><code class="docutils literal notranslate"><span class="pre">torch.sigmoid</span></code></a> in PyTorch)</p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">Softmax</a> (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html"><code class="docutils literal notranslate"><span class="pre">torch.softmax</span></code></a> in PyTorch)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Loss function</strong></p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression">Binary crossentropy</a> (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BCELoss</span></code></a> in PyTorch)</p></td>
<td><p>Cross entropy (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code></a> in PyTorch)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Optimizer</strong></p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">SGD</a> (stochastic gradient descent), <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">Adam</a> (see <a class="reference external" href="https://pytorch.org/docs/stable/optim.html"><code class="docutils literal notranslate"><span class="pre">torch.optim</span></code></a> for more options)</p></td>
<td><p>Same as binary classification</p></td>
</tr>
</tbody>
</table>
</div>
<p>This ingredient list of classification neural network components will vary depending on the problem you‚Äôre working on.</p>
</section>
<section id="make-classification-data-and-get-it-ready">
<h2>Make classification data and get it ready<a class="headerlink" href="#make-classification-data-and-get-it-ready" title="Link to this heading">#</a></h2>
<p>Let‚Äôs begin by making some data.</p>
<p>We‚Äôll use the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html"><code class="docutils literal notranslate"><span class="pre">make_circles()</span></code></a> method from Scikit-Learn to generate two circles with different coloured dots.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>


<span class="c1"># Make 1000 samples </span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Create circles</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span>
                    <span class="n">noise</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="c1"># a little bit of noise to the dots</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># keep random state so we get the same values</span>
</pre></div>
</div>
</div>
</div>
<p>Alright, now let‚Äôs view the first 5 <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First 5 X features:</span><span class="se">\n</span><span class="si">{</span><span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">First 5 y labels:</span><span class="se">\n</span><span class="si">{</span><span class="n">y</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>First 5 X features:
[[ 0.75424625  0.23148074]
 [-0.75615888  0.15325888]
 [-0.81539193  0.17328203]
 [-0.39373073  0.69288277]
 [ 0.44220765 -0.89672343]]

First 5 y labels:
[1 1 1 1 0]
</pre></div>
</div>
</div>
</div>
<p>Looks like there‚Äôs two <code class="docutils literal notranslate"><span class="pre">X</span></code> values per one <code class="docutils literal notranslate"><span class="pre">y</span></code> value.</p>
<p>Let‚Äôs keep following the data explorer‚Äôs motto of <em>visualize, visualize, visualize</em> and put them into a pandas DataFrame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make DataFrame of circle data</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">circles</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;X1&quot;</span><span class="p">:</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="s2">&quot;X2&quot;</span><span class="p">:</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">y</span>
<span class="p">})</span>
<span class="n">circles</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X1</th>
      <th>X2</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.754246</td>
      <td>0.231481</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.756159</td>
      <td>0.153259</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.815392</td>
      <td>0.173282</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.393731</td>
      <td>0.692883</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.442208</td>
      <td>-0.896723</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-0.479646</td>
      <td>0.676435</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-0.013648</td>
      <td>0.803349</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.771513</td>
      <td>0.147760</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>-0.169322</td>
      <td>-0.793456</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-0.121486</td>
      <td>1.021509</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>It looks like each pair of <code class="docutils literal notranslate"><span class="pre">X</span></code> features (<code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X2</span></code>) has a label (<code class="docutils literal notranslate"><span class="pre">y</span></code>) value of either 0 or 1.</p>
<p>This tells us that our problem is <strong>binary classification</strong> since there‚Äôs only two options (0 or 1).</p>
<p>How many values of each class is there?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check different labels</span>
<span class="n">circles</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>label
1    500
0    500
Name: count, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>500 each, nice and balanced.</p>
<p>Let‚Äôs plot them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize with a plot</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> 
            <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> 
            <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> 
            <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdYlBu</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../../_images/1afcf255b5e67b1411cfc8a7713547c8a811e691166a49cf1a06de987781c0e5.png" src="../../../../../_images/1afcf255b5e67b1411cfc8a7713547c8a811e691166a49cf1a06de987781c0e5.png" />
</div>
</div>
<p>Let‚Äôs find out how we could build a PyTorch neural network to classify dots into red (0) or blue (1).</p>
<blockquote>
<div><p><strong>Note:</strong> This dataset is often what‚Äôs considered a <strong>toy problem</strong> (a problem that‚Äôs used to try and test things out on) in machine learning.</p>
<p>But it represents the major key of classification, you have some kind of data represented as numerical values and you‚Äôd like to build a model that‚Äôs able to classify it, in our case, separate it into red or blue dots.</p>
</div></blockquote>
<section id="input-and-output-shapes">
<h3>Input and output shapes<a class="headerlink" href="#input-and-output-shapes" title="Link to this heading">#</a></h3>
<p>One of the most common errors in deep learning is shape errors.</p>
<p>Mismatching the shapes of tensors and tensor operations with result in errors in your models.</p>
<p>You can do instead is continually familiarize yourself with the shape of the data you‚Äôre working with.</p>
<p>Ask yourself:</p>
<p>‚ÄúWhat shapes are my inputs and what shapes are my outputs?‚Äù</p>
<p>Let‚Äôs find out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check the shapes of our features and labels</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((1000, 2), (1000,))
</pre></div>
</div>
</div>
</div>
<p>Looks like we‚Äôve got a match on the first dimension of each.</p>
<p>There‚Äôs 1000 <code class="docutils literal notranslate"><span class="pre">X</span></code> and 1000 <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<p>But what‚Äôs the second dimension on <code class="docutils literal notranslate"><span class="pre">X</span></code>?</p>
<p>It often helps to view the values and shapes of a single sample (features and labels).</p>
<p>Doing so will help you understand what input and output shapes you‚Äôd be expecting from your model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># View the first example of features and labels</span>
<span class="n">X_sample</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y_sample</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Values for one sample of X: </span><span class="si">{</span><span class="n">X_sample</span><span class="si">}</span><span class="s2"> and the same for y: </span><span class="si">{</span><span class="n">y_sample</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shapes for one sample of X: </span><span class="si">{</span><span class="n">X_sample</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> and the same for y: </span><span class="si">{</span><span class="n">y_sample</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Values for one sample of X: [0.75424625 0.23148074] and the same for y: 1
Shapes for one sample of X: (2,) and the same for y: ()
</pre></div>
</div>
</div>
</div>
<p>This tells us the second dimension for <code class="docutils literal notranslate"><span class="pre">X</span></code> means it has two features (vector) where as <code class="docutils literal notranslate"><span class="pre">y</span></code> has a single feature (scalar).</p>
<p>We have two inputs for one output.</p>
</section>
<section id="turn-data-into-tensors-and-create-train-and-test-splits">
<h3>Turn data into tensors and create train and test splits<a class="headerlink" href="#turn-data-into-tensors-and-create-train-and-test-splits" title="Link to this heading">#</a></h3>
<p>We‚Äôve investigated the input and output shapes of our data, now let‚Äôs prepare it for being used with PyTorch and for modelling.</p>
<p>Specifically, we‚Äôll need to:</p>
<ol class="arabic simple">
<li><p>Turn our data into tensors (right now our data is in NumPy arrays and PyTorch prefers to work with PyTorch tensors).</p></li>
<li><p>Split our data into training and test sets (we‚Äôll train a model on the training set to learn the patterns between <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> and then evaluate those learned patterns on the test dataset).</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Turn data into tensors</span>
<span class="c1"># Otherwise this causes issues with computations later on</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

<span class="c1"># View the first five samples</span>
<span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">y</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[ 0.7542,  0.2315],
         [-0.7562,  0.1533],
         [-0.8154,  0.1733],
         [-0.3937,  0.6929],
         [ 0.4422, -0.8967]]),
 tensor([1., 1., 1., 1., 0.]))
</pre></div>
</div>
</div>
</div>
<p>Now our data is in tensor format, let‚Äôs split it into training and test sets.</p>
<p>To do so, let‚Äôs use the helpful function <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"><code class="docutils literal notranslate"><span class="pre">train_test_split()</span></code></a> from Scikit-Learn.</p>
<p>We‚Äôll use <code class="docutils literal notranslate"><span class="pre">test_size=0.2</span></code> (80% training, 20% testing) and because the split happens randomly across the data, let‚Äôs use <code class="docutils literal notranslate"><span class="pre">random_state=42</span></code> so the split is reproducible.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split data into train and test sets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> 
                                                    <span class="n">y</span><span class="p">,</span> 
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="c1"># 20% test, 80% train</span>
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># make the random split reproducible</span>

<span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(800, 200, 800, 200)
</pre></div>
</div>
</div>
</div>
<p>Nice! Looks like we‚Äôve now got 800 training samples and 200 testing samples.</p>
</section>
</section>
<section id="building-a-model">
<h2>Building a model<a class="headerlink" href="#building-a-model" title="Link to this heading">#</a></h2>
<p>We‚Äôve got some data ready, now it‚Äôs time to build a model.</p>
<p>We‚Äôll break it down into a few parts.</p>
<ol class="arabic simple">
<li><p>Setting up device agnostic code (so our model can run on CPU or GPU if it‚Äôs available).</p></li>
<li><p>Constructing a model by subclassing <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>.</p></li>
<li><p>Defining a loss function and optimizer.</p></li>
<li><p>Creating a training loop (this‚Äôll be in the next section).</p></li>
</ol>
<p>Let‚Äôs start by importing PyTorch and <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> as well as setting up device agnostic code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Standard PyTorch imports</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Make device agnostic code</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cpu&#39;
</pre></div>
</div>
</div>
</div>
<p>Now <code class="docutils literal notranslate"><span class="pre">device</span></code> is setup, we can use it for any data or models we create and PyTorch will handle it on the CPU (default) or GPU if it‚Äôs available.</p>
<p>How about we create a model?</p>
<p>We‚Äôll want a model capable of handling our <code class="docutils literal notranslate"><span class="pre">X</span></code> data as inputs and producing something in the shape of our <code class="docutils literal notranslate"><span class="pre">y</span></code> data as ouputs.</p>
<p>In other words, given <code class="docutils literal notranslate"><span class="pre">X</span></code> (features) we want our model to predict <code class="docutils literal notranslate"><span class="pre">y</span></code> (label).</p>
<p>This setup where you have features and labels is referred to as <strong>supervised learning</strong>. Because your data is telling your model what the outputs should be given a certain input.</p>
<p>To create such a model it‚Äôll need to handle the input and output shapes of <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<p>Remember how I said input and output shapes are important? Here we‚Äôll see why.</p>
<p>Let‚Äôs create a model class that:</p>
<ol class="arabic simple">
<li><p>Subclasses <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> (almost all PyTorch models are subclasses of <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>).</p></li>
<li><p>Creates 2 <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> layers in the constructor capable of handling the input and output shapes of <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p></li>
<li><p>Defines a <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method containing the forward pass computation of the model.</p></li>
<li><p>Instantiates the model class and sends it to the target <code class="docutils literal notranslate"><span class="pre">device</span></code>.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Construct a model class that subclasses nn.Module</span>
<span class="k">class</span> <span class="nc">CircleModelV0</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># takes in 2 features (X), produces 5 features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># takes in 5 features, produces 1 feature (y)</span>
    
    <span class="c1"># 3. Define a forward method containing the forward pass computation</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Return the output of layer_2, a single feature, the same shape as y</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># computation goes through layer_1 first then the output of layer_1 goes through layer_2</span>

<span class="c1"># 4. Create an instance of the model and send it to target device</span>
<span class="n">model_0</span> <span class="o">=</span> <span class="n">CircleModelV0</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_0</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CircleModelV0(
  (layer_1): Linear(in_features=2, out_features=5, bias=True)
  (layer_2): Linear(in_features=5, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">self.layer_1</span></code> takes 2 input features <code class="docutils literal notranslate"><span class="pre">in_features=2</span></code> and produces 5 output features <code class="docutils literal notranslate"><span class="pre">out_features=5</span></code>.</p>
<p>This is known as having 5 <strong>hidden units</strong> or <strong>neurons</strong>.</p>
<p>This layer turns the input data from having 2 features to 5 features.</p>
<p>Why do this?</p>
<p>This allows the model to learn patterns from 5 numbers rather than just 2 numbers, <em>potentially</em> leading to better outputs.</p>
<p>I say potentially because sometimes it doesn‚Äôt work.</p>
<p>The number of hidden units you can use in neural network layers is a <strong>hyperparameter</strong> (a value you can set yourself) and there‚Äôs no set in stone value you have to use.</p>
<p>Generally more is better but there‚Äôs also such a thing as too much. The amount you choose will depend on your model type and dataset you‚Äôre working with.</p>
<p>Since our dataset is small and simple, we‚Äôll keep it small.</p>
<p>The only rule with hidden units is that the next layer, in our case, <code class="docutils literal notranslate"><span class="pre">self.layer_2</span></code> has to take the same <code class="docutils literal notranslate"><span class="pre">in_features</span></code> as the previous layer <code class="docutils literal notranslate"><span class="pre">out_features</span></code>.</p>
<p>That‚Äôs why <code class="docutils literal notranslate"><span class="pre">self.layer_2</span></code> has <code class="docutils literal notranslate"><span class="pre">in_features=5</span></code>, it takes the <code class="docutils literal notranslate"><span class="pre">out_features=5</span></code> from <code class="docutils literal notranslate"><span class="pre">self.layer_1</span></code> and performs a linear computation on them, turning them into <code class="docutils literal notranslate"><span class="pre">out_features=1</span></code> (the same shape as <code class="docutils literal notranslate"><span class="pre">y</span></code>).</p>
<p><img alt="A visual example of what a classification neural network with linear activation looks like on the tensorflow playground" src="../../../../../_images/02-tensorflow-playground-linear-activation.png" />
<em>A visual example of what a similar classificiation neural network to the one we‚Äôve just built looks like. Try create one of your own on the <a class="reference external" href="https://playground.tensorflow.org/">TensorFlow Playground website</a>.</em></p>
<p>You can also do the same as above using <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"><code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code></a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code> performs a forward pass computation of the input data through the layers in the order they appear.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Replicate CircleModelV0 with nn.Sequential</span>
<span class="n">model_0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">model_0</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Linear(in_features=2, out_features=5, bias=True)
  (1): Linear(in_features=5, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<p>That looks much simpler than subclassing <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>, why not just always use <code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code>?</p>
<p><code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code> is fantastic for straight-forward computations, however, as the namespace says, it <em>always</em> runs in sequential order.</p>
<p>So if you‚Äôd something else to happen (rather than just straight-forward sequential computation) you‚Äôll want to define your own custom <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> subclass.</p>
<p>Now we‚Äôve got a model, let‚Äôs see what happens when we pass some data through it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make predictions with the model</span>
<span class="n">untrained_preds</span> <span class="o">=</span> <span class="n">model_0</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Length of predictions: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">untrained_preds</span><span class="p">)</span><span class="si">}</span><span class="s2">, Shape: </span><span class="si">{</span><span class="n">untrained_preds</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Length of test samples: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s2">, Shape: </span><span class="si">{</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">First 10 predictions:</span><span class="se">\n</span><span class="si">{</span><span class="n">untrained_preds</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">First 10 test labels:</span><span class="se">\n</span><span class="si">{</span><span class="n">y_test</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Length of predictions: 200, Shape: torch.Size([200, 1])
Length of test samples: 200, Shape: torch.Size([200])

First 10 predictions:
tensor([[ 0.3463],
        [ 0.3017],
        [ 0.2367],
        [ 0.3783],
        [-0.1231],
        [-0.1475],
        [ 0.0119],
        [-0.0942],
        [ 0.2520],
        [ 0.2947]], grad_fn=&lt;SliceBackward0&gt;)

First 10 test labels:
tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])
</pre></div>
</div>
</div>
</div>
<p>It seems there‚Äôs the same amount of predictions as there is test labels but the predictions don‚Äôt look like they‚Äôre in the same form or shape as the test labels.</p>
<p>We‚Äôve got a couple steps we can do to fix this, we‚Äôll see these later on.</p>
<section id="setup-loss-function-and-optimizer">
<h3>Setup loss function and optimizer<a class="headerlink" href="#setup-loss-function-and-optimizer" title="Link to this heading">#</a></h3>
<p>We‚Äôve setup a loss (also called a criterion or cost function) and optimizer.</p>
<p>But different problem types require different loss functions.</p>
<p>For example, for a regression problem (predicting a number) you might used mean absolute error (MAE) loss.</p>
<p>And for a binary classification problem (like ours), you‚Äôll often use <a class="reference external" href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a">binary cross entropy</a> as the loss function.</p>
<p>However, the same optimizer function can often be used across different problem spaces.</p>
<p>For example, the stochastic gradient descent optimizer (SGD, <code class="docutils literal notranslate"><span class="pre">torch.optim.SGD()</span></code>) can be used for a range of problems, and the same applies to the Adam optimizer (<code class="docutils literal notranslate"><span class="pre">torch.optim.Adam()</span></code>).</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Loss function/Optimizer</p></th>
<th class="head"><p>Problem type</p></th>
<th class="head"><p>PyTorch Code</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Stochastic Gradient Descent (SGD) optimizer</p></td>
<td><p>Classification, regression, many others.</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html"><code class="docutils literal notranslate"><span class="pre">torch.optim.SGD()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p>Adam Optimizer</p></td>
<td><p>Classification, regression, many others.</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"><code class="docutils literal notranslate"><span class="pre">torch.optim.Adam()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p>Binary cross entropy loss</p></td>
<td><p>Binary classification</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BCELossWithLogits</span></code></a> or <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BCELoss</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p>Cross entropy loss</p></td>
<td><p>Mutli-class classification</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p>Mean absolute error (MAE) or L1 Loss</p></td>
<td><p>Regression</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.L1Loss</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p>Mean squared error (MSE) or L2 Loss</p></td>
<td><p>Regression</p></td>
<td><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss"><code class="docutils literal notranslate"><span class="pre">torch.nn.MSELoss</span></code></a></p></td>
</tr>
</tbody>
</table>
</div>
<p><em>Table of various loss functions and optimizers, there are more but these some common ones you‚Äôll see.</em></p>
<p>Since we‚Äôre working with a binary classification problem, let‚Äôs use a binary cross entropy loss function.</p>
<blockquote>
<div><p><strong>Note:</strong> Recall a <strong>loss function</strong> is what measures how <em>wrong</em> your model predictions are, the higher the loss, the worse your model.</p>
<p>Also, PyTorch documentation often refers to loss functions as ‚Äúloss criterion‚Äù or ‚Äúcriterion‚Äù, these are all different ways of describing the same thing.</p>
</div></blockquote>
<p>PyTorch has two binary cross entropy implementations:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BCELoss()</span></code></a> - Creates a loss function that measures the binary cross entropy between the target (label) and input (features).</p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.BCEWithLogitsLoss()</span></code></a> - This is the same as above except it has a sigmoid layer (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html"><code class="docutils literal notranslate"><span class="pre">nn.Sigmoid</span></code></a>) built-in (we‚Äôll see what this means soon).</p></li>
</ol>
<p>Which one should you use?</p>
<p>The <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html">documentation for <code class="docutils literal notranslate"><span class="pre">torch.nn.BCEWithLogitsLoss()</span></code></a> states that it‚Äôs more numerically stable than using <code class="docutils literal notranslate"><span class="pre">torch.nn.BCELoss()</span></code> after a <code class="docutils literal notranslate"><span class="pre">nn.Sigmoid</span></code> layer.</p>
<p>So generally, implementation 2 is a better option. However for advanced usage, you may want to separate the combination of <code class="docutils literal notranslate"><span class="pre">nn.Sigmoid</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.nn.BCELoss()</span></code> but that is beyond the scope of this notebook.</p>
<p>Knowing this, let‚Äôs create a loss function and an optimizer.</p>
<p>For the optimizer we‚Äôll use <code class="docutils literal notranslate"><span class="pre">torch.optim.SGD()</span></code> to optimize the model parameters with learning rate 0.1.</p>
<blockquote>
<div><p><strong>Note:</strong> There‚Äôs a <a class="reference external" href="https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586/4">discussion on the PyTorch forums about the use of <code class="docutils literal notranslate"><span class="pre">nn.BCELoss</span></code> vs. <code class="docutils literal notranslate"><span class="pre">nn.BCEWithLogitsLoss</span></code></a>. It can be confusing at first but as with many things, it becomes easier with practice.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a loss function</span>
<span class="c1"># loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span> <span class="c1"># BCEWithLogitsLoss = sigmoid built-in</span>

<span class="c1"># Create an optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model_0</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> 
                            <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let‚Äôs also create an <strong>evaluation metric</strong>.</p>
<p>An evaluation metric can be used to offer another perspective on how your model is going.</p>
<p>If a loss function measures how <em>wrong</em> your model is, I like to think of evaluation metrics as measuring how <em>right</em> it is.</p>
<p>Of course, you could argue both of these are doing the same thing but evaluation metrics offer a different perspective.</p>
<p>After all, when evaluating your models it‚Äôs good to look at things from multiple points of view.</p>
<p>There are several evaluation metrics that can be used for classification problems but let‚Äôs start out with <strong>accuracy</strong>.</p>
<p>Accuracy can be measured by dividing the total number of correct predictions over the total number of predictions.</p>
<p>For example, a model that makes 99 correct predictions out of 100 will have an accuracy of 99%.</p>
<p>Let‚Äôs write a function to do so.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate accuracy (a classification metric)</span>
<span class="k">def</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># torch.eq() calculates where two tensors are equal</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span> 
    <span class="k">return</span> <span class="n">acc</span>
</pre></div>
</div>
</div>
</div>
<p>Excellent! We can now use this function whilst training our model to measure it‚Äôs performance alongside the loss.</p>
</section>
</section>
<section id="train-model">
<h2>Train model<a class="headerlink" href="#train-model" title="Link to this heading">#</a></h2>
<p>Okay, now we‚Äôve got a loss function and optimizer ready to go, let‚Äôs train a model.</p>
<p>Let‚Äôs understand step in training</p>
<p>Steps in training:</p>
<details>
    <summary>PyTorch training loop steps</summary>
    <ol>
        <li><b>Forward pass</b> - The model goes through all of the training data once, performing its
            <code>forward()</code> function
            calculations (<code>model(x_train)</code>).
        </li>
        <li><b>Calculate the loss</b> - The model's outputs (predictions) are compared to the ground truth and evaluated
            to see how
            wrong they are (<code>loss = loss_fn(y_pred, y_train</code>).</li>
        <li><b>Zero gradients</b> - The optimizers gradients are set to zero (they are accumulated by default) so they
            can be
            recalculated for the specific training step (<code>optimizer.zero_grad()</code>).</li>
        <li><b>Perform backpropagation on the loss</b> - Computes the gradient of the loss with respect for every model
            parameter to
            be updated (each parameter
            with <code>requires_grad=True</code>). This is known as <b>backpropagation</b>, hence "backwards"
            (<code>loss.backward()</code>).</li>
        <li><b>Step the optimizer (gradient descent)</b> - Update the parameters with <code>requires_grad=True</code>
            with respect to the loss
            gradients in order to improve them (<code>optimizer.step()</code>).</li>
    </ol>
</details>
<section id="going-from-raw-model-outputs-to-predicted-labels-logits-prediction-probabilities-prediction-labels">
<h3>Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels)<a class="headerlink" href="#going-from-raw-model-outputs-to-predicted-labels-logits-prediction-probabilities-prediction-labels" title="Link to this heading">#</a></h3>
<p>Before the training loop steps, let‚Äôs see what comes out of our model during the forward pass (the forward pass is defined by the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method).</p>
<p>To do so, let‚Äôs pass the model some data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># View the frist 5 outputs of the forward pass on the test data</span>
<span class="n">y_logits</span> <span class="o">=</span> <span class="n">model_0</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))[:</span><span class="mi">5</span><span class="p">]</span>
<span class="n">y_logits</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.3463],
        [ 0.3017],
        [ 0.2367],
        [ 0.3783],
        [-0.1231]], grad_fn=&lt;SliceBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Since our model hasn‚Äôt been trained, these outputs are basically random.</p>
<p>But <em>what</em> are they?</p>
<p>They‚Äôre the output of our <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method.</p>
<p>Which implements two layers of <code class="docutils literal notranslate"><span class="pre">nn.Linear()</span></code> which internally calls the following equation:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y} = x \cdot \mathbf{Weights}^T  + \mathbf{bias}
\]</div>
<p>The <em>raw outputs</em> (unmodified) of this equation (<span class="math notranslate nohighlight">\(\mathbf{y}\)</span>) and in turn, the raw outputs of our model are often referred to as <a class="reference external" href="https://datascience.stackexchange.com/a/31045"><strong>logits</strong></a>.</p>
<p>That‚Äôs what our model is outputing above when it takes in the input data (<span class="math notranslate nohighlight">\(x\)</span> in the equation or <code class="docutils literal notranslate"><span class="pre">X_test</span></code> in the code), logits.</p>
<p>However, these numbers are hard to interpret.</p>
<p>We‚Äôd like some numbers that are comparable to our truth labels.</p>
<p>To get our model‚Äôs raw outputs (logits) into such a form, we can use the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html">sigmoid activation function</a>.</p>
<p>Let‚Äôs try it out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use sigmoid on model logits</span>
<span class="n">y_pred_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y_logits</span><span class="p">)</span>
<span class="n">y_pred_probs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.3946],
        [0.4154],
        [0.3549],
        [0.4061],
        [0.3757]], device=&#39;cuda:0&#39;, grad_fn=&lt;SigmoidBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Okay, it seems like the outputs now have some kind of consistency (even though they‚Äôre still random).</p>
<p>They‚Äôre now in the form of <strong>prediction probabilities</strong> (I usually refer to these as <code class="docutils literal notranslate"><span class="pre">y_pred_probs</span></code>), in other words, the values are now how much the model thinks the data point belongs to one class or another.</p>
<p>In our case, since we‚Äôre dealing with binary classification, our ideal outputs are 0 or 1.</p>
<p>So these values can be viewed as a decision boundary.</p>
<p>The closer to 0, the more the model thinks the sample belongs to class 0, the closer to 1, the more the model thinks the sample belongs to class 1.</p>
<p>More specificially:</p>
<ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">y_pred_probs</span></code> &gt;= 0.5, <code class="docutils literal notranslate"><span class="pre">y=1</span></code> (class 1)</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">y_pred_probs</span></code> &lt; 0.5, <code class="docutils literal notranslate"><span class="pre">y=0</span></code> (class 0)</p></li>
</ul>
<p>To turn our prediction probabilities in prediction labels, we can round the outputs of the sigmoid activation function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Find the predicted labels (round the prediction probabilities)</span>
<span class="n">y_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">y_pred_probs</span><span class="p">)</span>

<span class="c1"># In full</span>
<span class="n">y_pred_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">model_0</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))[:</span><span class="mi">5</span><span class="p">]))</span>

<span class="c1"># Check for equality</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">y_preds</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_pred_labels</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()))</span>

<span class="c1"># Get rid of extra dimension</span>
<span class="n">y_preds</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([True, True, True, True, True], device=&#39;cuda:0&#39;)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 0., 0., 0., 0.], device=&#39;cuda:0&#39;, grad_fn=&lt;SqueezeBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Now it looks like our model‚Äôs predictions are in the same form as our truth labels (<code class="docutils literal notranslate"><span class="pre">y_test</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_test</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1., 0., 1., 0., 1.])
</pre></div>
</div>
</div>
</div>
<p>This means we‚Äôll be able to compare our models predictions to the test labels to see how well it‚Äôs going.</p>
<p>To recap, we converted our model‚Äôs raw outputs (logits) to predicition probabilities using a sigmoid activation function.</p>
<p>And then converted the prediction probabilities to prediction labels by rounding them.</p>
<blockquote>
<div><p><strong>Note:</strong> The use of the sigmoid activation function is often only for binary classification logits. For multi-class classification, we‚Äôll be looking at using the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html">softmax activation function</a> (this will come later on).</p>
<p>And the use of the sigmoid activation function is not required when passing our model‚Äôs raw outputs to the <code class="docutils literal notranslate"><span class="pre">nn.BCEWithLogitsLoss</span></code> (the ‚Äúlogits‚Äù in logits loss is because it works on the model‚Äôs raw logits output), this is because it has a sigmoid function built-in.</p>
</div></blockquote>
</section>
<section id="building-a-training-and-testing-loop">
<h3>Building a training and testing loop<a class="headerlink" href="#building-a-training-and-testing-loop" title="Link to this heading">#</a></h3>
<p>Alright, we‚Äôve discussed how to take our raw model outputs and convert them to prediction labels, now let‚Äôs build a training loop.</p>
<p>Let‚Äôs start by training for 100 epochs and outputing the model‚Äôs progress every 10 epochs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Set the number of epochs</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Put data to target device</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Build training and evaluation loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1">### Training</span>
    <span class="n">model_0</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="c1"># 1. Forward pass (model outputs raw logits)</span>
    <span class="n">y_logits</span> <span class="o">=</span> <span class="n">model_0</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="c1"># squeeze to remove extra `1` dimensions, this won&#39;t work unless model and data are on same device </span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y_logits</span><span class="p">))</span> <span class="c1"># turn logits -&gt; pred probs -&gt; pred labls</span>
  
    <span class="c1"># 2. Calculate loss/accuracy</span>
    <span class="c1"># loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()</span>
    <span class="c1">#                y_train) </span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_logits</span><span class="p">,</span> <span class="c1"># Using nn.BCEWithLogitsLoss works with raw logits</span>
                   <span class="n">y_train</span><span class="p">)</span> 
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> 
                      <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span> 

    <span class="c1"># 3. Optimizer zero grad</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 4. Loss backwards</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 5. Optimizer step</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1">### Testing</span>
    <span class="n">model_0</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
        <span class="c1"># 1. Forward pass</span>
        <span class="n">test_logits</span> <span class="o">=</span> <span class="n">model_0</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> 
        <span class="n">test_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">test_logits</span><span class="p">))</span>
        <span class="c1"># 2. Caculate loss/accuracy</span>
        <span class="n">test_loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">test_logits</span><span class="p">,</span>
                            <span class="n">y_test</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
                               <span class="n">y_pred</span><span class="o">=</span><span class="n">test_pred</span><span class="p">)</span>

    <span class="c1"># Print out what&#39;s happening every 10 epochs</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> | Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, Accuracy: </span><span class="si">{</span><span class="n">acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% | Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, Test acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 0 | Loss: 0.72090, Accuracy: 50.00% | Test loss: 0.72196, Test acc: 50.00%
Epoch: 10 | Loss: 0.70291, Accuracy: 50.00% | Test loss: 0.70542, Test acc: 50.00%
Epoch: 20 | Loss: 0.69659, Accuracy: 50.00% | Test loss: 0.69942, Test acc: 50.00%
Epoch: 30 | Loss: 0.69432, Accuracy: 43.25% | Test loss: 0.69714, Test acc: 41.00%
Epoch: 40 | Loss: 0.69349, Accuracy: 47.00% | Test loss: 0.69623, Test acc: 46.50%
Epoch: 50 | Loss: 0.69319, Accuracy: 49.00% | Test loss: 0.69583, Test acc: 46.00%
Epoch: 60 | Loss: 0.69308, Accuracy: 50.12% | Test loss: 0.69563, Test acc: 46.50%
Epoch: 70 | Loss: 0.69303, Accuracy: 50.38% | Test loss: 0.69551, Test acc: 46.00%
Epoch: 80 | Loss: 0.69302, Accuracy: 51.00% | Test loss: 0.69543, Test acc: 46.00%
Epoch: 90 | Loss: 0.69301, Accuracy: 51.00% | Test loss: 0.69537, Test acc: 46.00%
</pre></div>
</div>
</div>
</div>
<p>It looks like it went through the training and testing steps fine but the results don‚Äôt seem to have moved too much.</p>
<p>The accuracy barely moves above 50% on each data split.</p>
<p>And because we‚Äôre working with a balanced binary classification problem, it means our model is performing as good as random guessing (with 500 samples of class 0 and class 1 a model predicting class 1 every single time would achieve 50% accuracy).</p>
</section>
</section>
<section id="make-predictions-and-evaluate-the-model">
<h2>Make predictions and evaluate the model<a class="headerlink" href="#make-predictions-and-evaluate-the-model" title="Link to this heading">#</a></h2>
<p>From the metrics it looks like our model is random guessing.</p>
<p>How could we investigate this further?</p>
<p>The data explorer‚Äôs motto!</p>
<p>‚ÄúVisualize, visualize, visualize!‚Äù</p>
<p>Let‚Äôs make a plot of our model‚Äôs predictions, the data it‚Äôs trying to predict on and the decision boundary it‚Äôs creating for whether something is class 0 or class 1.</p>
<p>To do so, we‚Äôll write some code to download and import the <a class="reference download internal" download="" href="../../../../../_downloads/925748efe9ebc3b1352789c78ee64836/helper_functions.py"><span class="xref download myst"><code class="docutils literal notranslate"><span class="pre">helper_functions.py</span></code> script</span></a></p>
<p>It contains a helpful function called <code class="docutils literal notranslate"><span class="pre">plot_decision_boundary()</span></code> which creates a NumPy meshgrid to visually plot the different points where our model is predicting certain classes.</p>
<p>We‚Äôll also import <code class="docutils literal notranslate"><span class="pre">plot_predictions()</span></code> which we wrote in notebook 01 to use later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span> 

<span class="c1"># Download helper functions from Learn PyTorch repo (if not already downloaded)</span>
<span class="k">if</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;helper_functions.py&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;helper_functions.py already exists, skipping download&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Downloading helper_functions.py&quot;</span><span class="p">)</span>
  <span class="n">request</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;helper_functions.py&quot;</span><span class="p">)</span>
  <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;helper_functions.py&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">helper_functions</span> <span class="kn">import</span> <span class="n">plot_predictions</span><span class="p">,</span> <span class="n">plot_decision_boundary</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot decision boundaries for training and test sets</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Train&quot;</span><span class="p">)</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">model_0</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Test&quot;</span><span class="p">)</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">model_0</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../../_images/c0fa864c462af719d37ef741119257304da2c2031fae607d72fea53898cf75b0.png" src="../../../../../_images/c0fa864c462af719d37ef741119257304da2c2031fae607d72fea53898cf75b0.png" />
</div>
</div>
<p>Seems like we‚Äôve found the cause of model‚Äôs performance issue.</p>
<p>It‚Äôs currently trying to split the red and blue dots using a straight line‚Ä¶</p>
<p>That explains the 50% accuracy. Since our data is circular, drawing a straight line can at best cut it down the middle.</p>
<p>In machine learning terms, our model is <strong>underfitting</strong>, meaning it‚Äôs not learning predictive patterns from the data.</p>
<p>How could we improve this?</p>
</section>
<section id="improving-a-model">
<h2>Improving a model<a class="headerlink" href="#improving-a-model" title="Link to this heading">#</a></h2>
<p>Let‚Äôs try to fix our model‚Äôs underfitting problem.</p>
<p>Focusing specifically on the model (not the data), there are a few ways we could do this.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model improvement technique*</p></th>
<th class="head"><p>What does it do?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Add more layers</strong></p></td>
<td><p>Each layer <em>potentially</em> increases the learning capabilities of the model with each layer being able to learn some kind of new pattern in the data, more layers is often referred to as making your neural network <em>deeper</em>.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Add more hidden units</strong></p></td>
<td><p>Similar to the above, more hidden units per layer means a <em>potential</em> increase in learning capabilities of the model, more hidden units is often referred to as making your neural network <em>wider</em>.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Fitting for longer (more epochs)</strong></p></td>
<td><p>Your model might learn more if it had more opportunities to look at the data.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Changing the activation functions</strong></p></td>
<td><p>Some data just can‚Äôt be fit with only straight lines (like what we‚Äôve seen), using non-linear activation functions can help with this (hint, hint).</p></td>
</tr>
<tr class="row-even"><td><p><strong>Change the learning rate</strong></p></td>
<td><p>Less model specific, but still related, the learning rate of the optimizer decides how much a model should change its parameters each step, too much and the model overcorrects, too little and it doesn‚Äôt learn enough.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Change the loss function</strong></p></td>
<td><p>Again, less model specific but still important, different problems require different loss functions. For example, a binary cross entropy loss function won‚Äôt work with a multi-class classification problem.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Use transfer learning</strong></p></td>
<td><p>Take a pretrained model from a problem domain similar to yours and adjust it to your own problem. We cover transfer learning in <a class="reference internal" href="06_pytorch_transfer_learning.html"><span class="std std-doc">Pytorch transfer learning</span></a>.</p></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<div><p><strong>Note:</strong> *because you can adjust all of these by hand, they‚Äôre referred to as <strong>hyperparameters</strong>.</p>
<p>And this is also where machine learning‚Äôs half art half science comes in, there‚Äôs no real way to know here what the best combination of values is for your project, best to follow the data scientist‚Äôs motto of ‚Äúexperiment, experiment, experiment‚Äù.</p>
</div></blockquote>
<p>Let‚Äôs see what happens if we add an extra layer to our model, fit for longer (<code class="docutils literal notranslate"><span class="pre">epochs=1000</span></code> instead of <code class="docutils literal notranslate"><span class="pre">epochs=100</span></code>) and increase the number of hidden units from <code class="docutils literal notranslate"><span class="pre">5</span></code> to <code class="docutils literal notranslate"><span class="pre">10</span></code>.</p>
<p>We‚Äôll follow the same steps we did above but with a few changed hyperparameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CircleModelV1</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># extra layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="c1"># note: always make sure forward is spelt correctly!</span>
        <span class="c1"># Creating a model like this is the same as below, though below</span>
        <span class="c1"># generally benefits from speedups where possible.</span>
        <span class="c1"># z = self.layer_1(x)</span>
        <span class="c1"># z = self.layer_2(z)</span>
        <span class="c1"># z = self.layer_3(z)</span>
        <span class="c1"># return z</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<span class="n">model_1</span> <span class="o">=</span> <span class="n">CircleModelV1</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CircleModelV1(
  (layer_1): Linear(in_features=2, out_features=10, bias=True)
  (layer_2): Linear(in_features=10, out_features=10, bias=True)
  (layer_3): Linear(in_features=10, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<p>Now we‚Äôve got a model, we‚Äôll recreate a loss function and optimizer instance, using the same settings as before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># loss_fn = nn.BCELoss() # Requires sigmoid on input</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span> <span class="c1"># Does not require sigmoid on input</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_1</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Our model, optimizer and loss function ready, let‚Äôs make a training loop.</p>
<p>This time we‚Äôll train for longer (<code class="docutils literal notranslate"><span class="pre">epochs=1000</span></code> vs <code class="docutils literal notranslate"><span class="pre">epochs=100</span></code>) and see if it improves our model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1"># Train for longer</span>

<span class="c1"># Put data to target device</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1">### Training</span>
    <span class="c1"># 1. Forward pass</span>
    <span class="n">y_logits</span> <span class="o">=</span> <span class="n">model_1</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y_logits</span><span class="p">))</span> <span class="c1"># logits -&gt; predicition probabilities -&gt; prediction labels</span>

    <span class="c1"># 2. Calculate loss/accuracy</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_logits</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> 
                      <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>

    <span class="c1"># 3. Optimizer zero grad</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 4. Loss backwards</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 5. Optimizer step</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1">### Testing</span>
    <span class="n">model_1</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
        <span class="c1"># 1. Forward pass</span>
        <span class="n">test_logits</span> <span class="o">=</span> <span class="n">model_1</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> 
        <span class="n">test_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">test_logits</span><span class="p">))</span>
        <span class="c1"># 2. Caculate loss/accuracy</span>
        <span class="n">test_loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">test_logits</span><span class="p">,</span>
                            <span class="n">y_test</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
                               <span class="n">y_pred</span><span class="o">=</span><span class="n">test_pred</span><span class="p">)</span>

    <span class="c1"># Print out what&#39;s happening every 10 epochs</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> | Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, Accuracy: </span><span class="si">{</span><span class="n">acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% | Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, Test acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 0 | Loss: 0.69396, Accuracy: 50.88% | Test loss: 0.69261, Test acc: 51.00%
Epoch: 100 | Loss: 0.69305, Accuracy: 50.38% | Test loss: 0.69379, Test acc: 48.00%
Epoch: 200 | Loss: 0.69299, Accuracy: 51.12% | Test loss: 0.69437, Test acc: 46.00%
Epoch: 300 | Loss: 0.69298, Accuracy: 51.62% | Test loss: 0.69458, Test acc: 45.00%
Epoch: 400 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69465, Test acc: 46.00%
Epoch: 500 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69467, Test acc: 46.00%
Epoch: 600 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%
Epoch: 700 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%
Epoch: 800 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%
Epoch: 900 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%
</pre></div>
</div>
</div>
</div>
<p>Our model trained for longer and with an extra layer but it still looks like it didn‚Äôt learn any patterns better than random guessing.</p>
<p>Let‚Äôs visualize.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot decision boundaries for training and test sets</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Train&quot;</span><span class="p">)</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">model_1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Test&quot;</span><span class="p">)</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">model_1</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../../_images/25175489382a2c5bf35d73960e3b02bc524cd522df58eeaa22c26d724e6f5eb3.png" src="../../../../../_images/25175489382a2c5bf35d73960e3b02bc524cd522df58eeaa22c26d724e6f5eb3.png" />
</div>
</div>
<p>Our model is still drawing a straight line between the red and blue dots.</p>
<p>If our model is drawing a straight line, could it model linear data? Like we did in <a class="reference external" href="https://www.learnpytorch.io/01_pytorch_workflow/">notebook 01</a>?</p>
<section id="preparing-data-to-see-if-our-model-can-model-a-straight-line">
<h3>Preparing data to see if our model can model a straight line<a class="headerlink" href="#preparing-data-to-see-if-our-model-can-model-a-straight-line" title="Link to this heading">#</a></h3>
<p>Let‚Äôs create some linear data to see if our model‚Äôs able to model it and we‚Äôre not just using a model that can‚Äôt learn anything.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create some data (same as notebook 01)</span>
<span class="n">weight</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">bias</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">end</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">step</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="c1"># Create data</span>
<span class="n">X_regression</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_regression</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">X_regression</span> <span class="o">+</span> <span class="n">bias</span> <span class="c1"># linear regression formula</span>

<span class="c1"># Check the data</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_regression</span><span class="p">))</span>
<span class="n">X_regression</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">y_regression</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[0.0000],
         [0.0100],
         [0.0200],
         [0.0300],
         [0.0400]]),
 tensor([[0.3000],
         [0.3070],
         [0.3140],
         [0.3210],
         [0.3280]]))
</pre></div>
</div>
</div>
</div>
<p>Wonderful, now let‚Äôs split our data into training and test sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create train and test splits</span>
<span class="n">train_split</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_regression</span><span class="p">))</span> <span class="c1"># 80% of data used for training set</span>
<span class="n">X_train_regression</span><span class="p">,</span> <span class="n">y_train_regression</span> <span class="o">=</span> <span class="n">X_regression</span><span class="p">[:</span><span class="n">train_split</span><span class="p">],</span> <span class="n">y_regression</span><span class="p">[:</span><span class="n">train_split</span><span class="p">]</span>
<span class="n">X_test_regression</span><span class="p">,</span> <span class="n">y_test_regression</span> <span class="o">=</span> <span class="n">X_regression</span><span class="p">[</span><span class="n">train_split</span><span class="p">:],</span> <span class="n">y_regression</span><span class="p">[</span><span class="n">train_split</span><span class="p">:]</span>

<span class="c1"># Check the lengths of each split</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train_regression</span><span class="p">),</span> 
    <span class="nb">len</span><span class="p">(</span><span class="n">y_train_regression</span><span class="p">),</span> 
    <span class="nb">len</span><span class="p">(</span><span class="n">X_test_regression</span><span class="p">),</span> 
    <span class="nb">len</span><span class="p">(</span><span class="n">y_test_regression</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>80 80 20 20
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs see how the data looks.</p>
<p>To do so, we‚Äôll use the <code class="docutils literal notranslate"><span class="pre">plot_predictions()</span></code> function we created in notebook 01.</p>
<p>It‚Äôs contained within the <a class="reference download internal" download="" href="../../../../../_downloads/925748efe9ebc3b1352789c78ee64836/helper_functions.py"><span class="xref download myst"><code class="docutils literal notranslate"><span class="pre">helper_functions.py</span></code> script</span></a> on the Learn PyTorch for Deep Learning repo which we downloaded above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_predictions</span><span class="p">(</span><span class="n">train_data</span><span class="o">=</span><span class="n">X_train_regression</span><span class="p">,</span>
    <span class="n">train_labels</span><span class="o">=</span><span class="n">y_train_regression</span><span class="p">,</span>
    <span class="n">test_data</span><span class="o">=</span><span class="n">X_test_regression</span><span class="p">,</span>
    <span class="n">test_labels</span><span class="o">=</span><span class="n">y_test_regression</span>
<span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../../_images/cc8c55e81c8f0bf0fd5584d8763713971ac7ae3bab5a7d7b1fa3b0be53b45de1.png" src="../../../../../_images/cc8c55e81c8f0bf0fd5584d8763713971ac7ae3bab5a7d7b1fa3b0be53b45de1.png" />
</div>
</div>
</section>
<section id="adjusting-model-1-to-fit-a-straight-line">
<h3>Adjusting <code class="docutils literal notranslate"><span class="pre">model_1</span></code> to fit a straight line<a class="headerlink" href="#adjusting-model-1-to-fit-a-straight-line" title="Link to this heading">#</a></h3>
<p>Now we‚Äôve got some data, let‚Äôs recreate <code class="docutils literal notranslate"><span class="pre">model_1</span></code> but with a loss function suited to our regression data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Same architecture as model_1 (but using nn.Sequential)</span>
<span class="n">model_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">model_2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Linear(in_features=1, out_features=10, bias=True)
  (1): Linear(in_features=10, out_features=10, bias=True)
  (2): Linear(in_features=10, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<p>We‚Äôll setup the loss function to be <code class="docutils literal notranslate"><span class="pre">nn.L1Loss()</span></code> (the same as mean absolute error) and the optimizer to be <code class="docutils literal notranslate"><span class="pre">torch.optim.SGD()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Loss and optimizer</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_2</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let‚Äôs train the model using the regular training loop steps for <code class="docutils literal notranslate"><span class="pre">epochs=1000</span></code> (just like <code class="docutils literal notranslate"><span class="pre">model_1</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the model</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Set the number of epochs</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Put data to target device</span>
<span class="n">X_train_regression</span><span class="p">,</span> <span class="n">y_train_regression</span> <span class="o">=</span> <span class="n">X_train_regression</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train_regression</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_test_regression</span><span class="p">,</span> <span class="n">y_test_regression</span> <span class="o">=</span> <span class="n">X_test_regression</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_test_regression</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1">### Training </span>
    <span class="c1"># 1. Forward pass</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model_2</span><span class="p">(</span><span class="n">X_train_regression</span><span class="p">)</span>
    
    <span class="c1"># 2. Calculate loss (no accuracy since it&#39;s a regression problem, not classification)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_train_regression</span><span class="p">)</span>

    <span class="c1"># 3. Optimizer zero grad</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 4. Loss backwards</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 5. Optimizer step</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1">### Testing</span>
    <span class="n">model_2</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
      <span class="c1"># 1. Forward pass</span>
      <span class="n">test_pred</span> <span class="o">=</span> <span class="n">model_2</span><span class="p">(</span><span class="n">X_test_regression</span><span class="p">)</span>
      <span class="c1"># 2. Calculate the loss </span>
      <span class="n">test_loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">test_pred</span><span class="p">,</span> <span class="n">y_test_regression</span><span class="p">)</span>

    <span class="c1"># Print out what&#39;s happening</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> 
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> | Train loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 0 | Train loss: 0.75986, Test loss: 0.54143
Epoch: 100 | Train loss: 0.09309, Test loss: 0.02901
Epoch: 200 | Train loss: 0.07376, Test loss: 0.02850
Epoch: 300 | Train loss: 0.06745, Test loss: 0.00615
Epoch: 400 | Train loss: 0.06107, Test loss: 0.02004
Epoch: 500 | Train loss: 0.05698, Test loss: 0.01061
Epoch: 600 | Train loss: 0.04857, Test loss: 0.01326
Epoch: 700 | Train loss: 0.06109, Test loss: 0.02127
Epoch: 800 | Train loss: 0.05599, Test loss: 0.01426
Epoch: 900 | Train loss: 0.05571, Test loss: 0.00603
</pre></div>
</div>
</div>
</div>
<p>Unlike <code class="docutils literal notranslate"><span class="pre">model_1</span></code> on the classification data, it looks like <code class="docutils literal notranslate"><span class="pre">model_2</span></code>‚Äôs loss is actually going down.</p>
<p>Let‚Äôs plot its predictions to see if that‚Äôs so.</p>
<p>And remember, since our model and data are using the target <code class="docutils literal notranslate"><span class="pre">device</span></code>, and this device may be a GPU, however, our plotting function uses matplotlib and matplotlib can‚Äôt handle data on the GPU.</p>
<p>To handle that, we‚Äôll send all of our data to the CPU using <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html"><code class="docutils literal notranslate"><span class="pre">.cpu()</span></code></a> when we pass it to <code class="docutils literal notranslate"><span class="pre">plot_predictions()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Turn on evaluation mode</span>
<span class="n">model_2</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Make predictions (inference)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">y_preds</span> <span class="o">=</span> <span class="n">model_2</span><span class="p">(</span><span class="n">X_test_regression</span><span class="p">)</span>

<span class="c1"># Plot data and predictions with data on the CPU (matplotlib can&#39;t handle data on the GPU)</span>
<span class="c1"># (try removing .cpu() from one of the below and see what happens)</span>
<span class="n">plot_predictions</span><span class="p">(</span><span class="n">train_data</span><span class="o">=</span><span class="n">X_train_regression</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span>
                 <span class="n">train_labels</span><span class="o">=</span><span class="n">y_train_regression</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span>
                 <span class="n">test_data</span><span class="o">=</span><span class="n">X_test_regression</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span>
                 <span class="n">test_labels</span><span class="o">=</span><span class="n">y_test_regression</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span>
                 <span class="n">predictions</span><span class="o">=</span><span class="n">y_preds</span><span class="o">.</span><span class="n">cpu</span><span class="p">());</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../../_images/473ab078b33141751f59d9b75995ce7243492685879cd9e01856b62d29fd0394.png" src="../../../../../_images/473ab078b33141751f59d9b75995ce7243492685879cd9e01856b62d29fd0394.png" />
</div>
</div>
<p>Alright, it looks like our model is able to do far better than random guessing on straight lines.</p>
<p>It means our model at least has <em>some</em> capacity to learn.</p>
<blockquote>
<div><p><strong>Note:</strong> A helpful troubleshooting step when building deep learning models is to start as small as possible to see if the model works before scaling it up.</p>
<p>This could mean starting with a simple neural network (not many layers, not many hidden neurons) and a small dataset (like the one we‚Äôve made) and then <strong>overfitting</strong> (making the model perform too well) on that small example before increasing the amount data or the model size/design to <em>reduce</em> overfitting.</p>
</div></blockquote>
<p>So what could it be?</p>
<p>Let‚Äôs find out.</p>
</section>
</section>
<section id="the-missing-piece-non-linearity">
<h2>The missing piece: non-linearity<a class="headerlink" href="#the-missing-piece-non-linearity" title="Link to this heading">#</a></h2>
<p>We‚Äôve seen our model can draw straight (linear) lines, thanks to its linear layers.</p>
<p>But how about we give it the capacity to draw non-straight (non-linear) lines?</p>
<p>Let‚Äôs find out.</p>
<section id="recreating-non-linear-data-red-and-blue-circles">
<h3>Recreating non-linear data (red and blue circles)<a class="headerlink" href="#recreating-non-linear-data-red-and-blue-circles" title="Link to this heading">#</a></h3>
<p>First, let‚Äôs recreate the data to start off fresh. We‚Äôll use the same setup as before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make and plot data</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">noise</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdBu</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../../_images/22e192d862628508a66708d49974dcc863a749a45804da698ae7ca2d6f4d73ae.png" src="../../../../../_images/22e192d862628508a66708d49974dcc863a749a45804da698ae7ca2d6f4d73ae.png" />
</div>
</div>
<p>Nice! Now let‚Äôs split it into training and test sets using 80% of the data for training and 20% for testing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert to tensors and split into train and test sets</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Turn data into tensors</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

<span class="c1"># Split into train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> 
                                                    <span class="n">y</span><span class="p">,</span> 
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([[ 0.6579, -0.4651],
         [ 0.6319, -0.7347],
         [-1.0086, -0.1240],
         [-0.9666, -0.2256],
         [-0.1666,  0.7994]]),
 tensor([1., 0., 0., 0., 1.]))
</pre></div>
</div>
</div>
</div>
</section>
<section id="building-a-model-with-non-linearity">
<h3>Building a model with non-linearity<a class="headerlink" href="#building-a-model-with-non-linearity" title="Link to this heading">#</a></h3>
<p>So far our neural networks have only been using linear (straight) line functions.</p>
<p>But the data we‚Äôve been working with is non-linear (circles).</p>
<p>Let‚Äôs see what happens when we introduce the capability for our model to use <strong>non-linear actviation functions</strong>?</p>
<p>Well let‚Äôs see.</p>
<p>PyTorch has a bunch of <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">ready-made non-linear activation functions</a> that do similiar but different things.</p>
<p>One of the most common and best performing is [ReLU](<a class="reference external" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">https://en.wikipedia.org/wiki/Rectifier_(neural_networks)</a> (rectified linear-unit, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.ReLU()</span></code></a>).</p>
<p>Rather than talk about it, let‚Äôs put it in our neural network between the hidden layers in the forward pass and see what happens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build model with non-linear activation function</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="k">class</span> <span class="nc">CircleModelV2</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span> <span class="c1"># &lt;- add in ReLU activation function</span>
        <span class="c1"># Can also put sigmoid in the model </span>
        <span class="c1"># This would mean you don&#39;t need to use it on the predictions</span>
        <span class="c1"># self.sigmoid = nn.Sigmoid()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
      <span class="c1"># Intersperse the ReLU activation function between layers</span>
       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_1</span><span class="p">(</span><span class="n">x</span><span class="p">)))))</span>

<span class="n">model_3</span> <span class="o">=</span> <span class="n">CircleModelV2</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CircleModelV2(
  (layer_1): Linear(in_features=2, out_features=10, bias=True)
  (layer_2): Linear(in_features=10, out_features=10, bias=True)
  (layer_3): Linear(in_features=10, out_features=1, bias=True)
  (relu): ReLU()
)
</pre></div>
</div>
</div>
</div>
<p><img alt="a classification neural network on TensorFlow playground with ReLU activation" src="../../../../../_images/02-tensorflow-playground-relu-activation.png" />
<em>A visual example of what a similar classificiation neural network to the one we‚Äôve just built (using ReLU activation) looks like. Try create one of your own on the <a class="reference external" href="https://playground.tensorflow.org/">TensorFlow Playground website</a>.</em></p>
<p>Now we‚Äôve got a model ready to go, let‚Äôs create a binary classification loss function as well as an optimizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup loss and optimizer </span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_3</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Wonderful!</p>
</section>
<section id="training-a-model-with-non-linearity">
<h3>Training a model with non-linearity<a class="headerlink" href="#training-a-model-with-non-linearity" title="Link to this heading">#</a></h3>
<p>With given model, loss function, optimizer ready to go, let‚Äôs create a training and testing loop.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the model</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Put all data on target device</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># 1. Forward pass</span>
    <span class="n">y_logits</span> <span class="o">=</span> <span class="n">model_3</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y_logits</span><span class="p">))</span> <span class="c1"># logits -&gt; prediction probabilities -&gt; prediction labels</span>
    
    <span class="c1"># 2. Calculate loss and accuracy</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_logits</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># BCEWithLogitsLoss calculates loss using logits</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> 
                      <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
    
    <span class="c1"># 3. Optimizer zero grad</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 4. Loss backward</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 5. Optimizer step</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1">### Testing</span>
    <span class="n">model_3</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
      <span class="c1"># 1. Forward pass</span>
      <span class="n">test_logits</span> <span class="o">=</span> <span class="n">model_3</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
      <span class="n">test_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">test_logits</span><span class="p">))</span> <span class="c1"># logits -&gt; prediction probabilities -&gt; prediction labels</span>
      <span class="c1"># 2. Calcuate loss and accuracy</span>
      <span class="n">test_loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">test_logits</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
      <span class="n">test_acc</span> <span class="o">=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
                             <span class="n">y_pred</span><span class="o">=</span><span class="n">test_pred</span><span class="p">)</span>

    <span class="c1"># Print out what&#39;s happening</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> | Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, Accuracy: </span><span class="si">{</span><span class="n">acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% | Test Loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, Test Accuracy: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 0 | Loss: 0.69295, Accuracy: 50.00% | Test Loss: 0.69319, Test Accuracy: 50.00%
Epoch: 100 | Loss: 0.69115, Accuracy: 52.88% | Test Loss: 0.69102, Test Accuracy: 52.50%
Epoch: 200 | Loss: 0.68977, Accuracy: 53.37% | Test Loss: 0.68940, Test Accuracy: 55.00%
Epoch: 300 | Loss: 0.68795, Accuracy: 53.00% | Test Loss: 0.68723, Test Accuracy: 56.00%
Epoch: 400 | Loss: 0.68517, Accuracy: 52.75% | Test Loss: 0.68411, Test Accuracy: 56.50%
Epoch: 500 | Loss: 0.68102, Accuracy: 52.75% | Test Loss: 0.67941, Test Accuracy: 56.50%
Epoch: 600 | Loss: 0.67515, Accuracy: 54.50% | Test Loss: 0.67285, Test Accuracy: 56.00%
Epoch: 700 | Loss: 0.66659, Accuracy: 58.38% | Test Loss: 0.66322, Test Accuracy: 59.00%
Epoch: 800 | Loss: 0.65160, Accuracy: 64.00% | Test Loss: 0.64757, Test Accuracy: 67.50%
Epoch: 900 | Loss: 0.62362, Accuracy: 74.00% | Test Loss: 0.62145, Test Accuracy: 79.00%
</pre></div>
</div>
</div>
</div>
<p>That‚Äôs looking far better!</p>
</section>
<section id="evaluating-a-model-trained-with-non-linear-activation-functions">
<h3>Evaluating a model trained with non-linear activation functions<a class="headerlink" href="#evaluating-a-model-trained-with-non-linear-activation-functions" title="Link to this heading">#</a></h3>
<p>Remember how our circle data is non-linear? Well, let‚Äôs see how our models predictions look now the model‚Äôs been trained with non-linear activation functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make predictions</span>
<span class="n">model_3</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">y_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">model_3</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
<span class="n">y_preds</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="n">y</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span> <span class="c1"># want preds in same format as truth labels</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 0.], device=&#39;cuda:0&#39;),
 tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0.]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot decision boundaries for training and test sets</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Train&quot;</span><span class="p">)</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">model_1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># model_1 = no non-linearity</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Test&quot;</span><span class="p">)</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">model_3</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="c1"># model_3 = has non-linearity</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../../_images/0bb2b9665a3ce45096cc56f691451d703b6147e58d9975ef2457c3422117fe38.png" src="../../../../../_images/0bb2b9665a3ce45096cc56f691451d703b6147e58d9975ef2457c3422117fe38.png" />
</div>
</div>
<p>Not perfect but still far better than before.</p>
<p>Potentially we could try a few tricks to improve the test accuracy of the model?</p>
</section>
</section>
<section id="replicating-non-linear-activation-functions">
<h2>Replicating non-linear activation functions<a class="headerlink" href="#replicating-non-linear-activation-functions" title="Link to this heading">#</a></h2>
<p>We saw before how adding non-linear activation functions to our model can help it to model non-linear data.</p>
<blockquote>
<div><p><strong>Note:</strong> Much of the data you‚Äôll encounter in the wild is non-linear (or a combination of linear and non-linear). Right now we‚Äôve been working with dots on a 2D plot. But imagine if you had images of plants you‚Äôd like to classify, there‚Äôs a lot of different plant shapes. Or text from Wikipedia you‚Äôd like to summarize, there‚Äôs lots of different ways words can be put together (linear and non-linear patterns).</p>
</div></blockquote>
<p>But what does a non-linear activation <em>look</em> like?</p>
<p>Let‚Äôs start by creating a small amount of data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a toy tensor (similar to the data going into our model(s))</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">A</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,
          2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.])
</pre></div>
</div>
</div>
</div>
<p>Wonderful, now let‚Äôs plot it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the toy tensor</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../../_images/c334ca3e1719f4dd541cba23b2e3ccc195bcd59efcc096eab2b2a17bd9a69534.png" src="../../../../../_images/c334ca3e1719f4dd541cba23b2e3ccc195bcd59efcc096eab2b2a17bd9a69534.png" />
</div>
</div>
<p>Now let‚Äôs see how the ReLU activation function influences it.</p>
<p>And instead of using PyTorch‚Äôs ReLU (<code class="docutils literal notranslate"><span class="pre">torch.nn.ReLU</span></code>), we‚Äôll recreate it ourselves.</p>
<p>The ReLU function turns all negatives to 0 and leaves the positive values as they are.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create ReLU function by hand </span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># inputs must be tensors</span>

<span class="c1"># Pass toy tensor through ReLU function</span>
<span class="n">relu</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 7.,
        8., 9.])
</pre></div>
</div>
</div>
</div>
<p>It looks like our ReLU function worked, all of the negative values are zeros.</p>
<p>Let‚Äôs plot them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot ReLU activated toy tensor</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">relu</span><span class="p">(</span><span class="n">A</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../../_images/55c89781adc8eeba154386c33d0c478ebbbe350d9bb2cc06b5e4d4c835fba327.png" src="../../../../../_images/55c89781adc8eeba154386c33d0c478ebbbe350d9bb2cc06b5e4d4c835fba327.png" />
</div>
</div>
<p>That looks exactly like the shape of the ReLU function on the <a class="reference external" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Wikipedia page for ReLU</a>.</p>
<p>How about we try the <a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a> we‚Äôve been using?</p>
<p>The sigmoid function formula goes like so:</p>
<div class="math notranslate nohighlight">
\[ out_i = \frac{1}{1+e^{-input_i}} \]</div>
<p>Or using <span class="math notranslate nohighlight">\(x\)</span> as input:</p>
<div class="math notranslate nohighlight">
\[ S(x) = \frac{1}{1+e^{-x_i}} \]</div>
<p>Where <span class="math notranslate nohighlight">\(S\)</span> stands for sigmoid, <span class="math notranslate nohighlight">\(e\)</span> stands for <a class="reference external" href="https://en.wikipedia.org/wiki/Exponential_function">exponential</a> (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.exp.html"><code class="docutils literal notranslate"><span class="pre">torch.exp()</span></code></a>) and <span class="math notranslate nohighlight">\(i\)</span> stands for a particular element in a tensor.</p>
<p>Let‚Äôs build a function to replicate the sigmoid function with PyTorch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a custom sigmoid function</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Test custom sigmoid on toy tensor</span>
<span class="n">sigmoid</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([4.5398e-05, 1.2339e-04, 3.3535e-04, 9.1105e-04, 2.4726e-03, 6.6929e-03,
        1.7986e-02, 4.7426e-02, 1.1920e-01, 2.6894e-01, 5.0000e-01, 7.3106e-01,
        8.8080e-01, 9.5257e-01, 9.8201e-01, 9.9331e-01, 9.9753e-01, 9.9909e-01,
        9.9966e-01, 9.9988e-01])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot sigmoid activated toy tensor</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">A</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../../_images/e2fab06e49eff652e0fb3b7d5db679e9017ab6911978fb8327f740466d6ed348.png" src="../../../../../_images/e2fab06e49eff652e0fb3b7d5db679e9017ab6911978fb8327f740466d6ed348.png" />
</div>
</div>
<p>Now there‚Äôs plenty more <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">non-linear activation functions</a> that exist in PyTorch that we haven‚Äôt tried.</p>
<p>But these two are two of the most common.</p>
<p>And the point remains, what patterns could you draw using an unlimited amount of linear (straight) and non-linear (not straight) lines?</p>
<p>Almost anything right?</p>
<p>That‚Äôs exactly what our model is doing when we combine linear and non-linear functions.</p>
<p>Instead of telling our model what to do, we give it tools to figure out how to best discover patterns in the data.</p>
<p>And those tools are linear and non-linear functions.</p>
</section>
<section id="putting-things-together">
<h2>Putting things together<a class="headerlink" href="#putting-things-together" title="Link to this heading">#</a></h2>
<p>We‚Äôve covered a fair bit.</p>
<p>But now let‚Äôs put it all together using a multi-class classification problem.</p>
<p>Recall a <strong>binary classification</strong> problem deals with classifying something as one of two options (e.g. a photo as a cat photo or a dog photo) where as a <strong>multi-class classification</strong> problem deals with classifying something from a list of <em>more than</em> two options (e.g. classifying a photo as a cat a dog or a chicken).</p>
<p><img alt="binary vs multi-class classification image with the example of dog vs cat for binary classification and dog vs cat vs chicken for multi-class classification" src="../../../../../_images/02-binary-vs-multi-class-classification.png" />
<em>Example of binary vs. multi-class classification. Binary deals with two classes (one thing or another), where as multi-class classification can deal with any number of classes over two, for example, the popular <a class="reference external" href="https://www.image-net.org/">ImageNet-1k dataset</a> is used as a computer vision benchmark and has 1000 classes.</em></p>
<section id="creating-mutli-class-classification-data">
<h3>Creating mutli-class classification data<a class="headerlink" href="#creating-mutli-class-classification-data" title="Link to this heading">#</a></h3>
<p>To begin a multi-class classification problem, let‚Äôs create some multi-class data.</p>
<p>To do so, we can leverage Scikit-Learn‚Äôs <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html"><code class="docutils literal notranslate"><span class="pre">make_blobs()</span></code></a> method.</p>
<p>This method will create however many classes (using the <code class="docutils literal notranslate"><span class="pre">centers</span></code> parameter) we want.</p>
<p>Specifically, let‚Äôs do the following:</p>
<ol class="arabic simple">
<li><p>Create some multi-class data with <code class="docutils literal notranslate"><span class="pre">make_blobs()</span></code>.</p></li>
<li><p>Turn the data into tensors (the default of <code class="docutils literal notranslate"><span class="pre">make_blobs()</span></code> is to use NumPy arrays).</p></li>
<li><p>Split the data into training and test sets using <code class="docutils literal notranslate"><span class="pre">train_test_split()</span></code>.</p></li>
<li><p>Visualize the data.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import dependencies</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Set the hyperparameters for data creation</span>
<span class="n">NUM_CLASSES</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">NUM_FEATURES</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">42</span>

<span class="c1"># 1. Create multi-class data</span>
<span class="n">X_blob</span><span class="p">,</span> <span class="n">y_blob</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">n_features</span><span class="o">=</span><span class="n">NUM_FEATURES</span><span class="p">,</span> <span class="c1"># X features</span>
    <span class="n">centers</span><span class="o">=</span><span class="n">NUM_CLASSES</span><span class="p">,</span> <span class="c1"># y labels </span>
    <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="c1"># give the clusters a little shake up (try changing this to 1.0, the default)</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">RANDOM_SEED</span>
<span class="p">)</span>

<span class="c1"># 2. Turn data into tensors</span>
<span class="n">X_blob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_blob</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="n">y_blob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_blob</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_blob</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">y_blob</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>

<span class="c1"># 3. Split into train and test sets</span>
<span class="n">X_blob_train</span><span class="p">,</span> <span class="n">X_blob_test</span><span class="p">,</span> <span class="n">y_blob_train</span><span class="p">,</span> <span class="n">y_blob_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_blob</span><span class="p">,</span>
    <span class="n">y_blob</span><span class="p">,</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="n">RANDOM_SEED</span>
<span class="p">)</span>

<span class="c1"># 4. Plot data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_blob</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_blob</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_blob</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdYlBu</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-8.4134,  6.9352],
        [-5.7665, -6.4312],
        [-6.0421, -6.7661],
        [ 3.9508,  0.6984],
        [ 4.2505, -0.2815]]) tensor([3, 2, 2, 1, 1])
</pre></div>
</div>
<img alt="../../../../../_images/df6139e3b4721466b89f6fa8a139aef0ad62682ff29b05716f21235cde9474df.png" src="../../../../../_images/df6139e3b4721466b89f6fa8a139aef0ad62682ff29b05716f21235cde9474df.png" />
</div>
</div>
<p>Looks like we‚Äôve got some multi-class data ready to go.</p>
<p>Let‚Äôs build a model to separate the coloured blobs.</p>
<blockquote>
<div><p><strong>Question:</strong> Does this dataset need non-linearity? Or could you draw a succession of straight lines to separate it?</p>
</div></blockquote>
</section>
<section id="building-a-multi-class-classification-model-in-pytorch">
<h3>Building a multi-class classification model in PyTorch<a class="headerlink" href="#building-a-multi-class-classification-model-in-pytorch" title="Link to this heading">#</a></h3>
<p>We‚Äôve created a few models in PyTorch so far.</p>
<p>How about we build one similar to <code class="docutils literal notranslate"><span class="pre">model_3</span></code> but this still capable of handling multi-class data?</p>
<p>To do so, let‚Äôs create a subclass of <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> that takes in three hyperparameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_features</span></code> - the number of <code class="docutils literal notranslate"><span class="pre">X</span></code> features coming into the model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_features</span></code> - the ideal numbers of output features we‚Äôd like (this will be equivalent to <code class="docutils literal notranslate"><span class="pre">NUM_CLASSES</span></code> or the number of classes in your multi-class classification problem).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_units</span></code> - the number of hidden neurons we‚Äôd like each hidden layer to use.</p></li>
</ul>
<p>Since we‚Äôre putting things together, let‚Äôs setup some device agnostic code (we don‚Äôt have to do this again in the same notebook, it‚Äôs only a reminder).</p>
<p>Then we‚Äôll create the model class using the hyperparameters above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create device agnostic code</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;cuda&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Build model</span>
<span class="k">class</span> <span class="nc">BlobModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">output_features</span><span class="p">,</span> <span class="n">hidden_units</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes all required hyperparameters for a multi-class classification model.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_features (int): Number of input features to the model.</span>
<span class="sd">            out_features (int): Number of output features of the model</span>
<span class="sd">              (how many classes there are).</span>
<span class="sd">            hidden_units (int): Number of hidden units between layers, default 8.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_layer_stack</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">),</span>
            <span class="c1"># nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">),</span>
            <span class="c1"># nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">output_features</span><span class="p">),</span> <span class="c1"># how many classes are there?</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layer_stack</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create an instance of BlobModel and send it to the target device</span>
<span class="n">model_4</span> <span class="o">=</span> <span class="n">BlobModel</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">NUM_FEATURES</span><span class="p">,</span> 
                    <span class="n">output_features</span><span class="o">=</span><span class="n">NUM_CLASSES</span><span class="p">,</span> 
                    <span class="n">hidden_units</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_4</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BlobModel(
  (linear_layer_stack): Sequential(
    (0): Linear(in_features=2, out_features=8, bias=True)
    (1): Linear(in_features=8, out_features=8, bias=True)
    (2): Linear(in_features=8, out_features=4, bias=True)
  )
)
</pre></div>
</div>
</div>
</div>
<p>Our multi-class model is ready to go, let‚Äôs create a loss function and optimizer for it.</p>
</section>
<section id="creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model">
<h3>Creating a loss function and optimizer for a multi-class PyTorch model<a class="headerlink" href="#creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model" title="Link to this heading">#</a></h3>
<p>Since we‚Äôre working on a multi-class classification problem, we‚Äôll use the <code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss()</span></code> method as our loss function.</p>
<p>And we‚Äôll stick with using SGD with a learning rate of 0.1 for optimizing our <code class="docutils literal notranslate"><span class="pre">model_4</span></code> parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create loss and optimizer</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_4</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> 
                            <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># exercise: try changing the learning rate here and seeing what happens to the model&#39;s performance</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="getting-prediction-probabilities-for-a-multi-class-pytorch-model">
<h3>Getting prediction probabilities for a multi-class PyTorch model<a class="headerlink" href="#getting-prediction-probabilities-for-a-multi-class-pytorch-model" title="Link to this heading">#</a></h3>
<p>Alright, we‚Äôve got a loss function and optimizer ready, and we‚Äôre ready to train our model but before we do let‚Äôs do a single forward pass with our model to see if it works.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform a single forward pass on the data (we&#39;ll need to put it to the target device for it to work)</span>
<span class="n">model_4</span><span class="p">(</span><span class="n">X_blob_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-1.2711, -0.6494, -1.4740, -0.7044],
        [ 0.2210, -1.5439,  0.0420,  1.1531],
        [ 2.8698,  0.9143,  3.3169,  1.4027],
        [ 1.9576,  0.3125,  2.2244,  1.1324],
        [ 0.5458, -1.2381,  0.4441,  1.1804]], device=&#39;cuda:0&#39;,
       grad_fn=&lt;SliceBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>What‚Äôs coming out here?</p>
<p>It looks like we get one value per feature of each sample.</p>
<p>Let‚Äôs check the shape to confirm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># How many elements in a single prediction sample?</span>
<span class="n">model_4</span><span class="p">(</span><span class="n">X_blob_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">NUM_CLASSES</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([4]), 4)
</pre></div>
</div>
</div>
</div>
<p>Our model is predicting one value for each class that we have.</p>
<p>If you guessed <em>logits</em>, you‚Äôd be correct.</p>
<p>So right now our model is outputing logits but what if we wanted to figure out exactly which label is was giving the sample?</p>
<p>As in, how do we go from <code class="docutils literal notranslate"><span class="pre">logits</span> <span class="pre">-&gt;</span> <span class="pre">prediction</span> <span class="pre">probabilities</span> <span class="pre">-&gt;</span> <span class="pre">prediction</span> <span class="pre">labels</span></code> just like we did with the binary classification problem?</p>
<p>That‚Äôs where the <a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">softmax activation function</a> comes into play.</p>
<p>The softmax function calculates the probability of each prediction class being the actual predicted class compared to all other possible classes.</p>
<p>If this doesn‚Äôt make sense, let‚Äôs see in code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make prediction logits with model</span>
<span class="n">y_logits</span> <span class="o">=</span> <span class="n">model_4</span><span class="p">(</span><span class="n">X_blob_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="c1"># Perform softmax calculation on logits across dimension 1 to get prediction probabilities</span>
<span class="n">y_pred_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">y_logits</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_pred_probs</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[-1.2549, -0.8112, -1.4795, -0.5696],
        [ 1.7168, -1.2270,  1.7367,  2.1010],
        [ 2.2400,  0.7714,  2.6020,  1.0107],
        [-0.7993, -0.3723, -0.9138, -0.5388],
        [-0.4332, -1.6117, -0.6891,  0.6852]], device=&#39;cuda:0&#39;,
       grad_fn=&lt;SliceBackward0&gt;)
tensor([[0.1872, 0.2918, 0.1495, 0.3715],
        [0.2824, 0.0149, 0.2881, 0.4147],
        [0.3380, 0.0778, 0.4854, 0.0989],
        [0.2118, 0.3246, 0.1889, 0.2748],
        [0.1945, 0.0598, 0.1506, 0.5951]], device=&#39;cuda:0&#39;,
       grad_fn=&lt;SliceBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>What‚Äôs happened here?</p>
<p>It may still look like the outputs of the softmax function are jumbled numbers (and they are, since our model hasn‚Äôt been trained and is predicting using random patterns) but there‚Äôs a very specific thing different about each sample.</p>
<p>After passing the logits through the softmax function, each individual sample now adds to 1 (or very close to).</p>
<p>Let‚Äôs check.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sum the first sample output of the softmax activation function </span>
<span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_pred_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(1., device=&#39;cuda:0&#39;, grad_fn=&lt;SumBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>These prediction probablities are essentially saying how much the model <em>thinks</em> the target <code class="docutils literal notranslate"><span class="pre">X</span></code> sample (the input) maps to each class.</p>
<p>Since there‚Äôs one value for each class in <code class="docutils literal notranslate"><span class="pre">y_pred_probs</span></code>, the index of the <em>highest</em> value is the class the model thinks the specific data sample <em>most</em> belongs to.</p>
<p>We can check which index has the highest value using <code class="docutils literal notranslate"><span class="pre">torch.argmax()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Which class does the model think is *most* likely at the index 0 sample?</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_pred_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.1872, 0.2918, 0.1495, 0.3715], device=&#39;cuda:0&#39;,
       grad_fn=&lt;SelectBackward0&gt;)
tensor(3, device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<p>You can see the output of <code class="docutils literal notranslate"><span class="pre">torch.argmax()</span></code> returns 3, so for the features (<code class="docutils literal notranslate"><span class="pre">X</span></code>) of the sample at index 0, the model is predicting that the most likely class value (<code class="docutils literal notranslate"><span class="pre">y</span></code>) is 3.</p>
<p>Of course, right now this is just random guessing so it‚Äôs got a 25% chance of being right (since there‚Äôs four classes). But we can improve those chances by training the model.</p>
<blockquote>
<div><p><strong>Note:</strong> To summarize the above, a model‚Äôs raw output is referred to as <strong>logits</strong>.</p>
<p>For a multi-class classification problem, to turn the logits into <strong>prediction probabilities</strong>, you use the softmax activation function (<code class="docutils literal notranslate"><span class="pre">torch.softmax</span></code>).</p>
<p>The index of the value with the highest <strong>prediction probability</strong> is the class number the model thinks is <em>most</em> likely given the input features for that sample (although this is a prediction, it doesn‚Äôt mean it will be correct).</p>
</div></blockquote>
</section>
<section id="creating-a-training-and-testing-loop-for-a-multi-class-pytorch-model">
<h3>Creating a training and testing loop for a multi-class PyTorch model<a class="headerlink" href="#creating-a-training-and-testing-loop-for-a-multi-class-pytorch-model" title="Link to this heading">#</a></h3>
<p>Alright, now we‚Äôve got all of the preparation steps out of the way, let‚Äôs write a training and testing loop to improve and evaluate our model.</p>
<p>We‚Äôve done many of these steps before so much of this will be practice.</p>
<p>The only difference is that we‚Äôll be adjusting the steps to turn the model outputs (logits) to prediction probabilities (using the softmax activation function) and then to prediction labels (by taking the argmax of the output of the softmax activation function).</p>
<p>Let‚Äôs train the model for <code class="docutils literal notranslate"><span class="pre">epochs=100</span></code> and evaluate it every 10 epochs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the model</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Set number of epochs</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Put data to target device</span>
<span class="n">X_blob_train</span><span class="p">,</span> <span class="n">y_blob_train</span> <span class="o">=</span> <span class="n">X_blob_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_blob_train</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_blob_test</span><span class="p">,</span> <span class="n">y_blob_test</span> <span class="o">=</span> <span class="n">X_blob_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_blob_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1">### Training</span>
    <span class="n">model_4</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="c1"># 1. Forward pass</span>
    <span class="n">y_logits</span> <span class="o">=</span> <span class="n">model_4</span><span class="p">(</span><span class="n">X_blob_train</span><span class="p">)</span> <span class="c1"># model outputs raw logits </span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># go from logits -&gt; prediction probabilities -&gt; prediction labels</span>
    <span class="c1"># print(y_logits)</span>
    <span class="c1"># 2. Calculate loss and accuracy</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_logits</span><span class="p">,</span> <span class="n">y_blob_train</span><span class="p">)</span> 
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_blob_train</span><span class="p">,</span>
                      <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>

    <span class="c1"># 3. Optimizer zero grad</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 4. Loss backwards</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 5. Optimizer step</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1">### Testing</span>
    <span class="n">model_4</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
      <span class="c1"># 1. Forward pass</span>
      <span class="n">test_logits</span> <span class="o">=</span> <span class="n">model_4</span><span class="p">(</span><span class="n">X_blob_test</span><span class="p">)</span>
      <span class="n">test_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">test_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="c1"># 2. Calculate test loss and accuracy</span>
      <span class="n">test_loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">test_logits</span><span class="p">,</span> <span class="n">y_blob_test</span><span class="p">)</span>
      <span class="n">test_acc</span> <span class="o">=</span> <span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_blob_test</span><span class="p">,</span>
                             <span class="n">y_pred</span><span class="o">=</span><span class="n">test_pred</span><span class="p">)</span>

    <span class="c1"># Print out what&#39;s happening</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> | Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, Acc: </span><span class="si">{</span><span class="n">acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% | Test Loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, Test Acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 0 | Loss: 1.04324, Acc: 65.50% | Test Loss: 0.57861, Test Acc: 95.50%
Epoch: 10 | Loss: 0.14398, Acc: 99.12% | Test Loss: 0.13037, Test Acc: 99.00%
Epoch: 20 | Loss: 0.08062, Acc: 99.12% | Test Loss: 0.07216, Test Acc: 99.50%
Epoch: 30 | Loss: 0.05924, Acc: 99.12% | Test Loss: 0.05133, Test Acc: 99.50%
Epoch: 40 | Loss: 0.04892, Acc: 99.00% | Test Loss: 0.04098, Test Acc: 99.50%
Epoch: 50 | Loss: 0.04295, Acc: 99.00% | Test Loss: 0.03486, Test Acc: 99.50%
Epoch: 60 | Loss: 0.03910, Acc: 99.00% | Test Loss: 0.03083, Test Acc: 99.50%
Epoch: 70 | Loss: 0.03643, Acc: 99.00% | Test Loss: 0.02799, Test Acc: 99.50%
Epoch: 80 | Loss: 0.03448, Acc: 99.00% | Test Loss: 0.02587, Test Acc: 99.50%
Epoch: 90 | Loss: 0.03300, Acc: 99.12% | Test Loss: 0.02423, Test Acc: 99.50%
</pre></div>
</div>
</div>
</div>
</section>
<section id="making-and-evaluating-predictions-with-a-pytorch-multi-class-model">
<h3>Making and evaluating predictions with a PyTorch multi-class model<a class="headerlink" href="#making-and-evaluating-predictions-with-a-pytorch-multi-class-model" title="Link to this heading">#</a></h3>
<p>It looks like our trained model is performaning pretty well.</p>
<p>But to make sure of this, let‚Äôs make some predictions and visualize them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make predictions</span>
<span class="n">model_4</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">y_logits</span> <span class="o">=</span> <span class="n">model_4</span><span class="p">(</span><span class="n">X_blob_test</span><span class="p">)</span>

<span class="c1"># View the first 10 predictions</span>
<span class="n">y_logits</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[  4.3377,  10.3539, -14.8948,  -9.7642],
        [  5.0142, -12.0371,   3.3860,  10.6699],
        [ -5.5885, -13.3448,  20.9894,  12.7711],
        [  1.8400,   7.5599,  -8.6016,  -6.9942],
        [  8.0726,   3.2906, -14.5998,  -3.6186],
        [  5.5844, -14.9521,   5.0168,  13.2890],
        [ -5.9739, -10.1913,  18.8655,   9.9179],
        [  7.0755,  -0.7601,  -9.5531,   0.1736],
        [ -5.5918, -18.5990,  25.5309,  17.5799],
        [  7.3142,   0.7197, -11.2017,  -1.2011]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<p>Alright, looks like our model‚Äôs predictions are still in logit form.</p>
<p>Though to evaluate them, they‚Äôll have to be in the same form as our labels (<code class="docutils literal notranslate"><span class="pre">y_blob_test</span></code>) which are in integer form.</p>
<p>Let‚Äôs convert our model‚Äôs prediction logits to prediction probabilities (using <code class="docutils literal notranslate"><span class="pre">torch.softmax()</span></code>) then to prediction labels (by taking the <code class="docutils literal notranslate"><span class="pre">argmax()</span></code> of each sample).</p>
<blockquote>
<div><p><strong>Note:</strong> It‚Äôs possible to skip the <code class="docutils literal notranslate"><span class="pre">torch.softmax()</span></code> function and go straight from <code class="docutils literal notranslate"><span class="pre">predicted</span> <span class="pre">logits</span> <span class="pre">-&gt;</span> <span class="pre">predicted</span> <span class="pre">labels</span></code> by calling <code class="docutils literal notranslate"><span class="pre">torch.argmax()</span></code> directly on the logits.</p>
<p>For example, <code class="docutils literal notranslate"><span class="pre">y_preds</span> <span class="pre">=</span> <span class="pre">torch.argmax(y_logits,</span> <span class="pre">dim=1)</span></code>, this saves a computation step (no <code class="docutils literal notranslate"><span class="pre">torch.softmax()</span></code>) but results in no prediction probabilities being available to use.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Turn predicted logits in prediction probabilities</span>
<span class="n">y_pred_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Turn prediction probabilities into prediction labels</span>
<span class="n">y_preds</span> <span class="o">=</span> <span class="n">y_pred_probs</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compare first 10 model preds and test labels</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predictions: </span><span class="si">{</span><span class="n">y_preds</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">Labels: </span><span class="si">{</span><span class="n">y_blob_test</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test accuracy: </span><span class="si">{</span><span class="n">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_blob_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="o">=</span><span class="n">y_preds</span><span class="p">)</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predictions: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device=&#39;cuda:0&#39;)
Labels: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device=&#39;cuda:0&#39;)
Test accuracy: 99.5%
</pre></div>
</div>
</div>
</div>
<p>Our model predictions are now in the same form as our test labels.</p>
<p>Let‚Äôs visualize them with <code class="docutils literal notranslate"><span class="pre">plot_decision_boundary()</span></code>, remember because our data is on the GPU, we‚Äôll have to move it to the CPU for use with matplotlib (<code class="docutils literal notranslate"><span class="pre">plot_decision_boundary()</span></code> does this automatically for us).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Train&quot;</span><span class="p">)</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">model_4</span><span class="p">,</span> <span class="n">X_blob_train</span><span class="p">,</span> <span class="n">y_blob_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Test&quot;</span><span class="p">)</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">model_4</span><span class="p">,</span> <span class="n">X_blob_test</span><span class="p">,</span> <span class="n">y_blob_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../../_images/0056400faac3dc423772954c89ca95d628b8c2b7399e875f1db49925a84eb94f.png" src="../../../../../_images/0056400faac3dc423772954c89ca95d628b8c2b7399e875f1db49925a84eb94f.png" />
</div>
</div>
</section>
</section>
<section id="more-classification-evaluation-metrics">
<h2>More classification evaluation metrics<a class="headerlink" href="#more-classification-evaluation-metrics" title="Link to this heading">#</a></h2>
<p>So far we‚Äôve only covered a couple of ways of evaluating a classification model (accuracy, loss and visualizing predictions).</p>
<p>These are some of the most common methods you‚Äôll come across and are a good starting point.</p>
<p>However, you may want to evaluate your classification model using more metrics such as the following:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Metric name/Evaluation method</strong></p></th>
<th class="head"><p><strong>Defintion</strong></p></th>
<th class="head"><p><strong>Code</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Accuracy</p></td>
<td><p>Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct.</p></td>
<td><p><a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html#id3"><code class="docutils literal notranslate"><span class="pre">torchmetrics.Accuracy()</span></code></a> or <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"><code class="docutils literal notranslate"><span class="pre">sklearn.metrics.accuracy_score()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p>Precision</p></td>
<td><p>Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should‚Äôve been 0).</p></td>
<td><p><a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/classification/precision.html#id4"><code class="docutils literal notranslate"><span class="pre">torchmetrics.Precision()</span></code></a> or <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"><code class="docutils literal notranslate"><span class="pre">sklearn.metrics.precision_score()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p>Recall</p></td>
<td><p>Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should‚Äôve been 1). Higher recall leads to less false negatives.</p></td>
<td><p><a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/classification/recall.html#id5"><code class="docutils literal notranslate"><span class="pre">torchmetrics.Recall()</span></code></a> or <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html"><code class="docutils literal notranslate"><span class="pre">sklearn.metrics.recall_score()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p>F1-score</p></td>
<td><p>Combines precision and recall into one metric. 1 is best, 0 is worst.</p></td>
<td><p><a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/classification/f1_score.html#f1score"><code class="docutils literal notranslate"><span class="pre">torchmetrics.F1Score()</span></code></a> or <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"><code class="docutils literal notranslate"><span class="pre">sklearn.metrics.f1_score()</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/">Confusion matrix</a></p></td>
<td><p>Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line).</p></td>
<td><p><a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/classification/confusion_matrix.html#confusionmatrix"><code class="docutils literal notranslate"><span class="pre">torchmetrics.ConfusionMatrix</span></code></a> or <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions"><code class="docutils literal notranslate"><span class="pre">sklearn.metrics.plot_confusion_matrix()</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p>Classification report</p></td>
<td><p>Collection of some of the main classification metrics such as precision, recall and f1-score.</p></td>
<td><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"><code class="docutils literal notranslate"><span class="pre">sklearn.metrics.classification_report()</span></code></a></p></td>
</tr>
</tbody>
</table>
</div>
<p>Scikit-Learn (a popular and world-class machine learning library) has many implementations of the above metrics and you‚Äôre looking for a PyTorch-like version, check out <a class="reference external" href="https://torchmetrics.readthedocs.io/en/latest/">TorchMetrics</a>, especially the <a class="reference external" href="https://torchmetrics.readthedocs.io/en/stable/pages/classification.html">TorchMetrics classification section</a>.</p>
<p>Let‚Äôs try the <code class="docutils literal notranslate"><span class="pre">torchmetrics.Accuracy</span></code> metric out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torchmetrics</span> <span class="kn">import</span> <span class="n">Accuracy</span>
<span class="k">except</span><span class="p">:</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">torchmetrics</span><span class="o">==</span><span class="m">0</span>.9.3<span class="w"> </span>#<span class="w"> </span>this<span class="w"> </span>is<span class="w"> </span>the<span class="w"> </span>version<span class="w"> </span>we<span class="err">&#39;</span>re<span class="w"> </span>using<span class="w"> </span><span class="k">in</span><span class="w"> </span>this<span class="w"> </span>notebook<span class="w"> </span><span class="o">(</span>later<span class="w"> </span>versions<span class="w"> </span>exist<span class="w"> </span>here:<span class="w"> </span>https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog<span class="o">)</span>
    <span class="kn">from</span> <span class="nn">torchmetrics</span> <span class="kn">import</span> <span class="n">Accuracy</span>

<span class="c1"># Setup metric and make sure it&#39;s on the target device</span>
<span class="n">torchmetrics_accuracy</span> <span class="o">=</span> <span class="n">Accuracy</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">&#39;multiclass&#39;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Calculate accuracy</span>
<span class="n">torchmetrics_accuracy</span><span class="p">(</span><span class="n">y_preds</span><span class="p">,</span> <span class="n">y_blob_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.9950, device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/ai/neural/concepts/pytorch"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01_pytorch_workflow.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">PyTorch Workflow</p>
      </div>
    </a>
    <a class="right-next"
       href="03_pytorch_computer_vision.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">PyTorch Computer Vision</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-classification-problem">What is a classification problem?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-of-a-classification-neural-network">Architecture of a classification neural network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#make-classification-data-and-get-it-ready">Make classification data and get it ready</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-and-output-shapes">Input and output shapes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turn-data-into-tensors-and-create-train-and-test-splits">Turn data into tensors and create train and test splits</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-model">Building a model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-loss-function-and-optimizer">Setup loss function and optimizer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-model">Train model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#going-from-raw-model-outputs-to-predicted-labels-logits-prediction-probabilities-prediction-labels">Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-training-and-testing-loop">Building a training and testing loop</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#make-predictions-and-evaluate-the-model">Make predictions and evaluate the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-a-model">Improving a model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-data-to-see-if-our-model-can-model-a-straight-line">Preparing data to see if our model can model a straight line</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjusting-model-1-to-fit-a-straight-line">Adjusting <code class="docutils literal notranslate"><span class="pre">model_1</span></code> to fit a straight line</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-missing-piece-non-linearity">The missing piece: non-linearity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recreating-non-linear-data-red-and-blue-circles">Recreating non-linear data (red and blue circles)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-model-with-non-linearity">Building a model with non-linearity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-model-with-non-linearity">Training a model with non-linearity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-a-model-trained-with-non-linear-activation-functions">Evaluating a model trained with non-linear activation functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#replicating-non-linear-activation-functions">Replicating non-linear activation functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-things-together">Putting things together</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-mutli-class-classification-data">Creating mutli-class classification data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-multi-class-classification-model-in-pytorch">Building a multi-class classification model in PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-loss-function-and-optimizer-for-a-multi-class-pytorch-model">Creating a loss function and optimizer for a multi-class PyTorch model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-prediction-probabilities-for-a-multi-class-pytorch-model">Getting prediction probabilities for a multi-class PyTorch model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-training-and-testing-loop-for-a-multi-class-pytorch-model">Creating a training and testing loop for a multi-class PyTorch model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#making-and-evaluating-predictions-with-a-pytorch-multi-class-model">Making and evaluating predictions with a PyTorch multi-class model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-classification-evaluation-metrics">More classification evaluation metrics</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Anukool Chaturvedi
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>