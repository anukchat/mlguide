<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://mlguide.in</id>
  <title>Blog</title>
  <updated>2024-10-27T05:48:00.649601+00:00</updated>
  <link href="https://mlguide.in"/>
  <link href="https://mlguide.in/content/resources/blogs/atom.xml" rel="self"/>
  <generator uri="https://ablog.readthedocs.io/" version="0.11.11">ABlog</generator>
  <entry>
    <id>https://mlguide.in/content/resources/blogs/2024/Prompt%20tuning.html</id>
    <title>Prompt Tuning</title>
    <updated>2024-04-10T00:00:00+00:00</updated>
    <author>
      <name>Anukool Chaturvedi</name>
    </author>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Training large pretrained language models is very time-consuming and compute-intensive. As they continue to grow in size, there is increasing interest in more efficient training methods such as &lt;em&gt;prompting&lt;/em&gt;.&lt;/p&gt;
&lt;/p&gt;

    &lt;script type="text/x-thebe-config"&gt;
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/resources"
        },
        predefinedOutput: true
    }
    &lt;/script&gt;
    &lt;script&gt;kernelName = 'python3'&lt;/script&gt;</content>
    <link href="https://mlguide.in/content/resources/blogs/2024/Prompt%20tuning.html"/>
    <summary>Training large pretrained language models is very time-consuming and compute-intensive. As they continue to grow in size, there is increasing interest in more efficient training methods such as prompting.</summary>
    <category term="LLM" label="LLM"/>
    <category term="Prompt-Tuning" label="Prompt-Tuning"/>
    <published>2024-04-10T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://mlguide.in/content/resources/blogs/2024/LoRA%20%28Low%20Rank%20Adaptation%20of%20Large%20Language%20Models%29.html</id>
    <title>LoRA (Low Rank Adaptation of Large Language Models)</title>
    <updated>2024-04-10T00:00:00+00:00</updated>
    <author>
      <name>Anukool Chaturvedi</name>
    </author>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Before diving into LoRA, it’s essential to grasp the concept of &lt;strong&gt;Matrix Decomposition&lt;/strong&gt;.&lt;/p&gt;
&lt;/p&gt;

    &lt;script type="text/x-thebe-config"&gt;
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/resources"
        },
        predefinedOutput: true
    }
    &lt;/script&gt;
    &lt;script&gt;kernelName = 'python3'&lt;/script&gt;</content>
    <link href="https://mlguide.in/content/resources/blogs/2024/LoRA%20%28Low%20Rank%20Adaptation%20of%20Large%20Language%20Models%29.html"/>
    <summary>Before diving into LoRA, it’s essential to grasp the concept of Matrix Decomposition.</summary>
    <category term="Fine-Tuning" label="Fine-Tuning"/>
    <category term="LLM" label="LLM"/>
    <category term="LoRA" label="LoRA"/>
    <published>2024-04-10T00:00:00+00:00</published>
  </entry>
</feed>
