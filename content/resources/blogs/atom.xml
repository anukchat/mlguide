<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://mlguide.in</id>
  <title>Blog</title>
  <updated>2025-01-28T03:49:40.772425+00:00</updated>
  <link href="https://mlguide.in"/>
  <link href="https://mlguide.in/content/resources/blogs/atom.xml" rel="self"/>
  <generator uri="https://ablog.readthedocs.io/" version="0.11.12">ABlog</generator>
  <entry>
    <id>https://mlguide.in/content/resources/blogs/2024/Qwen%202.5%20Coder%20Models%20Enhanced%20Code%20Generation%20and%20Reasoning.html</id>
    <title>Qwen 2.5 Coder Models: Enhanced Code Generation and Reasoning</title>
    <updated>2024-12-27T00:00:00+00:00</updated>
    <author>
      <name>Anukool Chaturvedi</name>
    </author>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;The landscape of code generation is constantly evolving, demanding models that can not only produce syntactically correct code but also understand complex logic and reasoning. The Qwen 2.5 Coder models address this challenge by providing advanced capabilities in code synthesis and comprehension. These models, available in various parameter sizes, offer a solution for developers seeking more robust and reliable code generation tools. &lt;mark&gt;This introduction highlights the key problem these models solve and the solution they provide.&lt;/mark&gt; For those interested in exploring these models, a comprehensive collection is available on Hugging Face: &lt;a class="reference external" href="https://huggingface.co/collections/unsloth/qwen-25-coder-all-versions-6732bc833ed65dd1964994d4"&gt;Qwen 2.5 Coder All Versions&lt;/a&gt;. Additionally, a Google Colab notebook is provided for hands-on experimentation: &lt;a class="reference external" href="https://colab.research.google.com/drive/18sN803sU23XuJV9Q8On2xgqHSer6-UZF?usp=sharing"&gt;Qwen 2.5 Coder Colab&lt;/a&gt;.&lt;/p&gt;
&lt;/p&gt;

    &lt;script type="text/x-thebe-config"&gt;
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/resources"
        },
        predefinedOutput: true
    }
    &lt;/script&gt;
    &lt;script&gt;kernelName = 'python3'&lt;/script&gt;</content>
    <link href="https://mlguide.in/content/resources/blogs/2024/Qwen%202.5%20Coder%20Models%20Enhanced%20Code%20Generation%20and%20Reasoning.html"/>
    <summary>The landscape of code generation is constantly evolving, demanding models that can not only produce syntactically correct code but also understand complex logic and reasoning. The Qwen 2.5 Coder models address this challenge by providing advanced capabilities in code synthesis and comprehension. These models, available in various parameter sizes, offer a solution for developers seeking more robust and reliable code generation tools. &lt;mark&gt;This introduction highlights the key problem these models solve and the solution they provide.&lt;/mark&gt; For those interested in exploring these models, a comprehensive collection is available on Hugging Face: Qwen 2.5 Coder All Versions. Additionally, a Google Colab notebook is provided for hands-on experimentation: Qwen 2.5 Coder Colab.</summary>
    <category term="AI" label="AI"/>
    <category term="CodeGeneration" label="Code Generation"/>
    <category term="MachineLearning" label="Machine Learning"/>
    <category term="OpenSource" label="Open Source"/>
    <category term="Qwen" label="Qwen"/>
    <published>2024-12-27T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://mlguide.in/content/resources/blogs/2024/DeepSeek-V3_%20A%20Technical%20Overview%20of%20a%20Novel%20Mixture-of-Experts%20Model.html</id>
    <title>DeepSeek-V3: A Technical Overview of a Novel Mixture-of-Experts Model</title>
    <updated>2024-12-27T00:00:00+00:00</updated>
    <author>
      <name>Anukool Chaturvedi</name>
    </author>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;DeepSeek-V3 emerges as a significant advancement in the field of large language models (LLMs). This model, detailed in the accompanying technical report, &lt;mark&gt;employs a Mixture-of-Experts (MoE) architecture&lt;/mark&gt;, a design choice that allows for both high performance and computational efficiency. The architecture is composed of a number of expert sub-networks, which are activated conditionally based on the input, enabling the model to scale effectively. The training process involved a large corpus of text and code data, leveraging a combination of supervised learning and reinforcement learning techniques. &lt;mark&gt;DeepSeek-V3 demonstrates strong performance across a range of benchmarks&lt;/mark&gt;, while maintaining a focus on efficient inference. The model is available on GitHub, providing access to both the model weights and the technical documentation.&lt;/p&gt;
&lt;/p&gt;

    &lt;script type="text/x-thebe-config"&gt;
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/resources"
        },
        predefinedOutput: true
    }
    &lt;/script&gt;
    &lt;script&gt;kernelName = 'python3'&lt;/script&gt;</content>
    <link href="https://mlguide.in/content/resources/blogs/2024/DeepSeek-V3_%20A%20Technical%20Overview%20of%20a%20Novel%20Mixture-of-Experts%20Model.html"/>
    <summary>DeepSeek-V3 emerges as a significant advancement in the field of large language models (LLMs). This model, detailed in the accompanying technical report, &lt;mark&gt;employs a Mixture-of-Experts (MoE) architecture&lt;/mark&gt;, a design choice that allows for both high performance and computational efficiency. The architecture is composed of a number of expert sub-networks, which are activated conditionally based on the input, enabling the model to scale effectively. The training process involved a large corpus of text and code data, leveraging a combination of supervised learning and reinforcement learning techniques. &lt;mark&gt;DeepSeek-V3 demonstrates strong performance across a range of benchmarks&lt;/mark&gt;, while maintaining a focus on efficient inference. The model is available on GitHub, providing access to both the model weights and the technical documentation.</summary>
    <category term="AI" label="AI"/>
    <category term="DeepSeek-V3" label="DeepSeek-V3"/>
    <category term="LLM" label="LLM"/>
    <category term="Mixture-of-Experts" label="Mixture-of-Experts"/>
    <category term="OpenSource" label="Open Source"/>
    <published>2024-12-27T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://mlguide.in/content/resources/blogs/2024/Building%20Reliable%20Local%20Agents%20with%20LangGraph%2C%20LLaMA3%2C%20and%20Elasticsearch.html</id>
    <title>Building Reliable Local Agents with LangGraph, LLaMA3, and Elasticsearch</title>
    <updated>2024-12-25T00:00:00+00:00</updated>
    <author>
      <name>Anukool Chaturvedi</name>
    </author>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;This post details the construction of a robust local agent, leveraging the capabilities of LangGraph, LLaMA3, and Elasticsearch. &lt;mark&gt;We address the challenge of creating reliable local agents, which often suffer from inconsistencies and inaccuracies due to their reliance on limited context.&lt;/mark&gt; This blog post is a companion piece to the original article published by Elastic Search Labs, which can be found here: &lt;a class="reference external" href="https://www.elastic.co/search-labs/blog/local-rag-agent-elasticsearch-langgraph-llama3"&gt;https://www.elastic.co/search-labs/blog/local-rag-agent-elasticsearch-langgraph-llama3&lt;/a&gt;. &lt;mark&gt;Our solution employs LangGraph to orchestrate the agent’s workflow, LLaMA3 for its reasoning and generation capabilities, and Elasticsearch for efficient and accurate information retrieval.&lt;/mark&gt; This combination allows for a more dependable agent, capable of handling complex tasks with improved performance.&lt;/p&gt;
&lt;/p&gt;

    &lt;script type="text/x-thebe-config"&gt;
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/resources"
        },
        predefinedOutput: true
    }
    &lt;/script&gt;
    &lt;script&gt;kernelName = 'python3'&lt;/script&gt;</content>
    <link href="https://mlguide.in/content/resources/blogs/2024/Building%20Reliable%20Local%20Agents%20with%20LangGraph%2C%20LLaMA3%2C%20and%20Elasticsearch.html"/>
    <summary>This post details the construction of a robust local agent, leveraging the capabilities of LangGraph, LLaMA3, and Elasticsearch. &lt;mark&gt;We address the challenge of creating reliable local agents, which often suffer from inconsistencies and inaccuracies due to their reliance on limited context.&lt;/mark&gt; This blog post is a companion piece to the original article published by Elastic Search Labs, which can be found here: https://www.elastic.co/search-labs/blog/local-rag-agent-elasticsearch-langgraph-llama3. &lt;mark&gt;Our solution employs LangGraph to orchestrate the agent’s workflow, LLaMA3 for its reasoning and generation capabilities, and Elasticsearch for efficient and accurate information retrieval.&lt;/mark&gt; This combination allows for a more dependable agent, capable of handling complex tasks with improved performance.</summary>
    <category term="Elasticsearch" label="Elasticsearch"/>
    <category term="LLaMA3" label="LLaMA3"/>
    <category term="LangGraph" label="LangGraph"/>
    <category term="LocalAgents" label="Local Agents"/>
    <category term="RAG" label="RAG"/>
    <published>2024-12-25T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://mlguide.in/content/resources/blogs/2024/Decoding%20the%20Byte%20Latent%20Transformer_%20A%20New%20Approach%20to%20Language%20Processing.html</id>
    <title>Byte Latent Transformer: A New Approach to Language Processing</title>
    <updated>2024-12-22T00:00:00+00:00</updated>
    <author>
      <name>Anukool Chaturvedi</name>
    </author>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Let’s understand &lt;strong&gt;“Byte Latent Transformer: Patches Scale Better Than Tokens.”&lt;/strong&gt;  This paper introduces a novel approach to handling large language models tokens, potentially offering significant improvements in efficiency and performance. We will explore the core concepts, methodology, and potential implications of this research, drawing insights from the original paper and related discussions.&lt;/p&gt;
&lt;/p&gt;

    &lt;script type="text/x-thebe-config"&gt;
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/resources"
        },
        predefinedOutput: true
    }
    &lt;/script&gt;
    &lt;script&gt;kernelName = 'python3'&lt;/script&gt;</content>
    <link href="https://mlguide.in/content/resources/blogs/2024/Decoding%20the%20Byte%20Latent%20Transformer_%20A%20New%20Approach%20to%20Language%20Processing.html"/>
    <summary>Let’s understand “Byte Latent Transformer: Patches Scale Better Than Tokens.”  This paper introduces a novel approach to handling large language models tokens, potentially offering significant improvements in efficiency and performance. We will explore the core concepts, methodology, and potential implications of this research, drawing insights from the original paper and related discussions.</summary>
    <category term="AI" label="AI"/>
    <category term="LanguageProcessing" label="Language Processing"/>
    <category term="LargeLanguageModels" label="Large Language Models"/>
    <category term="MachineLearning" label="Machine Learning"/>
    <category term="NLP" label="NLP"/>
    <published>2024-12-22T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://mlguide.in/content/resources/blogs/2024/DeepSeek-VL2_%20A%20Powerful%20Vision-Language%20Model%20for%20Multimodal%20Understanding%20%281%29.html</id>
    <title>DeepSeek-VL2: A Powerful Vision-Language Model for Multimodal Understanding</title>
    <updated>2024-12-18T00:00:00+00:00</updated>
    <author>
      <name>Anukool Chaturvedi</name>
    </author>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;This blog post dives into the exciting advancements of DeepSeek-VL2, a new series of open-source Vision-Language Models (VLMs).  We’ll explore its architecture, training methodology, and impressive performance across diverse multimodal tasks, including visual question answering, optical character recognition, and document understanding.  The model’s innovative approach to handling high-resolution images and its efficient Mixture-of-Experts (MoE) architecture make it a significant leap forward in the field.&lt;/p&gt;
&lt;/p&gt;

    &lt;script type="text/x-thebe-config"&gt;
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/resources"
        },
        predefinedOutput: true
    }
    &lt;/script&gt;
    &lt;script&gt;kernelName = 'python3'&lt;/script&gt;</content>
    <link href="https://mlguide.in/content/resources/blogs/2024/DeepSeek-VL2_%20A%20Powerful%20Vision-Language%20Model%20for%20Multimodal%20Understanding%20%281%29.html"/>
    <summary>This blog post dives into the exciting advancements of DeepSeek-VL2, a new series of open-source Vision-Language Models (VLMs).  We’ll explore its architecture, training methodology, and impressive performance across diverse multimodal tasks, including visual question answering, optical character recognition, and document understanding.  The model’s innovative approach to handling high-resolution images and its efficient Mixture-of-Experts (MoE) architecture make it a significant leap forward in the field.</summary>
    <category term="AI" label="AI"/>
    <category term="ComputerVision" label="Computer Vision"/>
    <category term="DeepLearning" label="Deep Learning"/>
    <category term="Mixture-of-Experts" label="Mixture-of-Experts"/>
    <category term="NaturalLanguageProcessing" label="Natural Language Processing"/>
    <category term="Vision-LanguageModels" label="Vision-Language Models"/>
    <published>2024-12-18T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://mlguide.in/content/resources/blogs/2024/Prompt%20tuning.html</id>
    <title>Prompt Tuning</title>
    <updated>2024-04-10T00:00:00+00:00</updated>
    <author>
      <name>Anukool Chaturvedi</name>
    </author>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Training large pretrained language models is very time-consuming and compute-intensive. As they continue to grow in size, there is increasing interest in more efficient training methods such as &lt;em&gt;prompting&lt;/em&gt;.&lt;/p&gt;
&lt;/p&gt;

    &lt;script type="text/x-thebe-config"&gt;
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/resources"
        },
        predefinedOutput: true
    }
    &lt;/script&gt;
    &lt;script&gt;kernelName = 'python3'&lt;/script&gt;</content>
    <link href="https://mlguide.in/content/resources/blogs/2024/Prompt%20tuning.html"/>
    <summary>Training large pretrained language models is very time-consuming and compute-intensive. As they continue to grow in size, there is increasing interest in more efficient training methods such as prompting.</summary>
    <category term="LLM" label="LLM"/>
    <category term="Prompt-Tuning" label="Prompt-Tuning"/>
    <published>2024-04-10T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://mlguide.in/content/resources/blogs/2024/LoRA%20%28Low%20Rank%20Adaptation%20of%20Large%20Language%20Models%29.html</id>
    <title>LoRA (Low Rank Adaptation of Large Language Models)</title>
    <updated>2024-04-10T00:00:00+00:00</updated>
    <author>
      <name>Anukool Chaturvedi</name>
    </author>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Before diving into LoRA, it’s essential to grasp the concept of &lt;strong&gt;Matrix Decomposition&lt;/strong&gt;.&lt;/p&gt;
&lt;/p&gt;

    &lt;script type="text/x-thebe-config"&gt;
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/resources"
        },
        predefinedOutput: true
    }
    &lt;/script&gt;
    &lt;script&gt;kernelName = 'python3'&lt;/script&gt;</content>
    <link href="https://mlguide.in/content/resources/blogs/2024/LoRA%20%28Low%20Rank%20Adaptation%20of%20Large%20Language%20Models%29.html"/>
    <summary>Before diving into LoRA, it’s essential to grasp the concept of Matrix Decomposition.</summary>
    <category term="Fine-Tuning" label="Fine-Tuning"/>
    <category term="LLM" label="LLM"/>
    <category term="LoRA" label="LoRA"/>
    <published>2024-04-10T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://mlguide.in/content/resources/blogs/2024/A%20society%20of%20Generative%20AI%20agents%20%21.html</id>
    <title>A society of Generative AI agents !</title>
    <updated>2023-08-19T00:00:00+00:00</updated>
    <author>
      <name>Anukool Chaturvedi</name>
    </author>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;Imagine, you visit a place where a group of bots have created a society of their own. They have their own shops, homes, schools and they interact with knowns and unknowns, build relationships, do get-togethers, plan a party or say they create an entire ecosystem of their own just like humans do. It may be hard to imagine but fun to think of.&lt;/p&gt;
&lt;/p&gt;

    &lt;script type="text/x-thebe-config"&gt;
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/resources"
        },
        predefinedOutput: true
    }
    &lt;/script&gt;
    &lt;script&gt;kernelName = 'python3'&lt;/script&gt;</content>
    <link href="https://mlguide.in/content/resources/blogs/2024/A%20society%20of%20Generative%20AI%20agents%20%21.html"/>
    <summary>Imagine, you visit a place where a group of bots have created a society of their own. They have their own shops, homes, schools and they interact with knowns and unknowns, build relationships, do get-togethers, plan a party or say they create an entire ecosystem of their own just like humans do. It may be hard to imagine but fun to think of.</summary>
    <category term="AI" label="AI"/>
    <category term="GenerativeAI" label="GenerativeAI"/>
    <published>2023-08-19T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://mlguide.in/content/resources/blogs/2024/A%20Journey%20Through%20Time-%20The%20Transformation%20of%20AI%20Development.html</id>
    <title>A Journey Through Time- The Transformation of AI Development</title>
    <updated>2023-03-01T00:00:00+00:00</updated>
    <author>
      <name>Anukool Chaturvedi</name>
    </author>
    <content type="html">&lt;p class="ablog-post-excerpt"&gt;&lt;p&gt;I started studying about Machine Learning in 2016, when I took a famous course by Andrew Ng on Machine Learning. After a lot of breaks in between, I was able to complete it and learnt the basics.&lt;/p&gt;
&lt;/p&gt;

    &lt;script type="text/x-thebe-config"&gt;
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/resources"
        },
        predefinedOutput: true
    }
    &lt;/script&gt;
    &lt;script&gt;kernelName = 'python3'&lt;/script&gt;</content>
    <link href="https://mlguide.in/content/resources/blogs/2024/A%20Journey%20Through%20Time-%20The%20Transformation%20of%20AI%20Development.html"/>
    <summary>I started studying about Machine Learning in 2016, when I took a famous course by Andrew Ng on Machine Learning. After a lot of breaks in between, I was able to complete it and learnt the basics.</summary>
    <category term="AI" label="AI"/>
    <published>2023-03-01T00:00:00+00:00</published>
  </entry>
</feed>
