
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>LoRA (Low Rank Adaptation of Large Language Models)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/intro.css?v=adbe4504" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/custom.css?v=22107f9c" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-GJG3T4ZRZH"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-GJG3T4ZRZH');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-GJG3T4ZRZH');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/resources/blogs/2024/LoRA (Low Rank Adaptation of Large Language Models)';</script>
    <script src="../../../../_static/subscription_overlay.js?v=2e74803e"></script>
    <link rel="canonical" href="https://mlguide.in/content/resources/blogs/2024/LoRA (Low Rank Adaptation of Large Language Models).html" />
    <link rel="icon" href="../../../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" /> 
<link
  rel="alternate"
  type="application/atom+xml"
  href="../../blogs/atom.xml"
  title="Blog"
/>
  
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../../_static/MLGuide_logo_nb.png" class="logo__image only-light" alt=" - Home"/>
    <img src="../../../../_static/MLGuide_logo_nb.png" class="logo__image only-dark pst-js-only" alt=" - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../python/python_toc.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../python/1_installation.html">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/2_syntax_and_symantics.html">Syntax &amp; Symantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/3_functions_and_modules.html">Functions &amp; Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/4_Object_Oriented.html">Object Oriented Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/5_Exceptions_Handling.html">Exceptions Handling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/6_Handling_Files.html">Handling Files</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/7_Datetime_Operations.html">Datetime Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/8_advanced.html">Advanced Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/conceptual_topics.html">Interpreter vs Compiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../statistics/statistics-101.html">Statistics</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../mathematics/mathematics_toc.html">Mathematics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../mathematics/linear-algebra_vectors.html">Vectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mathematics/linear-algebra_matrices.html">Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../mathematics/dissimilarity_measures.html">Similarity measure</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../analytics/intro_analytics.html">Data analytics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../analytics/numpy/numpy_toc.html">Numpy</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/numpy/001_Python_NumPy.html">NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/numpy/Python_Numpy_Exercises_with_hints.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../analytics/pandas/pandas_toc.html">Pandas</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/pandas/001_Python_Pandas_DataFrame.html">Pandas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/pandas/002_Pandas_HowTos.html">How To's</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/pandas/003_Pandas_Exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../analytics/matplotlib/matplotlib_toc.html">Matplotlib</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/matplotlib/001_Python_Matplotlib.html">Matplotlib</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../analytics/matplotlib/003_Python_Matplotlib_Exercises.html">Exercises</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../ai/Introduction_to_ml.html">Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/classicml/concepts/01_Introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/classicml/concepts/000_Data_Exploration.html">Exploratory Data Analysis</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../ai/classicml/concepts/001_Data_Preparation.html">Data Preparation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/classicml/concepts/002_Regression.html">Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/classicml/concepts/003_Classification.html">Classfication</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../ai/classicml/concepts/004_Clustering.html">Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/classicml/concepts/005_Evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/classicml/concepts/006_Advanced.html">K-Fold Cross Validation</a></li>


<li class="toctree-l2"><a class="reference internal" href="../../../ai/classicml/concepts/007_Dimensionality_Reduction.html">Dimensionality Reduction</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../ai/neural/neural_toc.html">Neural Networks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/neural/concepts/001_Introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/neural/concepts/002_Backpropogation.html">Backpropogation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/neural/concepts/003_Activations.html">Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/neural/concepts/004_Optimization.html">Optimizations</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../ai/neural/concepts/pytorch/pytorch_toc.html">Pytorch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../ai/neural/concepts/pytorch/00_pytorch_fundamentals.html">Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ai/neural/concepts/pytorch/01_pytorch_workflow.html">Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ai/neural/concepts/pytorch/02_pytorch_classification.html">Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ai/neural/concepts/pytorch/03_pytorch_computer_vision.html">Computer Vision</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ai/neural/concepts/pytorch/04_pytorch_custom_datasets.html">Custom Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ai/neural/concepts/pytorch/06_pytorch_transfer_learning.html">Transfer Learning</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../ai/nlp/nlp_intro.html">Natural Language Processing</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/nlp/concepts/001_traditional_nlp.html">Word Vectors &amp; Dependency Parsing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/nlp/concepts/002_embeddings.html">Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/nlp/concepts/003_ngram_cnn.html">N Gram using CNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/nlp/concepts/004_word2vec.html">Word2Vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/nlp/concepts/005_language_model_basic.html">Neural Language Model</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../ai/nlp/concepts/006_language_model_rnn.html">Recurrent Neural Network (RNN)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../ai/nlp/concepts/007_encoder_decoder.html">Encoder Decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/nlp/concepts/008_attention.html">Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/nlp/concepts/009_transformer.html">Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/nlp/concepts/010_llm_tasks.html">Language Modelling Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ai/nlp/concepts/011_appendix.html">Appendix</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../genai/introduction.html">Generative AI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../genai/prompt-engineering/intro.html">Prompt Engineering</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/prompt-engineering/basic_prompting.html">Basic Prompting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/prompt-engineering/advance_prompts.html">Advanced Prompting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/prompt-engineering/prompts-applications.html">Prompts Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/prompt-engineering/prompts-adversarial.html">Prompts Adversarial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/prompt-engineering/prompts-reliability.html">Reliability</a></li>



</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../genai/langchain/intro.html">Langchain</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/langchain/01_LangChain_Fundamentals.html">Langchain Cookbook 1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/langchain/02_LangChain_Use_Cases.html">Langchain Cookbook 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../genai/langchain/projects/project_toc.html">Projects</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../genai/RAG/intro.html">RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../genai/agents/intro.html">Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../genai/llm-recipes/intro.html">LLM Recipes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../genai/evaluations/intro.html">Evaluations</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../blogs_toc.html">Blogs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../courses/courses_toc.html">Courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../books/books_toc.html">Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../github/awesome-repos.html">Github</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../readings/articles.html">News</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../readings/papers_toc.html">Research papers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../intro_me.html">About me</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../../_sources/content/resources/blogs/2024/LoRA (Low Rank Adaptation of Large Language Models).md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LoRA (Low Rank Adaptation of Large Language Models)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-language-models-existing-approaches">Training Language Models: Existing Approaches</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introducing-lora-low-rank-adaptation">Introducing LoRA: Low-Rank Adaptation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#which-weight-matrices-should-be-adapted">Which weight matrices should be adapted</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-should-be-the-optimal-rank-r">What should be the optimal rank <span class="math notranslate nohighlight">\(r\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-between-w-w">Correlation between <span class="math notranslate nohighlight">\(ΔW\)</span> &amp; <span class="math notranslate nohighlight">\(W\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-lora">Advantages of LoRA</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                   <section class="tex2jax_ignore mathjax_ignore" id="lora-low-rank-adaptation-of-large-language-models">
<h1>LoRA (Low Rank Adaptation of Large Language Models)<a class="headerlink" href="#lora-low-rank-adaptation-of-large-language-models" title="Link to this heading">#</a></h1>
<p>Before diving into LoRA, it’s essential to grasp the concept of <strong>Matrix Decomposition</strong>.</p>
<p>Matrix Decomposition involves breaking down large matrices into smaller ones while retaining as much information as possible. This process is particularly useful when dealing with large rank matrices, such as those in Large Language Models (LLMs). By removing highly correlated features (or repeated sets of information), we can compress the data into smaller matrices.</p>
<blockquote>
<div><p>For example, in images, multiple pixels that are close to each other may be highly correlated. These highly correlated matrices are called <strong>Low-Rank Matrices</strong> because essential information is present only in fewer rows or columns…</p>
</div></blockquote>
<p>The essence of <strong>Low-Rank Matrix Factorization</strong> lies in decomposing a large matrix into a product of two or more smaller, simpler matrices with a lower rank.</p>
<p>The <strong>goal of Low-Rank Matrix Factorization</strong> is to approximate a high-dimensional matrix <strong>M</strong> and <span class="math notranslate nohighlight">\(B\)</span> as a product of two smaller matrices <strong>A</strong> and <strong>B</strong></p>
<p>Mathematically, this can be expressed as <strong>M ≈ A x B,</strong> where <strong>M</strong> is an <em>m x n</em> matrix, <strong>A</strong> is an <em>m x k</em>  matrix, and <strong>B</strong> is a <em>k × n</em> matrix, with k (the rank) being much smaller than both <strong>m</strong> and <strong>n</strong>.</p>
<p><em>The purpose of this decomposition is to reduce the dimensionality of the matrix while retaining its primary features and structure.</em></p>
<p>For a detailed explanation, refer to the blog post below:</p>
<p><a class="reference external" href="https://medium.com/&#64;rendazhang/matrix-decomposition-series-6-low-rank-matrix-factorization-5a3b96832bad">Matrix Decomposition Series: Low-Rank Matrix Factorization</a></p>
<section id="training-language-models-existing-approaches">
<h2>Training Language Models: Existing Approaches<a class="headerlink" href="#training-language-models-existing-approaches" title="Link to this heading">#</a></h2>
<p>When training a language model on your dataset, you typically have a few options:</p>
<ol class="arabic simple">
<li><p><strong>Full Fine-Tuning</strong>: Retraining all model parameters becomes less feasible as model size increases. For example, fine-tuning GPT-3 with 175 billion parameters for multiple instances is prohibitively expensive.</p></li>
<li><p><strong>Fine-Tuning with Additional Layers</strong>: Adding trainable layers on top of a pre-trained model, known as adapter layers, introduces additional latency and doesn’t integrate well with distributed processing, as adapter layers must be inferred sequentially.</p></li>
<li><p><strong>Prompt Tuning</strong>:</p>
<ul class="simple">
<li><p><strong>Hard Prompt Tuning</strong>: Involves creating handcrafted text prompts with discrete input tokens, which is labor-intensive.</p></li>
<li><p><strong>Soft Prompt Tuning</strong>: Uses learnable tensors concatenated with input embeddings optimized for a dataset. However, these “virtual tokens” are not human-readable, as they don’t match real-word embeddings.</p></li>
</ul>
</li>
</ol>
</section>
<section id="introducing-lora-low-rank-adaptation">
<h2>Introducing LoRA: Low-Rank Adaptation<a class="headerlink" href="#introducing-lora-low-rank-adaptation" title="Link to this heading">#</a></h2>
<p>Fine-tuning large models like GPT-3 by retraining all parameters is both computationally and memory expensive. <strong>Low-Rank Adaptation (LoRA)</strong> offers a solution by freezing pre-trained model weights and injecting <strong>trainable rank decomposition matrices</strong> into each layer of the Transformer architecture. This significantly reduces the number of trainable parameters and GPU memory requirements, while maintaining or even improving model performance, thus accelerating fine-tuning while consuming less memory.</p>
<p>A neural network contains many dense layers which perform matrix multiplication. The weight matrices in these layers typically have full-rank. When adapting to a specific task, the pre-trained language models have a low <strong>“instrisic dimension”</strong> and can still learn efficiently despite a random projection to a smaller subspace.</p>
<p>Inspired by this, there is a hypothesis that the updates to the weights also have a low <strong>“intrinsic rank”</strong> during adaptation.  For a pre-trained weight matrix <span class="math notranslate nohighlight">\(W_0 ∈  W_0 \in \mathbb{R}^{d \times k}\)</span> , we constrain its update by representing the latter with a low-rank decomposition  <span class="math notranslate nohighlight">\(W_0 + \Delta W = W_0 + BA\)</span>, where <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}\)</span> , and the rank r
<span class="math notranslate nohighlight">\(r \leq \min(d, k)\)</span>. During training, <span class="math notranslate nohighlight">\(W_{0}\)</span> is frozen and does not receive gradient updates, while A and B contain trainable parameters. Note both <span class="math notranslate nohighlight">\(W_{0}\)</span> and <span class="math notranslate nohighlight">\(∆W = BA\)</span> are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For <span class="math notranslate nohighlight">\(h = W_{0}x\)</span>, our modified forward pass yields: $<span class="math notranslate nohighlight">\(h = W_{0}x + ∆W x = W_{0}x + BAx\)</span><span class="math notranslate nohighlight">\(
At the beginning of training \)</span>A<span class="math notranslate nohighlight">\( is initialised with random Gaussian initialisation, while \)</span>B$ is initialised with 0.</p>
<p>LoRA represents the weight updates <strong>ΔW</strong> with two smaller matrices (called <em>update matrices</em>) through low-rank decomposition. These new matrices can be trained to adapt to new data while keeping the overall number of parameters low. The original weight matrix remains frozen and doesn’t receive further updates. To produce the final results, the original and adapted weights are combined. Alternatively, you can merge the adapter weights with the base model to eliminate inference latency.</p>
<p><img alt="LoRA" src="../../../../_images/lora_animated.gif" /></p>
</section>
<section id="which-weight-matrices-should-be-adapted">
<h2>Which weight matrices should be adapted<a class="headerlink" href="#which-weight-matrices-should-be-adapted" title="Link to this heading">#</a></h2>
<p>Based on the findings mentioned in the paper, adapting LoRA to both <span class="math notranslate nohighlight">\(W_{}q\)</span>​ and <span class="math notranslate nohighlight">\(W_{v}\)</span> together​ yields the best downstream performance.</p>
</section>
<section id="what-should-be-the-optimal-rank-r">
<h2>What should be the optimal rank <span class="math notranslate nohighlight">\(r\)</span><a class="headerlink" href="#what-should-be-the-optimal-rank-r" title="Link to this heading">#</a></h2>
<p>LoRA already performs competitively with a very small r (more so for <span class="math notranslate nohighlight">\({Wq, Wv}\)</span> than just <span class="math notranslate nohighlight">\(Wq\)</span>). This suggests the update matrix <span class="math notranslate nohighlight">\(∆W\)</span> could have a very small “intrinsic rank”. Small ranks like <span class="math notranslate nohighlight">\(r=1\)</span> and <span class="math notranslate nohighlight">\(r=2\)</span> can be surprisingly effective.</p>
</section>
<section id="correlation-between-w-w">
<h2>Correlation between <span class="math notranslate nohighlight">\(ΔW\)</span> &amp; <span class="math notranslate nohighlight">\(W\)</span><a class="headerlink" href="#correlation-between-w-w" title="Link to this heading">#</a></h2>
<p>∆W has a stronger correlation with <span class="math notranslate nohighlight">\(W\)</span> compared to a random matrix,</p>
<ol class="arabic simple">
<li><p>Indicating that ∆W amplifies some features that are already in <span class="math notranslate nohighlight">\(W\)</span>.</p></li>
<li><p>Second, instead of repeating the top singular directions of <span class="math notranslate nohighlight">\(W\)</span>, <span class="math notranslate nohighlight">\(∆W\)</span> only amplifies directions that are not emphasized in <span class="math notranslate nohighlight">\(W\)</span>.</p></li>
<li><p>Third, the amplification factor is rather huge: 21.5 ≈ 6.91/0.32 for r = 4.</p></li>
</ol>
<p><em>This suggests that the low-rank adaptation matrix potentially amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pre-training model.</em></p>
</section>
<section id="advantages-of-lora">
<h2>Advantages of LoRA<a class="headerlink" href="#advantages-of-lora" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Efficiency</strong>: LoRA drastically reduces the number of trainable parameters.</p></li>
<li><p><strong>Flexibility</strong>: The original <strong>pretrained weights are kept frozen</strong>, allowing multiple lightweight and portable LoRA models to be built for various downstream tasks.</p></li>
<li><p><strong>Compatibility</strong>: LoRA is orthogonal to other parameter-efficient methods and can be combined with many of them.</p></li>
<li><p><strong>Performance</strong>: Models fine-tuned using LoRA perform comparably to fully fine-tuned models.</p></li>
</ul>
<p>In principle, LoRA can be applied to any subset of weight matrices in a neural network to reduce the number of trainable parameters. However, for simplicity and further parameter efficiency, LoRA is typically only applied to the attention blocks in Transformer models. The resulting number of trainable parameters in a LoRA model depends on the size of the update matrices, which is determined mainly by the rank <code class="docutils literal notranslate"><span class="pre">r</span></code> and the shape of the original weight matrix.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "anukchat/mlguide",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/resources/blogs/2024"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
<div class="section ablog__blog_comments">
     
<div class="section ablog__prev-next">
  <span class="ablog__prev">
     
    <a href="A%20society%20of%20Generative%20AI%20agents%20%21.html">
      
      <i class="fa fa-arrow-circle-left"></i>
      
      <span>A society of Generative AI agents !</span>
    </a>
    
  </span>
  <span class="ablog__spacer">&nbsp;</span>
  <span class="ablog__next">
     
    <a href="Prompt%20tuning.html">
      <span>Prompt Tuning</span>
      
      <i class="fa fa-arrow-circle-right"></i>
      
    </a>
    
  </span>
</div>
  
  <div class="section ablog__comments">
    <h2>Comments</h2>
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname = "mlguide";
      var disqus_identifier = "/content/resources/blogs/2024/LoRA (Low Rank Adaptation of Large Language Models)/";
      var disqus_title = "LoRA (Low Rank Adaptation of Large Language Models)";
      var disqus_url = "https://mlguide.in/content/resources/blogs/2024/LoRA (Low Rank Adaptation of Large Language Models)";

      (function () {
        var dsq = document.createElement("script");
        dsq.type = "text/javascript";
        dsq.async = true;
        dsq.src = "//" + disqus_shortname + ".disqus.com/embed.js";
        (
          document.getElementsByTagName("head")[0] ||
          document.getElementsByTagName("body")[0]
        ).appendChild(dsq);
      })();
    </script>
    <noscript>
      Please enable JavaScript to view the
      <a href="https://disqus.com/?ref_noscript">
        comments powered by Disqus.</a
      ></noscript
    >
    <a href="https://disqus.com" class="dsq-brlink">
      comments powered by <span class="logo-disqus">Disqus</span>
    </a>
  </div>
  
</div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div class="ablog-sidebar-item ablog__recentposts">
  <h3>
    <a href="../../blogs.html">Recent Posts</a>
  </h3>
  <ul>
     
    <li>
      <a href="Building%20Reliable%20Local%20Agents%20with%20LangGraph%2C%20LLaMA3%2C%20and%20Elasticsearch.html">
        25 December - Building Reliable Local Agents with LangGraph, LLaMA3, and Elasticsearch
      </a>
    </li>
    
    <li>
      <a href="Decoding%20the%20Byte%20Latent%20Transformer_%20A%20New%20Approach%20to%20Language%20Processing.html">
        22 December - Byte Latent Transformer: A New Approach to Language Processing
      </a>
    </li>
    
    <li>
      <a href="DeepSeek-VL2_%20A%20Powerful%20Vision-Language%20Model%20for%20Multimodal%20Understanding%20%281%29.html">
        18 December - DeepSeek-VL2: A Powerful Vision-Language Model for Multimodal Understanding
      </a>
    </li>
    
    <li>
      <a href="Prompt%20tuning.html">
        10 April - Prompt Tuning
      </a>
    </li>
    
    <li>
      <a href="A%20society%20of%20Generative%20AI%20agents%20%21.html">
        19 August - A society of Generative AI agents !
      </a>
    </li>
    
  </ul>
</div>
</div>

  <div class="sidebar-secondary-item">
<div class="ablog-sidebar-item ablog__categories">
  <h3>
    <a href="../category.html">Categories</a>
  </h3>
  <ul>
     
    <li>
      <a href="../category/llm.html">LLM (7)</a>
    </li>
     
  </ul>
</div>
</div>

  <div class="sidebar-secondary-item">
<div class="ablog-sidebar-item ablog__tagcloud">
  <link
    rel="stylesheet"
    href="../../../../_static/ablog/tagcloud.css"
    type="text/css"
  />
  <h3><a href="../tag.html">Tags</a></h3>
  <ul class="ablog-cloud">
     
    <li class="ablog-cloud ablog-cloud-5">
      <a href="../tag/ai.html">AI</a>
    </li>
        
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/computer-vision.html">Computer Vision</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/deep-learning.html">Deep Learning</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/elasticsearch.html">Elasticsearch</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/fine-tuning.html">Fine-Tuning</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/generativeai.html">GenerativeAI</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-2">
      <a href="../tag/llm.html">LLM</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/llama3.html">LLaMA3</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/langgraph.html">LangGraph</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/language-processing.html">Language Processing</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/large-language-models.html">Large Language Models</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/lora.html">LoRA</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/local-agents.html">Local Agents</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/machine-learning.html">Machine Learning</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/mixture-of-experts.html">Mixture-of-Experts</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/nlp.html">NLP</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/natural-language-processing.html">Natural Language Processing</a>
    </li>
        
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/prompt-tuning.html">Prompt-Tuning</a>
    </li>
        
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/rag.html">RAG</a>
    </li>
      
    <li class="ablog-cloud ablog-cloud-1">
      <a href="../tag/vision-language-models.html">Vision-Language Models</a>
    </li>
     
  </ul>
</div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Anukool Chaturvedi
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div class="social-icons">
    <a href="https://twitter.com/chaturanukool" target="_blank"><i class="fab fa-twitter"></i></a>
    <a href="https://linkedin.com/in/anukool-chaturvedi" target="_blank"><i class="fab fa-linkedin"></i></a>
    <a href="https://github.com/anukchat" target="_blank"><i class="fab fa-github"></i></a>
    </p>
</div>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>