{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Vanishing gradients*** occur when the gradients (partial derivatives of the loss function with respect to each weight) become very small as they propagate backward through a deep neural network during training. \n",
    "\n",
    "This is particularly common in networks with many layers, making it difficult for the model to learn.\n",
    "\n",
    "As gradients move backward from the output to the input layer, they can diminish exponentially, leading to the *vanishing gradients* problem.\n",
    "\n",
    "<img src='./img/stanford/7-vanishingGradients.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "üëâ In very deep networks, <mark>repeated multiplication of small gradients across layers causes the gradient to vanish</mark> as it propagates.\n",
    "\n",
    "\n",
    "For example: $\\frac{J^{(4)}}{h^{(1)}} < \\frac{J^{(4)}}{h^{(2)}} < \\frac{J^{(4)}}{h^{(3)}} < \\frac{J^{(4)}}{h^{(3)}}$. And more loops the training goes through, the longest distance gradients (weights far away from the J) tend to be vanishing as they approach 0.\n",
    "\n",
    "<img src='./img/stanford/7-whyVGaProblem.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "Because of vanishing gradients:\n",
    "\n",
    "- **Slow or Stalled Learning:** Layers receive very small updates, slowing down or halting learning altogether.\n",
    "\n",
    "- **Difficulty in Capturing Long-Range Dependencies:** The network struggles to learn relationships between distant elements, a key challenge in tasks like language modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too long a sequence. This is a major problem because for a vanilla RNN (the RNN introudced so far, the state is constantly updated in each time step, which makes it impossible or hardly possible for the model to preserve long-distance dependency. In other word, the longer distance a piece of info is, the harder it will be kept in the model. \n",
    "\n",
    "\n",
    "<img src='./img/stanford/7-VGEffectonRNN-LM0.png' width='600' height='300'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploding Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exploding gradients*** refer to a problem in the training of deep neural networks where the gradients (partial derivatives of the loss function with respect to each weight) become excessively large. This causes unstable updates to the network's weights, leading to erratic behavior during training.\n",
    "\n",
    "Similar to the vanishing gradients problem, exploding gradients occur during the backpropagation process, where gradients are propagated from the output layer back to the input layer to update the network's weights.\n",
    "\n",
    "<mark>Unlike vanishing gradients, which cause gradients to become too small, exploding gradients cause them to become excessively large, often leading to numerical instability.</mark>\n",
    "\n",
    "\n",
    "<img src='./img/stanford/7-explodingGradient.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "Because of Exploding Gradient we can face:\n",
    "\n",
    "- **Numerical Instability:** Extremely large gradients can cause numerical issues, such as overflow, resulting in `NaN` (Not a Number) values in the network's parameters.\n",
    "- **Diverging Loss:** Instead of minimizing the loss function, the network's loss may increase uncontrollably, preventing the model from learning effectively.\n",
    "- **Oscillating Weights:** Weight updates can become so large that the network oscillates around the minimum of the loss function without converging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To solve the problem of exploding gradient, we can use different techniques**\n",
    "\n",
    "\n",
    "<img src='./img/stanford/7-GradientClipping.png' width='600' height='300'>\n",
    "\n",
    "Reference: [‚ÄúDeep Learning‚Äù, Goodfellow, Bengio and Courville, 2016. Chapter 10.11.1. Sequence Modeling: Recurrent and Recursive Nets](https://www.deeplearningbook.org/contents/rnn.html)\n",
    "  \n",
    "<img src='./img/stanford/7-GradientClipping2.png' width='600' height='300'>\n",
    "\n",
    "üëâ Can also employ more sophisticated optimizers, like Adam, Adagrad, RMSprop etc., to overcome the exploding gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General solutions to vanishing/exploding gradient \n",
    "\n",
    "Obviously, vanishing/exploding gradient is a program that is not only relevant for RNN, but for all NN (including feed-forward and convolutional), especially deep ones. **Although, for RNN, these problems are more serious due to the design of RNN (i.e., the repeated multiplication by the same weight matrix)**. \n",
    "\n",
    "See: ‚ÄùLearning Long-Term Dependencies with Gradient Descent is Difficult\", Bengio et al. 1994, http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf.\n",
    "\n",
    "**<mark>Causes and solutions</mark>:** \n",
    "\n",
    "- Due to chain rule / choice of nonlinearity function, gradient can become vanishingly small as it backpropagates\n",
    "- Thus lower layers are learnt very slowly (hard to train)\n",
    "- **Solution**: lots of new deep feedforward/convolutional architectures that add more direct connections (thus allowing the gradient to flow)\n",
    "\n",
    "#### <mark>Residual connections</mark>\n",
    "\n",
    "- **Reference**: [He et al.2015. Deep Residual Learning for Image Recognition.](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "- This is a very general trick\n",
    "  \n",
    "<img src='./img/stanford/7-ResNet.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "#### <mark>Dense connections</mark>\n",
    "\n",
    "- **Reference**: ‚ÄùDensely Connected Convolutional Networks\", Huang et al, 2017. https://arxiv.org/pdf/1608.06993.pdf\n",
    "- This is more specific to CNN\n",
    "  \n",
    "<img src='./img/stanford/7-DenseNet.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "#### <mark>Highway connections</mark>\n",
    "\n",
    "- **Reference**: ‚ÄùHighway Networks\", Srivastava et al, 2015. https://arxiv.org/pdf/1505.00387.pdf\n",
    "- Highway connections aka ‚ÄúHighwayNet‚Äù\n",
    "- Similar to residual connections, but the identity connection vs the transformation layer is controlled by a dynamic gate\n",
    "- Inspired by LSTMs, but applied to deep feedforward/convolutional networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**: [Hochreiter and Schmidhuber, 1997. ‚ÄúLong short-term memory‚Äù.](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "\n",
    "<img src='./img/stanford/7-LSTMDesc.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "üëâ <mark>Forget gate is similar to the idea of Dropout in Deep Neural Network, an intuitive trick to reduce the risk of Vanishing Gradient.</mark>\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "<img src='./img/stanford/7-LSTMDesc2.png'>\n",
    "\n",
    "\n",
    "<img src='./img/stanford/7-LSTMDiag.png' width='600' height='300'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**: \"Learning Phrase Representations using RNN Encoder‚ÄìDecoder for Statistical Machine Translation\", Cho et al. 2014, https://arxiv.org/pdf/1406.1078v3.pdf\n",
    "\n",
    "<img src='./img/stanford/7-GRUDesc.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "### LSTM vs GRU\n",
    "\n",
    "üëâ Researchers have proposed many gated RNN variants, but LSTM and GRU are the most widely-used\n",
    "\n",
    "üëâ The biggest difference is that **GRU is quicker** to compute and has fewer parameters\n",
    "\n",
    "üëâ There is **no conclusive evidence** that one consistently performs better than the other\n",
    "\n",
    "üëâ **LSTM is a good default choice** (especially if your data has particularly long dependencies, or you have lots of training data)\n",
    "\n",
    "üëâ **Rule of thumb**: start with LSTM, but switch to GRU if you want something more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contextual representation**\n",
    "\n",
    "üëâ <mark>Look for both directions</mark>\n",
    "\n",
    "<img src='./img/stanford/7-BiRNN.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "### Structure \n",
    "<img src='./img/stanford/7-BiRNN2.png' width='600' height='300'>\n",
    "<img src='./img/stanford/7-BiRNN3.png' width='600' height='300'>\n",
    "<img src='./img/stanford/7-BiRNN4.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "### Restrictions\n",
    "\n",
    "<img src='./img/stanford/7-BiRNNRestriction.png' width='600' height='300'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer RNNs (Stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ  RNNs are already ‚Äúdeep‚Äù on one dimension (they unroll over many timesteps)\n",
    "\n",
    "üëâ  We can also make them ‚Äúdeep‚Äù in another dimension by applying multiple RNNs ‚Äì this is a multi-layer RNN.\n",
    "\n",
    "üëâ  This allows the network to compute more complex representations\n",
    "\n",
    "üëâ  The lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features.\n",
    "\n",
    "üëâ  Multi-layer RNNs are also called stacked RNNs.\n",
    "\n",
    "üëâ  This can be bidirectional provided that the entire input sentence is accessible.\n",
    "  \n",
    "<img src='./img/stanford/7-MultiLRNN.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "### In practice \n",
    "\n",
    "**Reference**: ‚ÄúMassive Exploration of Neural Machine Translation Architecutres‚Äù, Britz et al, 2017. https://arxiv.org/pdf/1703.03906.pdf\n",
    "\n",
    "üëâ <mark>Skips are usually heavily used. </mark>\n",
    "\n",
    "<img src='./img/stanford/7-MultiLRNNInPractice.png' width='600' height='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Text-to-Text Transfer Transformer (T5)__ is a large language model (LLM). A lot of people are working in this field and it's difficult to see who's methods are doing better if so many variables are changing. This paper was on seeing how far they could take the current tools available. \n",
    "\n",
    "In the paper they built their own dataset (c4, available on TensorFlow) to test how pretraining helps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./img/t5_pipeline.PNG) <br>\n",
    "_Figure 1. T5 pipeline._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
