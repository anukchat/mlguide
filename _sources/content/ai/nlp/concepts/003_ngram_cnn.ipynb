{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram detection with 1D Convolution\n",
    "\n",
    "Earlier we have run tasks on embeddings in each word, but we sometimes should handle a set of ordered items.<br>\n",
    "\n",
    "ðŸ‘‰ For instance, **\"hot dog\"** won't be in the **species of \"dog\"**, but it will be a part of food. \n",
    "\n",
    "ðŸ‘‰ **\"Paris Hilton\"** will also be far from **\"Paris\"** in language context. \n",
    "\n",
    "ðŸ‘‰ Even when you find **\"good\"** in the sentence, it might be a signal of negative sentiment in the context **\"not good\"**.<br>\n",
    "\n",
    "Not only bi-grams, but the same is true for tri-grams and generic N-grams.\n",
    "\n",
    "The convolution network (CNN) is a today's widely used model in computer vision (such as, image classification, object detection, segmentation, etc). In NLP, this convolutional architecture can also be applied in N-gram detection.<br>\n",
    "\n",
    "In computer vision, 2D convolution (convolution by 2 dimensions of width and height) is generally used, but in N-gram detection, 1D convolution is applied as follows.\n",
    "\n",
    "![Bi-gram CNN](images/bigram_convolution.png)\n",
    "\n",
    "There exist several variations for N-gram detection in NLP by convolutions.<br>\n",
    "\n",
    "The **hierarchical convolutions** can capture **patterns with gaps**, such as, \"not --- good\" or \"see --- little\" where \"---\" stands for a short sequence of words.<br>\n",
    "\n",
    "Similar to image processing, **multiple channels** can also be applied in NLP convolution. For instance, when each word has multiple embeddings (such as, word embedding, POS-tag embedding, position-wise word embedding, etc), these embeddings can be manipulated as multiple channels in NLP.<br>\n",
    "\n",
    "Or, after applying multiple N-grams (such as, 2-gram, 4-gram, and 6-gram), the results can also be manipulated as multiple channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.3.0 torchtext==0.18.0 --no-cache-dir --extra-index-url https://download.pytorch.org/whl/cu113\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0\n",
      "torchtext version: 0.18.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print('PyTorch version:', torch.__version__)\n",
    "\n",
    "import torchtext\n",
    "print('torchtext version:', torchtext.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use text in news papers dataset. (In this example, we use 2 columns of \"headline\" and \"short description\".)\n",
    "\n",
    "Before starting, please download [News_Category_Dataset_v3.json](https://www.kaggle.com/datasets/rmisra/news-category-dataset) (collected by HuffPost) in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(\"News_Category_Dataset_v3.json\",lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll apply text classification task.\n",
    "\n",
    "The words appearing in the former part in sequence will be more indicative (topical) rather than the latter part. \n",
    "\n",
    "For this reason, in practical text classification, a long text will then be separated into **regions**. In each region, the convolution (with pooling) is then applied and concatenated. (See below.)<br>\n",
    "\n",
    "For instance, with RCV1 (Reuters Corpus Volume I) dataset, 20 equally sized regions has better performance in category classification. (See [Johnson and Zhang (2015)](https://arxiv.org/abs/1504.01255).)\n",
    "\n",
    "![region separation](images/region_separation.png)\n",
    "\n",
    "In this example, ```headline``` and ```short_description``` are both short text, and we then treat these features as regions, instead of separating a single text into regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline  \\\n",
       "0       Over 4 Million Americans Roll Up Sleeves For O...   \n",
       "1       American Airlines Flyer Charged, Banned For Li...   \n",
       "2       23 Of The Funniest Tweets About Cats And Dogs ...   \n",
       "3       The Funniest Tweets From Parents This Week (Se...   \n",
       "4       Woman Who Called Cops On Black Bird-Watcher Lo...   \n",
       "...                                                   ...   \n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...   \n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I...   \n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M...   \n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...   \n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ...   \n",
       "\n",
       "                                        short_description  \n",
       "0       Health experts said it is too early to predict...  \n",
       "1       He was subdued by passengers and crew when he ...  \n",
       "2       \"Until you have a dog you don't understand wha...  \n",
       "3       \"Accidentally put grown-up toothpaste on my to...  \n",
       "4       Amy Cooper accused investment firm Franklin Te...  \n",
       "...                                                   ...  \n",
       "209522  Verizon Wireless and AT&T are already promotin...  \n",
       "209523  Afterward, Azarenka, more effusive with the pr...  \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...  \n",
       "209525  CORRECTION: An earlier version of this story i...  \n",
       "209526  The five-time all-star center tore into his te...  \n",
       "\n",
       "[209527 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = data[[\"headline\", \"short_description\"]]\n",
    "label_data = data[\"category\"]\n",
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the better performance (accuracy), we standarize the input text as follows.\n",
    "- Make all words to lowercase in order to reduce words\n",
    "- Make \"-\" (hyphen) to space\n",
    "- Remove stop words\n",
    "- Remove all punctuation\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Here I have removed stop words, but we need to take care if we train model for other tasks (such as, sentiment detection), since it might include important words for n-gram detection (such as, \"not\", \"don't\", \"isn't\", etc).\n",
    "\n",
    "Lemmatization (standardization for such as \"have\", \"had\" or \"having\") should be dealed with, but here I have skipped these pre-processing.<br>\n",
    "\n",
    "In the strict pre-processing, we should also care about the polysemy. (The different meanings in the same word should have different tokens.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to lowercase\n",
    "text_data = text_data.apply(lambda x: x.str.lower())\n",
    "\n",
    "# Replace hyphens with spaces\n",
    "text_data = text_data.apply(lambda x: x.str.replace(\"-\", \" \", regex=False))\n",
    "\n",
    "# Remove stop words (only when they include punctuation)\n",
    "punct_pattern = re.compile(r\"(^|\\w+)[%s](\\w+|$)\" % re.escape(string.punctuation))\n",
    "\n",
    "for w in stopwords.words(\"english\"):\n",
    "    if punct_pattern.match(w):\n",
    "        # Use raw string for the pattern\n",
    "        pattern = re.compile(r\"(^|\\s+)%s(\\s+|$)\" % re.escape(w))\n",
    "        text_data = text_data.apply(lambda x: x.str.replace(pattern, \" \", regex=True))\n",
    "\n",
    "text_data = text_data.apply(lambda x: x.str.strip())\n",
    "\n",
    "# Remove punctuation\n",
    "punct_replace_pattern = re.compile(r\"[%s]\" % re.escape(string.punctuation))\n",
    "\n",
    "text_data = text_data.apply(lambda x: x.str.replace(punct_replace_pattern, \"\", regex=True))\n",
    "text_data = text_data.apply(lambda x: x.str.strip())\n",
    "\n",
    "# Remove stop words (only when they don't include punctuation)\n",
    "non_punct_pattern = re.compile(r\"(^|\\w+)[%s](\\w+|$)\" % re.escape(string.punctuation))\n",
    "\n",
    "for w in stopwords.words(\"english\"):\n",
    "    if not non_punct_pattern.match(w):\n",
    "        # Use raw string for the pattern\n",
    "        pattern = re.compile(r\"(^|\\s+)%s(\\s+|$)\" % re.escape(w))\n",
    "        text_data = text_data.apply(lambda x: x.str.replace(pattern, \" \", regex=True))\n",
    "\n",
    "text_data = text_data.apply(lambda x: x.str.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 million americans roll sleeves omicron targe...</td>\n",
       "      <td>health experts said early predict whether dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american airlines flyer charged banned life pu...</td>\n",
       "      <td>subdued passengers crew fled back aircraft con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 funniest tweets cats dogs week sept 17 23</td>\n",
       "      <td>dog understand could eaten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>funniest tweets parents week sept 17 23</td>\n",
       "      <td>accidentally put grown toothpaste toddlerâ€™s to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woman called cops black bird watcher loses law...</td>\n",
       "      <td>amy cooper accused investment firm franklin te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>rim ceo thorsten heins significant plans black...</td>\n",
       "      <td>verizon wireless att already promoting lte dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>maria sharapova stunned victoria azarenka aust...</td>\n",
       "      <td>afterward azarenka effusive press normal credi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>giants patriots jets colts among improbable su...</td>\n",
       "      <td>leading super bowl xlvi talked game could end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>aldon smith arrested 49ers linebacker busted dui</td>\n",
       "      <td>correction earlier version story incorrectly s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>dwight howard rips teammates magic loss hornets</td>\n",
       "      <td>five time star center tore teammates friday ni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline  \\\n",
       "0       4 million americans roll sleeves omicron targe...   \n",
       "1       american airlines flyer charged banned life pu...   \n",
       "2            23 funniest tweets cats dogs week sept 17 23   \n",
       "3                 funniest tweets parents week sept 17 23   \n",
       "4       woman called cops black bird watcher loses law...   \n",
       "...                                                   ...   \n",
       "209522  rim ceo thorsten heins significant plans black...   \n",
       "209523  maria sharapova stunned victoria azarenka aust...   \n",
       "209524  giants patriots jets colts among improbable su...   \n",
       "209525   aldon smith arrested 49ers linebacker busted dui   \n",
       "209526    dwight howard rips teammates magic loss hornets   \n",
       "\n",
       "                                        short_description  \n",
       "0       health experts said early predict whether dema...  \n",
       "1       subdued passengers crew fled back aircraft con...  \n",
       "2                              dog understand could eaten  \n",
       "3       accidentally put grown toothpaste toddlerâ€™s to...  \n",
       "4       amy cooper accused investment firm franklin te...  \n",
       "...                                                   ...  \n",
       "209522  verizon wireless att already promoting lte dev...  \n",
       "209523  afterward azarenka effusive press normal credi...  \n",
       "209524  leading super bowl xlvi talked game could end ...  \n",
       "209525  correction earlier version story incorrectly s...  \n",
       "209526  five time star center tore teammates friday ni...  \n",
       "\n",
       "[209527 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we convert a category name (e.g, \"WORLD NEWS\") to label ID (e.g, 2).<br>\n",
    "\n",
    "First we build functions for convertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'U.S. NEWS': 0,\n",
       " 'COMEDY': 1,\n",
       " 'PARENTING': 2,\n",
       " 'WORLD NEWS': 3,\n",
       " 'CULTURE & ARTS': 4,\n",
       " 'TECH': 5,\n",
       " 'SPORTS': 6,\n",
       " 'ENTERTAINMENT': 7,\n",
       " 'POLITICS': 8,\n",
       " 'WEIRD NEWS': 9,\n",
       " 'ENVIRONMENT': 10,\n",
       " 'EDUCATION': 11,\n",
       " 'CRIME': 12,\n",
       " 'SCIENCE': 13,\n",
       " 'WELLNESS': 14,\n",
       " 'BUSINESS': 15,\n",
       " 'STYLE & BEAUTY': 16,\n",
       " 'FOOD & DRINK': 17,\n",
       " 'MEDIA': 18,\n",
       " 'QUEER VOICES': 19,\n",
       " 'HOME & LIVING': 20,\n",
       " 'WOMEN': 21,\n",
       " 'BLACK VOICES': 22,\n",
       " 'TRAVEL': 23,\n",
       " 'MONEY': 24,\n",
       " 'RELIGION': 25,\n",
       " 'LATINO VOICES': 26,\n",
       " 'IMPACT': 27,\n",
       " 'WEDDINGS': 28,\n",
       " 'COLLEGE': 29,\n",
       " 'PARENTS': 30,\n",
       " 'ARTS & CULTURE': 31,\n",
       " 'STYLE': 32,\n",
       " 'GREEN': 33,\n",
       " 'TASTE': 34,\n",
       " 'HEALTHY LIVING': 35,\n",
       " 'THE WORLDPOST': 36,\n",
       " 'GOOD NEWS': 37,\n",
       " 'WORLDPOST': 38,\n",
       " 'FIFTY': 39,\n",
       " 'ARTS': 40,\n",
       " 'DIVORCE': 41}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_array = label_data.unique()\n",
    "category_dic = {c: i for i, c in enumerate(category_array)}\n",
    "itoc = list(category_dic.keys())\n",
    "ctoi = category_dic\n",
    "category_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctoi[\"WORLD NEWS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WORLD NEWS'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itoc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert all label to label IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         1\n",
       "3         2\n",
       "4         0\n",
       "         ..\n",
       "209522    5\n",
       "209523    6\n",
       "209524    6\n",
       "209525    6\n",
       "209526    6\n",
       "Name: category, Length: 209527, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data = label_data.apply(lambda y: ctoi[y])\n",
    "label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will tokenize, in which it converts each text to the sequence of word's indices as follows.<br>\n",
    "\n",
    "In this example, each text will be padded by the padding index (here, 50000) when the the length of text is smaller than 140.\n",
    "\n",
    "![Index vectorize](images/index_vectorize2.png)\n",
    "\n",
    "First we create a list of vocabulary (```vocab```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "vocab_size = 50000\n",
    "max_seq_len = 140\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# define tokenization function\n",
    "def yield_tokens(text_data):\n",
    "    for text in text_data:\n",
    "        tokens = tokenizer(text)\n",
    "        tokens = tokens[:max_seq_len]\n",
    "        yield tokens\n",
    "\n",
    "# union headline and short_description\n",
    "text_all = pd.concat([text_data[\"headline\"], text_data[\"short_description\"]])\n",
    "\n",
    "# build vocabulary list\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(text_all),\n",
    "    specials=[\"<unk>\"],\n",
    "    max_tokens=vocab_size\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated token index is ```0, 1, ... , vocab_size - 1```.<br>\n",
    "\n",
    "Now I will set ```vocab_size``` (here 50000) as a token id in padded positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_index = vocab.__len__()\n",
    "vocab.append_token(\"<pad>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list for both index-to-word and word-to-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = vocab.get_itos()\n",
    "stoi = vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of token index is 50001.\n",
      "The padded index is 50000.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(\"The number of token index is {}.\".format(vocab.__len__()))\n",
    "print(\"The padded index is {}.\".format(stoi[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a collator function, which is used for pre-processing in data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, head_token_list, desc_token_list = [], [], []\n",
    "    for label, head, desc in batch:\n",
    "\n",
    "        # 1. skip None data\n",
    "        if head is None or desc is None:\n",
    "            continue\n",
    "\n",
    "        # 2. generate word's index vector\n",
    "        head_token = vocab(tokenizer(head))\n",
    "        desc_token = vocab(tokenizer(desc))\n",
    "\n",
    "        # 3. limit token length to max_seq_len\n",
    "        head_token = head_token[:max_seq_len]\n",
    "        desc_token = desc_token[:max_seq_len]\n",
    "\n",
    "        # 4. pad sequence\n",
    "        head_token += [pad_index] * (max_seq_len - len(head_token))\n",
    "        desc_token += [pad_index] * (max_seq_len - len(desc_token))\n",
    "\n",
    "        # add to list\n",
    "        label_list.append(label)\n",
    "        head_token_list.append(head_token)\n",
    "        desc_token_list.append(desc_token)\n",
    "        \n",
    "    # convert to tensor\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64).to(device)\n",
    "    head_token_list = torch.tensor(head_token_list, dtype=torch.int64).to(device)\n",
    "    desc_token_list = torch.tensor(desc_token_list, dtype=torch.int64).to(device)\n",
    "    return label_list, head_token_list, desc_token_list\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    list(zip(label_data, text_data[\"headline\"], text_data[\"short_description\"])),\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape in batch : torch.Size([512])\n",
      "headline token shape in batch : torch.Size([512, 140])\n",
      "short_desc token shape in batch : torch.Size([512, 140])\n",
      "***** label sample *****\n",
      "tensor(28)\n",
      "***** headline token sample *****\n",
      "tensor([ 2020, 19177,   745, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000])\n",
      "***** short_desc token sample *****\n",
      "tensor([    5,   286,  2520,   973, 11909,   745,  9374,    18,    52,   350,\n",
      "          251,  3303,    34,    45,  3128,   247,   607,  1408,  2936,   470,\n",
      "         2359,  1087,  3108, 19177,   745, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for labels, heads, descs in dataloader:\n",
    "    break\n",
    "\n",
    "print(\"label shape in batch : {}\".format(labels.size()))\n",
    "print(\"headline token shape in batch : {}\".format(heads.size()))\n",
    "print(\"short_desc token shape in batch : {}\".format(descs.size()))\n",
    "print(\"***** label sample *****\")\n",
    "print(labels[0])\n",
    "print(\"***** headline token sample *****\")\n",
    "print(heads[0])\n",
    "print(\"***** short_desc token sample *****\")\n",
    "print(descs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build network.<br>\n",
    "\n",
    "ðŸ‘‰ As you saw in previous examples, we build embedding vectors (dense vectors) $ \\{ \\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_m \\} $ from text for both ```headline``` and ```short_description``` respectively.\n",
    "\n",
    "ðŸ‘‰ For these embedding vectors, we apply 1D convolution $ \\mathbf{p}_i = g(U (\\mathbf{x}_i) + \\mathbf{b}) $ where $ \\mathbf{x}_i = [\\mathbf{w}_i, \\mathbf{w}_{i+1}] $, $U$ is a weight matrix, $\\mathbf{b}$ is a bias vector, and $ g() $ is RELU activaiton. (i.e, In convolutions, the size of window is 2 (bi-gram) and the size of stride is 1.)<br>\n",
    "\n",
    "In this example, we apply half padding convolution (i.e, apply $ \\mathbf{x}_i = [\\mathbf{w}_i, \\mathbf{w}_{i+1}] $ for $ i=1,\\ldots,m $ where $\\mathbf{w}_{m+1}$ is zero) and the number of outputs will then also be $m$.<br>\n",
    "\n",
    "I assume that the result is $n$-dimensional vectors $ \\mathbf{p}_1, \\mathbf{p}_2, \\cdots, \\mathbf{p}_m $ .\n",
    "\n",
    "ðŸ‘‰ Next we get $n$-dimensional vector $\\mathbf{c}$ by applying $\\mathbf{c}_{[j]} = \\max_{1 \\leq i \\leq m} \\mathbf{p}_{i [j]} \\forall j \\in [1,n]$. (i.e, max pooling)<br>\n",
    "\n",
    "Here I have denoted $j$-th element of vecotr $\\mathbf{p}_i$ by $\\mathbf{p}_{i [j]}$. ($i \\in [1,m], j \\in [1,n]$)\n",
    "\n",
    "ðŸ‘‰ We concatenate the result's vectors $\\mathbf{c}$ and $\\mathbf{d}$, each of which is corresponing to ```headline``` and ```short_description```.\n",
    "\n",
    "ðŸ‘‰ Finally, we apply fully-connected feed-forward network (i.e, Dense Net) for predicting class label.\n",
    "\n",
    "![composing network](images/1d_conv_net.png)\n",
    "\n",
    "Before max pooling, the values in padding positions (which index of token is ```pad_index```) are converted into zero, and these will then be underestimated in gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 200\n",
    "\n",
    "class BigramClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, class_num, padding_idx, conv_channel=256, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.conv_channel = conv_channel\n",
    "\n",
    "        self.embedding01 = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.embedding02 = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.conv01 = torch.nn.Conv1d(\n",
    "            in_channels=embedding_dim,\n",
    "            out_channels=conv_channel,\n",
    "            kernel_size=2,\n",
    "            stride=1,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv02 = torch.nn.Conv1d(\n",
    "            in_channels=embedding_dim,\n",
    "            out_channels=conv_channel,\n",
    "            kernel_size=2,\n",
    "            stride=1,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = torch.nn.MaxPool1d(\n",
    "            kernel_size=max_seq_len,\n",
    "        )\n",
    "        self.hidden = nn.Linear(conv_channel*2, hidden_dim)\n",
    "        self.classify = nn.Linear(hidden_dim, class_num)\n",
    "\n",
    "    def forward(self, region01, region02):\n",
    "        # Get padding masks (in which, element is 0.0 when it's in padded position, otherwise 1.0)\n",
    "        mask01 = torch.ones(region01.size()).to(device)\n",
    "        mask01 = mask01.masked_fill(region01 == self.padding_idx, 0)\n",
    "        mask02 = torch.ones(region02.size()).to(device)\n",
    "        mask02 = mask02.masked_fill(region02 == self.padding_idx, 0)\n",
    "        # Embedding\n",
    "        #   --> [batch_size, max_seq_len, embedding_dim]\n",
    "        out01 = self.embedding01(region01)\n",
    "        out02 = self.embedding02(region02)\n",
    "        # Apply convolution on dimension=1\n",
    "        #   --> [batch_size, max_seq_len, conv_channel]\n",
    "        out01 = self.conv01(out01.transpose(1,2)).transpose(1,2)\n",
    "        out02 = self.conv02(out02.transpose(1,2)).transpose(1,2)\n",
    "        # Apply masking (In padded position, it will then be 0.0)\n",
    "        extend_mask01 = mask01.unsqueeze(dim=2)\n",
    "        extend_mask01 = extend_mask01.expand(-1, -1, self.conv_channel)\n",
    "        out01 = out01 * extend_mask01\n",
    "        extend_mask02 = mask02.unsqueeze(dim=2)\n",
    "        extend_mask02 = extend_mask02.expand(-1, -1, self.conv_channel)\n",
    "        out02 = out02 * extend_mask02\n",
    "        # Apply relu\n",
    "        out01 = self.relu(out01)\n",
    "        out02 = self.relu(out02)\n",
    "        # Apply max pooling on dimension=1\n",
    "        #   --> [batch_size, 1, conv_channel]\n",
    "        out01 = self.max_pool(out01.transpose(1,2)).transpose(1,2)\n",
    "        out02 = self.max_pool(out02.transpose(1,2)).transpose(1,2)\n",
    "        # Flatten\n",
    "        #   --> [batch_size, conv_channel]\n",
    "        out01 = out01.squeeze(dim=1)\n",
    "        out02 = out02.squeeze(dim=1)\n",
    "\n",
    "        # Concat outputs of head and short_description\n",
    "        #   --> [batch_size, conv_channel * 2]\n",
    "        out = torch.concat((out01, out02), dim=-1)\n",
    "\n",
    "        # Apply classification head\n",
    "        #   --> [batch_size, hidden_dim]\n",
    "        out = self.hidden(out)\n",
    "        out = self.relu(out)\n",
    "        #   --> [batch_size, class_num]\n",
    "        logits = self.classify(out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model = BigramClassifier(vocab.__len__(), embedding_dim, len(ctoi), pad_index).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Now let's train our network for 1 epochs (If you are running Google colab, select gpu and can run for multiple epochs).\n",
    "\n",
    "Here, we are using Stochastic Gradient Descent (SDG) i.e. Calculating loss and doing parameter updation after every sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "for epoch in range(num_epochs):\n",
    "    for labels, heads, descs in dataloader:\n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(heads, descs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate accuracy\n",
    "        pred_labels = logits.argmax(dim=1)\n",
    "        num_correct = (pred_labels == labels).float().sum()\n",
    "        accuracy = num_correct / len(labels)\n",
    "        print(\"Epoch {} - loss: {:2.4f} - accuracy: {:2.4f}\".format(epoch+1, loss.item(), accuracy), end=\"\\r\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify text\n",
    "\n",
    "Now we classify text with \"```Paris```\", \"```Hilton Hotel```\", and \"```Paris Hilton```\".<br>\n",
    "\n",
    "Only \"```Paris Hilton```\" will be categorized as ```ENTERTAINMENT```, because 2-gram word \"```Paris Hilton```\" frequently occurs in ```ENTERTAINMENT``` article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOOD & DRINK\n",
      "TRAVEL\n",
      "POLITICS\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_text(headline, description):\n",
    "    test_list = [\n",
    "        [1, headline, description],\n",
    "    ]\n",
    "    _, test_heads, test_descs = collate_batch(test_list)\n",
    "    pred_logits = model(test_heads, test_descs)\n",
    "    pred_index = pred_logits.argmax()\n",
    "    return itoc[pred_index.item()]\n",
    "\n",
    "print(classify_text(\n",
    "    \"report about paris\",\n",
    "    \"paris is brilliant\"\n",
    "))\n",
    "print(classify_text(\n",
    "    \"report about hilton hotel\",\n",
    "    \"hilton hotel is brilliant\"\n",
    "))\n",
    "print(classify_text(\n",
    "    \"report about paris hilton\",\n",
    "    \"paris hilton is brilliant\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example will classify text with \"```Michael Jackson```\", \"```Michael Avenatti```\", and \"```Ronny Jackson```\".<br>\n",
    "\n",
    "Each of text includes either of \"```Michael```\" or \"```Jackson```\", or both of these. But the results will differ, because these 2-gram phrases have different occurrences in the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLITICS\n",
      "POLITICS\n",
      "ENTERTAINMENT\n"
     ]
    }
   ],
   "source": [
    "print(classify_text(\n",
    "    \"report about michael jackson\",\n",
    "    \"michael jackson is wise and honest\"\n",
    "))\n",
    "print(classify_text(\n",
    "    \"report about michael avenatti\",\n",
    "    \"michael avenatti is wise and honest\"\n",
    "))\n",
    "print(classify_text(\n",
    "    \"report about ronny jackson\",\n",
    "    \"ronny jackson is wise and honest\"\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
