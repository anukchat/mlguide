{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Model \n",
    "\n",
    "**<mark>Basic (Word Prediction Example)</mark>**\n",
    "\n",
    "Here, we will go through an example of simple **language model**.<br>\n",
    "\n",
    "The language model is used for a variety of NLP tasks, such as, **translation**, **transcription**, **summarization**, **question-answering**, etc.\n",
    "\n",
    "<mark>Here we will just train language model for text generation (i.e, next word prediction) with primitive neural networks.</mark>\n",
    "\n",
    "Unlike previous examples , language model will recognize the order of words in the sequence. (You don't need other special architecture to detect the sequence of words, such as 1D convolution, any more.)<br>\n",
    "\n",
    "RNN-based specialized architecture (such as, LSTM, GRU, etc) can also be used to train in advanced language model. Furthermore, a lot of <mark>transformer-based</mark> algorithms are widely used in today's SOTA language models.<br>\n",
    "\n",
    "In this example, we'll briefly apply primitive feed-forward networks.\n",
    "\n",
    "See the following diagram for entire network in this primitive example.<br>\n",
    "\n",
    "ðŸ‘‰ First in this network, the sequence of last 5 words is embedded into the list of vectors. \n",
    "\n",
    "ðŸ‘‰ Embedded vectors are then concatenated into a single vector, and this vector is used for the next word's prediction.\n",
    "\n",
    "![Model in this exercise](images/language_model_beginning.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**<mark>Note that this model won't care the long past context</mark>**\n",
    "\n",
    "For example, even when the following sentence is given, \n",
    "\n",
    "**\"In the United States, the president has now been\"**\n",
    "\n",
    "It won't care the context \"In the United States\" when it refers the last 5 words in the network. (It might then predict the incorrect word in this context and the accuracy won't also be so high in this example. In the later examples, we will address this problem.)\n",
    "</div>\n",
    "\n",
    "Nevertheless, the neural language models will be well-generalized more than traditional statistical models for unseen data. \n",
    "\n",
    "For instance, if \"red shirt\" and \"blud shirt\" occurs in training set, \"green shirt\" (which is not seen in training set) will also be predicted by the trained neural model, because the model knows that \"red\", \"blue\", and \"green\" occur in the same context.\n",
    "\n",
    "As you can see in this example, the language model can be trained with large unlabeled data (not needing for the labeled data), and this approach is very important for the growth of today's neural language models. This learning method is called **self-supervised learning**.<br>\n",
    "\n",
    "A lot of today's SOTA algorithms (such as, BERT, T5, GPT, etc) learn a lot of language properties with large corpus in this unsupervised way (such as, masked word's prediction, next word's prediction), and can then be fine-tuned for specific downstream tasks with small amount of labeled data by transfer approach.\n",
    "\n",
    "As you saw in **custom embedding example**, the word embedding will also be a byproduct in this example.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "ðŸ‘‰ In these examples of this repository, We'll apply **word-level (word-to-word)** tokenization, but you can also use **character-level (character-to-character)** model, which can learn unseen words with signals - such as, prefixes (e.g, \"un...\", \"dis...\"), suffixes (e.g, \"...ed\", \"...ing\"), capitalization, or presence of certain characters (e.g, hyphen, digits), etc.<br>\n",
    "\n",
    "ðŸ‘‰ Subword tokenization is the popular method used in today's architecture (such as, Byte Pair Encoding in GPT-2), in which a set of commonly occurring word segments (like \"cious\", \"ing\", \"pre\", etc) is involved in a vocabulary list.<br>\n",
    "> See [here](https://tsmatz.wordpress.com/2022/10/24/huggingface-japanese-ner-named-entity-recognition/) for SentencePiece tokenization in non-English languages.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we use a finite vocabulary set, but in many languages with complex _morphologies_ or word structures, this doesn't make much sense. So mapping a single word to an embedding vector may just be something specific to English. \n",
    "\n",
    "The __byte-pair encoding__ algorithm is a go-between between the 2 options: looking at language at the character-level and looking at language at the word-level. This algorithm learns a vocabulary from subword tokens or tokens that represent parts of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![byte pair encoding](img/byte_pair_encoding.PNG)\n",
    "_Figure 1. Byte-pair encoding algorithm._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, in word2vec, we focused on pretrained word embeddings. This draws on the assumption that the downstream NLP task will have enough data to provide sufficient context for the word embeddings to be contextualized in your task. The model would not be pretrained.\n",
    "\n",
    "However, in modern deep learning and NLP, we pretrain both the word embeddings and the model. \n",
    "\n",
    "The __pretraining/finetuning__ paradigm is immensely successful in NLP (and also other fields of AI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.3.0 torchtext==0.18.0 --extra-index-url https://download.pytorch.org/whl/cu114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use short description text in news papers dataset.<br>\n",
    "\n",
    "Before starting, please download [News_Category_Dataset_v2.json](https://www.kaggle.com/datasets/rmisra/news-category-dataset) (collected by HuffPost) in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Health experts said it is too early to predict...\n",
       "1         He was subdued by passengers and crew when he ...\n",
       "2         \"Until you have a dog you don't understand wha...\n",
       "3         \"Accidentally put grown-up toothpaste on my to...\n",
       "4         Amy Cooper accused investment firm Franklin Te...\n",
       "                                ...                        \n",
       "209522    Verizon Wireless and AT&T are already promotin...\n",
       "209523    Afterward, Azarenka, more effusive with the pr...\n",
       "209524    Leading up to Super Bowl XLVI, the most talked...\n",
       "209525    CORRECTION: An earlier version of this story i...\n",
       "209526    The five-time all-star center tore into his te...\n",
       "Name: short_description, Length: 209527, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"News_Category_Dataset_v3.json\",lines=True)\n",
    "train_data = df[\"short_description\"]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the better performance (accuracy), we standarize the input text as follows.\n",
    "- Make all words to lowercase in order to reduce words\n",
    "- Make \"-\" (hyphen) to space\n",
    "- Remove all punctuation except \" ' \" (e.g, Ken's bag) and \"&\" (e.g, AT&T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\&'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\&'\n",
      "/var/folders/7m/04ssj6n96q984_6wsnr60dg00000gn/T/ipykernel_72884/2039005710.py:3: SyntaxWarning: invalid escape sequence '\\&'\n",
      "  train_data = train_data.str.replace(\"[^'\\&\\w\\s]\", \"\", regex=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         health experts said it is too early to predict...\n",
       "1         he was subdued by passengers and crew when he ...\n",
       "2         until you have a dog you don't understand what...\n",
       "3         accidentally put grown up toothpaste on my tod...\n",
       "4         amy cooper accused investment firm franklin te...\n",
       "                                ...                        \n",
       "209522    verizon wireless and at&t are already promotin...\n",
       "209523    afterward azarenka more effusive with the pres...\n",
       "209524    leading up to super bowl xlvi the most talked ...\n",
       "209525    correction an earlier version of this story in...\n",
       "209526    the five time all star center tore into his te...\n",
       "Name: short_description, Length: 209527, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.str.lower()\n",
    "train_data = train_data.str.replace(\"-\", \" \", regex=True)\n",
    "train_data = train_data.str.replace(\"[^'\\&\\w\\s]\", \"\", regex=True)\n",
    "train_data = train_data.str.strip()\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we add ```<start>``` and ```<end>``` tokens in each sequence as follows, because these are important information for learning the ordered sequence.\n",
    "\n",
    "```this is a pen``` --> ```<start> this is a pen <end>```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> <start> he was subdued by passengers and crew when he fled to the back of the aircraft after the confrontation according to the us attorney's office in los angeles <end> <end>\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = [\" \".join([\"<start>\", x, \"<end>\"]) for x in train_data]\n",
    "# print first row\n",
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequence inputs\n",
    "\n",
    "Same as in previous examples, we will generate the sequence of word's indices (i.e, tokenize) from text.\n",
    "\n",
    "![Index vectorize](images/index_vectorize.png)\n",
    "\n",
    "First we create a list of vocabulary (```vocab```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "max_word = 50000\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# define tokenization function\n",
    "def yield_tokens(train_data):\n",
    "    for text in train_data:\n",
    "        tokens = tokenizer(text)\n",
    "        yield tokens\n",
    "\n",
    "# build vocabulary list\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data),\n",
    "    specials=[\"<unk>\"],\n",
    "    max_tokens=max_word,\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# get list for index-to-word, and word-to-index.\n",
    "itos = vocab.get_itos()\n",
    "stoi = vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we separate each sentence into 5 preceding word's sequence and word label (total 6 words) as follows.\n",
    "\n",
    "![Separate words](images/separate_sequence_for_next_words.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training input sequence :4125266\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seq_len = 5 + 1\n",
    "input_seq = []\n",
    "for s in train_data:\n",
    "    token_list = vocab(tokenizer(s))\n",
    "    for i in range(seq_len, len(token_list) + 1):\n",
    "        seq_list = token_list[i-seq_len:i]\n",
    "        input_seq.append(seq_list)\n",
    "print(\"The number of training input sequence :{}\".format(len(input_seq)))\n",
    "input_seq = np.array(input_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate into inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = input_seq[:,:-1], input_seq[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    2,     2,   141,   905,    68],\n",
       "       [    2,   141,   905,    68,    13],\n",
       "       [  141,   905,    68,    13,    10],\n",
       "       ...,\n",
       "       [ 2066, 44373,  6705,  3480,     4],\n",
       "       [44373,  6705,  3480,     4,  1347],\n",
       "       [ 6705,  3480,     4,  1347,     1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  49,  685,   46, ...,    4, 1354,    1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network\n",
    "\n",
    "Now we build network for our primitive language model. (See above for details about this model.)\n",
    "\n",
    "![Model in this exercise](images/language_model_beginning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "        )\n",
    "        self.hidden = nn.Linear(embedding_dim*(seq_len - 1), hidden_dim)\n",
    "        self.classify = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outs = self.embedding(inputs)\n",
    "        outs = torch.flatten(outs, start_dim=1)\n",
    "        outs = self.hidden(outs)\n",
    "        outs = self.relu(outs)\n",
    "        logits = self.classify(outs)\n",
    "        return logits\n",
    "\n",
    "model = SimpleLM(vocab.__len__(), embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate text with this model.<br>\n",
    "\n",
    "The generated result is messy, because it's still not trained at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> in the united states president inexperienced sicario alicante caffeinated roberto tiptoeing reporters iberian palatable goer hangover universal unusually eats untrue weighed vii regularly yakama tackled tables panamanian baptismal outdoor lgbtqia adeline impartial childrearing disavowed diner dodgers tormenting pinder ruff stronghold johnny prince lifetime patmore esperanza diner healthier mulling ossoff baptismal beanery invisible complacently 4500 denunciation tasking reckons cars impartial wider 513 tech honored ist batmans reinforcing encountering conditioned thewrap denunciation 353 1776 2075 confess alberta 132000 overfeeding spokesmodels beauties balart entertainers leadership diagonal blacktranslivesmatter archivist lgbtqia daniela cars chariot invisible sequences paso adweek diner california ruff integral diner certification revisionism ego brush 28427 cormorants disavowed biswo artistchef marinating st universal dimmer jamar confrontation fran brush blackvoices prophet ethel bulky spotting shared willpower permit 19 1900 blacktranslivesmatter bulbs ego xenophobic condolence impartial animalistic rogue \n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_index = stoi[\"<start>\"]\n",
    "end_index = stoi[\"<end>\"]\n",
    "max_output = 128\n",
    "\n",
    "def pred_output(sentence, progressive_output=True):\n",
    "    test_seq = vocab(tokenizer(sentence))\n",
    "    test_seq.insert(0, start_index)\n",
    "    for loop in range(max_output):\n",
    "        input_tensor = torch.tensor([test_seq[-5:]], dtype=torch.int64).to(device)\n",
    "        pred_logits = model(input_tensor)\n",
    "        pred_index = pred_logits.argmax()\n",
    "        test_seq.append(pred_index.item())\n",
    "        if progressive_output:\n",
    "            for i in test_seq:\n",
    "                print(itos[i], end=\" \")\n",
    "            print(\"\\n\")\n",
    "        if pred_index.item() == end_index:\n",
    "            break\n",
    "    return test_seq\n",
    "\n",
    "generated_seq = pred_output(\"in the united states president\", progressive_output=False)\n",
    "for i in generated_seq:\n",
    "    print(itos[i], end=\" \")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Now let's train our network.\n",
    "\n",
    "Here we have used loss and accuracy for evaluation, but the metrics to evaluate text generation task is not so easy. (Because simply checking an exact match to a reference text is not optimal.)<br>\n",
    "\n",
    "In practice, use some common metrics available in language models, such as, **BLEU** or **ROUGE**. (See [here](https://tsmatz.wordpress.com/2022/11/25/huggingface-japanese-summarization/) for these metrics.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are running google colab, execute it in gpu and can increase the epochs as well, for demo i am using only 1 Epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    list(zip(y, X)),\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "for epoch in range(num_epochs):\n",
    "    for labels, seqs in dataloader:\n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(seqs.to(device))\n",
    "        loss = F.cross_entropy(logits, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate accuracy\n",
    "        pred_labels = logits.argmax(dim=1)\n",
    "        num_correct = (pred_labels == labels.to(device)).float().sum()\n",
    "        accuracy = num_correct / len(labels)\n",
    "        print(\"Epoch {} - loss: {:2.4f} - accuracy: {:2.4f}\".format(epoch+1, loss.item(), accuracy), end=\"\\r\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I'll just show you how it generates a sentence by predicting the possibility of vocabularies over the given recent 5 words, until predicting the end-of-sequence.<br>\n",
    "\n",
    "<mark>This model doesn't recognize the past context, because this model refers only last 5 words.</mark>\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "ðŸ“” **Note** : This approach - which repeatedly picks up the next word with maximum probability in each timestep and generates a consequent sentence - is called **greedy search**. \n",
    "\n",
    "For instance, when it retrieves the next word with probability 0.8 and the second next word with probability 0.2, the joint probability will then be 0.8 x 0.2 = 0.16. \n",
    "\n",
    "On the other hand, when it retrieves the next word with smaller probability 0.6 but the second next word with so higher probability 0.9, the joint probability becomes 0.54 and it's then be larger than the former one. This example shows that the greedy search algorithm may sometimes lead to sub-optimal solutions (i.e, label-bias problems). It's known that this algorithm also tends to produce repetitive outputs.<br>\n",
    "\n",
    "<mark> For this reason, greedy search algorithm is rarely used in practical inference in language models, and a popular method known as **beam search** is used to get more optimal solutions in production.</mark>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> in the united states president <end> \n",
      "\n",
      "<start> the man has accused by the \n",
      "\n",
      "<start> the man has accused by the world \n",
      "\n",
      "<start> the man has accused by the world of \n",
      "\n",
      "<start> the man has accused by the world of the \n",
      "\n",
      "<start> the man has accused by the world of the world \n",
      "\n",
      "<start> the man has accused by the world of the world <end> \n",
      "\n",
      "<start> now he was expected to be \n",
      "\n",
      "<start> now he was expected to be a \n",
      "\n",
      "<start> now he was expected to be a new \n",
      "\n",
      "<start> now he was expected to be a new year \n",
      "\n",
      "<start> now he was expected to be a new year old \n",
      "\n",
      "<start> now he was expected to be a new year old <end> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = pred_output(\"in the united states president\", progressive_output=True)\n",
    "_ = pred_output(\"the man has accused by\", progressive_output=True)\n",
    "_ = pred_output(\"now he was expected to\", progressive_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
