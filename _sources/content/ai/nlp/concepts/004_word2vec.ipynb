{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec algorithm (Negative Sampling Example)\n",
    "\n",
    "Earlier we have built our own embedding in simple sentiment detection. This trained embedding will then capture the tone of sentiment in words, but it won't capture other properties, such as, the similarity of \"dog\" and \"puppy\".<br>\n",
    "\n",
    "More advanced language models will capture a lot of language properties and these models are useful in practical use-case - such as, **searching similar text**, **clustering document**, **recommendation**, etc.\n",
    "\n",
    "Suppose, we need the model which predicts the next word. \n",
    "\n",
    "In building this model, when it has 70,000 words and 3,000,000 records in training set, it will need 70,000 * 3,000,000 float values in one-hot vectors.\n",
    "\n",
    "As you can easily find, this model will be <mark>computationally expensive</mark>(because it needs the probability over all target words) and will then consume a <mark>lot of computing resources</mark> (memory and disk space) depending on vocabulary size.\n",
    "\n",
    "ðŸ‘‰ In order for making it scalable to unlimited vocabularies, the algorithm can be modified by sampling k incorrect words and training the part of words, instead of computing possibilities for all words.<br>\n",
    "\n",
    "This method is called **Negative Sampling (NS)**.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "In Word2Vec family, you can take another optimization objectives, called **Hierarchical Softmax**, instead of Negative Sampling (NS).\n",
    "\n",
    "Today's refined embedding algorithms - such as, Word2Vec or GloVe - includes this idea of Negative Sampling method.\n",
    "</div>\n",
    "\n",
    "\n",
    "ðŸ‘‰ The well-trained dense vector will represent some aspects (meaning) for words or documents. For instance, if \"dog\" and \"cat\" are closely related each other, the dense vectors for \"dog\" and \"cat\" might have close cosine similarity. In this representation, \"burger\" and \"hot-dogs\" might be closer than \"ice-cream\". (This is called **distributional hypothesis**.)<br>\n",
    "\n",
    "More sophisticated vectors might have analogies for words - such as, \"king\" - \"man\" + \"woman\" = \"queen\".\n",
    "\n",
    "ðŸ‘‰ **Word2Vec** algorithm is based on the distributional hypothesis, which derives from word similarities by representing target words according to the contexts in which they occur.<br>\n",
    "\n",
    "In this example, I'll introduce Word2Vec model in neural networks with Negative Sampling (NS) method.\n",
    "\n",
    "When the target word (focus word) is given, first we'll pick up by sampling both correct and incorrect context words.<br>\n",
    "\n",
    "For each collected context words, we will then compute the difference between correct word's score and incorrect word's score.<br>\n",
    "\n",
    "Finally we then optimize the loss of scores to train Word2Vec model.\n",
    "\n",
    "This approach is called **Skip-Gram (SG)** model in Word2Vec algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.3.0 torchtext==0.18.0 --extra-index-url https://download.pytorch.org/whl/cu114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I have used short description text in news papers, since it's formal-styled concise sentence. (It's today's modern English, not including slangs.)<br>\n",
    "\n",
    "Before starting, please download [News_Category_Dataset_v3.json](https://www.kaggle.com/datasets/rmisra/news-category-dataset) (collected by HuffPost) in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         She left her husband. He killed their children...\n",
       "1                                  Of course it has a song.\n",
       "2         The actor and his longtime girlfriend Anna Ebe...\n",
       "3         The actor gives Dems an ass-kicking for not fi...\n",
       "4         The \"Dietland\" actress said using the bags is ...\n",
       "                                ...                        \n",
       "200848    Verizon Wireless and AT&T are already promotin...\n",
       "200849    Afterward, Azarenka, more effusive with the pr...\n",
       "200850    Leading up to Super Bowl XLVI, the most talked...\n",
       "200851    CORRECTION: An earlier version of this story i...\n",
       "200852    The five-time all-star center tore into his te...\n",
       "Name: short_description, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"News_Category_Dataset_v3.json\",lines=True)\n",
    "text_col = df[\"short_description\"]\n",
    "text_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we standarize the input text as follows.\n",
    "- Make all words to lowercase in order to reduce words\n",
    "- Make \"-\" (hyphen) to space\n",
    "- Remove all punctuation\n",
    "\n",
    "ðŸ‘‰ **Note** : Here we have not removed stop words - such as, \"the\", \"a\", etc -, because these words will be omitted (dropped) by the following subsampling process.\n",
    "\n",
    "ðŸ‘‰ **Note** : In practice, N-gram words (such as, \"New York\", \"Barack Obama\") should also be dealed with,\n",
    "\n",
    "ðŸ‘‰ In the strict pre-processing, we should also care about the polysemy. (The different meanings in the same word should have different tokens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         she left her husband he killed their children ...\n",
       "1                                   of course it has a song\n",
       "2         the actor and his longtime girlfriend anna ebe...\n",
       "3         the actor gives dems an ass kicking for not fi...\n",
       "4         the dietland actress said using the bags is a ...\n",
       "                                ...                        \n",
       "200848    verizon wireless and at&t are already promotin...\n",
       "200849    afterward azarenka more effusive with the pres...\n",
       "200850    leading up to super bowl xlvi the most talked ...\n",
       "200851    correction an earlier version of this story in...\n",
       "200852    the five time all star center tore into his te...\n",
       "Name: short_description, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_col = text_col.str.lower()\n",
    "text_col = text_col.str.replace(\"-\",\" \")\n",
    "text_col = text_col.str.replace(\"[^'\\&\\w\\s]\",\"\",regex=True)\n",
    "text_col = text_col.str.strip()\n",
    "text_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a word's index vector as follows, we create a list for words (```vocab```) used in the training set.<br>\n",
    "\n",
    "The generated list ```stoi``` converts word to index, and ```itos``` converts index to word.\n",
    "\n",
    "![Index vectorize](images/index_vectorize.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# define tokenization function\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# build vocabulary list\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(text_col),\n",
    "    specials=[\"<unk>\"],\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# get list for index-to-word, and word-to-index\n",
    "itos = vocab.get_itos()\n",
    "stoi = vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77081"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the number of words\n",
    "vocab_size = vocab.__len__()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[318, 24, 116]\n",
      "318\n",
      "obama\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(vocab([\"obama\", \"was\", \"president\"]))\n",
    "print(stoi[\"obama\"])\n",
    "print(itos[318])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we build target and positive context pairs for Skip-Gram (SG) training inputs.\n",
    "\n",
    "For instance, suppose, we want to find context words for the target word \"obama\" in the following sentence with window size 2.\n",
    "\n",
    "```\"in 2012 us president obama won votes and republican romney got 206 votes\"```\n",
    "\n",
    "In this example, the word \"us\", \"president\", or \"won\" are positive context words for the target word \"obama\", but \"2021\", \"republican\", or \"romney\" are negative context words for the target word \"obama\".\n",
    "\n",
    "![Skip-Gram](images/skip_gram.png)\n",
    "\n",
    "Here we build pairs only for positive contexts. (The negative sampling will be performed in batch processing later.)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "ðŸ’¡ **Note** : In this example, we pick up context words evenly, regardless of window position. For instance, the context words \"us\" and \"president\" has same weight against target word \"obama\" in above example.<br>\n",
    "\n",
    "In Word2Vec, you can take another variation with positional context.\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all positive pairs : 29596892\n",
      "***** fisrt 10 pairs *****\n",
      "[[68, 373], [68, 61], [68, 502], [68, 47], [373, 68], [373, 61], [373, 502], [373, 47], [373, 683], [61, 68]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 4\n",
    "\n",
    "train_pairs, train_labels = [], []\n",
    "\n",
    "for text in text_col:\n",
    "    tokens = tokenizer(text)\n",
    "    for i, t in enumerate(tokens):\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(tokens) - 1, i + window_size)\n",
    "        for j in range(window_start, window_end + 1):\n",
    "            if j != i:\n",
    "                train_pairs.append([stoi[t], stoi[tokens[j]]])\n",
    "                train_labels.append(1.0)\n",
    "\n",
    "print(\"number of all positive pairs : {}\".format(len(train_pairs)))\n",
    "print(\"***** fisrt 10 pairs *****\")\n",
    "print(train_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "The training set will have a bias by word's frequency.<br>\n",
    "\n",
    "For instance, the stop words (e.g., \"in\", \"the\", and \"a\") will have less meaning in contexts, and the word \"one\", \"new\", or \"make\" will also be frequently used in corpus, but it then won't be much useful information.<br>\n",
    "\n",
    "In [original paper](https://arxiv.org/abs/1310.4546) (Mikolov et al., 2013), Word2Vec implements **subsampling** to address this problem.<br>\n",
    "\n",
    "The paper says that each word $w_i$ in the training set is discarded (dropped) with probability computed by the formula :\n",
    "\n",
    "$$ \\displaystyle P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "where $t$ is some threshold and $ f(w_i) $ is frequency ratio.\n",
    "\n",
    "As you can see, the words which has high frequency are likely to be discarded.<br>\n",
    "\n",
    "The threshold is a parameter which determines how rarely discarded. (We use $10^{-5}$ along with this paper.)\n",
    "\n",
    "In order to implement subsampling function, first we build a dictionary for token index and frequency ratio. (These values are normalized and sum up to 1.0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "\n",
    "# create bag of token's indices (multiple tokens duplicated)\n",
    "def tokenize_all(col_text):\n",
    "    for text in col_text:\n",
    "        tokens = tokenizer(text)\n",
    "        for t in tokens:\n",
    "            yield stoi[t]\n",
    "\n",
    "all_token_indices = [i for i in tokenize_all(text_col)]\n",
    "\n",
    "# create statistics for all indices\n",
    "stat = Counter(all_token_indices)\n",
    "\n",
    "# build dictionary for frequency ratio\n",
    "ratio_dic = {index: count/len(all_token_indices) for index, count in stat.items()}\n",
    "dtoi = list(ratio_dic)\n",
    "ratio_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using this dictionary, we now build a function which determines whether it's discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# When one of index_list's elements is discarded, it will become True.\n",
    "# Otherwise, False.\n",
    "def is_discarded(index_list, random_val, threshold=1e-5):\n",
    "    bool_list = [random_val < 1 - np.sqrt(threshold/ratio_dic[i]) for i in index_list]\n",
    "    return any(bool_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling (NS)\n",
    "\n",
    "Now it's time to implement negative sampling (NS).\n",
    "\n",
    "The negative samples are selected using unigram distribution, in which more frequent token will be more likely selected.<br>\n",
    "\n",
    "However, the [original paper](https://arxiv.org/abs/1310.4546) says that the unigram distribution raised to the 3/4rd power (see below) will outperform rather than original distribution. :\n",
    "\n",
    "$$ \\displaystyle P(w_i) = \\frac{p(w_i)^{\\frac{3}{4}}}{\\sum_j \\left( p(w_j)^{\\frac{3}{4}} \\right)} $$\n",
    "\n",
    "In this example, we also pick up negative samples by this distribution.\n",
    "\n",
    "First we build the list of above distribution as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.02016515e-03, 2.63017814e-04, 1.22094381e-03, ...,\n",
       "       1.33448163e-06, 1.33448163e-06, 1.33448163e-06])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_dist = np.array(list(ratio_dic.values()))\n",
    "ns_dist = unigram_dist**0.75 / np.sum(unigram_dist**0.75)\n",
    "ns_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement a function to get negative samples.<br>\n",
    "This function picks up samples with above distribution and returns the list of token indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_negative_samples(positive_samples, ns_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        positive_samples (2d list): batch set of positive sample pairs\n",
    "        ns_ratio (float): the ratio of negative samples\n",
    "            0.0 : no negative samples\n",
    "            1.0 : same number as positive samples\n",
    "    Return:\n",
    "        list of negative sample pairs (2d list)\n",
    "    \"\"\"\n",
    "    # generate the list of token index for negative samples\n",
    "    batch_size = len(positive_samples)\n",
    "    num_negative_samples = int(batch_size * ns_ratio)\n",
    "    dic_idx_tensor = torch.multinomial(\n",
    "        torch.tensor(ns_dist, dtype=torch.float),\n",
    "        num_negative_samples,\n",
    "        replacement=True\n",
    "    )\n",
    "    ns_tokens = [dtoi[d] for d in dic_idx_tensor.tolist()]\n",
    "    # build negative sample pairs\n",
    "    target_tokens = [p[0] for p in positive_samples]\n",
    "    negative_pairs = []\n",
    "    for i in range(batch_size):\n",
    "        negative_pairs.append([target_tokens[i], ns_tokens[i]])\n",
    "    return negative_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** positive samples *****\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "***** negative samples *****\n",
      "[[1, 1192], [3, 376], [5, 44]]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "pos_test = np.reshape(np.arange(1,7), (3, 2))\n",
    "print(\"***** positive samples *****\")\n",
    "print(pos_test)\n",
    "neg_test = get_negative_samples(pos_test)\n",
    "print(\"***** negative samples *****\")\n",
    "print(neg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build data collator\n",
    "\n",
    "Now we build pre-processing (i.e, data collator) in each batch.\n",
    "\n",
    "In this data collator, we pre-process data as follows :\n",
    "\n",
    "- Pick up positive samples and labels.\n",
    "- Discard samples with above subsampling function.\n",
    "- Add negative samples with above negative sampling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    new_label, new_pair = [], []\n",
    "    rand_val = random.random()\n",
    "\n",
    "    # perform subsampling\n",
    "    for (label, pair) in batch:\n",
    "        if is_discarded(pair, rand_val):\n",
    "            continue\n",
    "        new_label.append(label)\n",
    "        new_pair.append(pair)\n",
    "    if len(new_label) == 0:\n",
    "        empty = torch.tensor([], dtype=torch.int64).to(device)\n",
    "        return empty, empty\n",
    "\n",
    "    # add negative samples\n",
    "    negative_samples = get_negative_samples(new_pair)\n",
    "    for ns in negative_samples:\n",
    "        new_label.append(0.0)\n",
    "        new_pair.append(ns)\n",
    "\n",
    "    # shuffle (with same seed)\n",
    "    seed = random.randint(0, 10e6)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(new_label)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(new_pair)\n",
    "\n",
    "    # convert to tensor\n",
    "    new_label = torch.tensor(new_label, dtype=torch.float).to(device)\n",
    "    new_pair = torch.tensor(new_pair, dtype=torch.int64).to(device)\n",
    "    return new_label, new_pair\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    list(zip(train_labels, train_pairs)),\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape in batch : torch.Size([108])\n",
      "token shape in batch : torch.Size([108, 2])\n",
      "***** label sample *****\n",
      "tensor(0., device='cuda:0')\n",
      "***** pair sample *****\n",
      "tensor([2566,  112], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for labels, pairs in dataloader:\n",
    "    break\n",
    "\n",
    "print(\"label shape in batch : {}\".format(labels.size()))\n",
    "print(\"token shape in batch : {}\".format(pairs.size()))\n",
    "print(\"***** label sample *****\")\n",
    "print(labels[0])\n",
    "print(\"***** pair sample *****\")\n",
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network and Train\n",
    "\n",
    "Now let's build Word2Vec (with Skip-Gram method) network and train.\n",
    "\n",
    "In this network, we generate dense vectors for both target and context words by embedding (see [exercise02](./02_custom_embedding.ipynb)), and perform dot product operation as follows.\n",
    "\n",
    "Here I don't go so far, but in traditional NLP, the matrix for word-context pairs (so called, PMI matrix) is considered and the dimension can be reduced with factorization by SVD (Singular Value Decomposition) in order for preventing from high computational costs and sparsity. (It's based on the idea of **PMI**, point-wise mutual information.)<br>\n",
    "\n",
    "In this Word2Vec model (neural methods), however, this PMI-based idea can be simply achieved by **dot product operation** between word's embedding vector and context's embedding vector, based on the sampling of word's frequency.\n",
    "\n",
    "We will then evaluate the loss by sigmoid :\n",
    "\n",
    "$$ \\displaystyle \\prod_{i=1}^{k} \\frac{1}{1+e^{-\\mathbf{w}\\cdot\\mathbf{c}_i}} $$\n",
    "\n",
    "where $\\mathbf{w}$ is target word (focus word) and $\\mathbf{c}_i$ is its corresponding context words\n",
    "\n",
    "(See [here](https://tsmatz.wordpress.com/2017/08/30/regression-in-machine-learning-math-for-beginners/) for details about sigmoid operation.)\n",
    "\n",
    "Now I illustrate our network in the following picture.\n",
    "\n",
    "![Word2Vec model](images/word2vec_network.png)\n",
    "\n",
    "<div class=\"alert alert-info alert-block\">\n",
    "\n",
    "**Note** : In Word2Vec family, you can also take another context representation, $\\frac{1}{1 + e^{-\\sum \\mathbf{w}\\cdot\\mathbf{c}_i}}$. This is called **CBOW** approach, instead of Skip-Gram (SG) approach.\n",
    "\n",
    "</div>\n",
    "\n",
    "In this model, only embedding is trained and it will then eventually give you a well-trained embedding for word vectorization. This is why this model (Word2Vec) is widely used for getting model for word vectorization.\n",
    "\n",
    "In the following example, sigmoid is operated in ```torch.nn.BCEWithLogitsLoss``` and I don't then include sigmoid operation in model class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_target = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_context = nn.Embedding(vocab_size, embedding_dim)\n",
    "    def forward(self, inputs):\n",
    "        emb_tar = self.embedding_target(inputs[:,0])\n",
    "        emb_con = self.embedding_context(inputs[:,1])\n",
    "        dot_prd = (emb_tar * emb_con).sum(dim=-1)\n",
    "        return dot_prd\n",
    "\n",
    "model = Word2Vec(vocab_size, embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 1.4364\n",
      "Epoch 2 - loss: 0.9896\n",
      "Epoch 3 - loss: 0.2934\n",
      "Epoch 4 - loss: 0.6281\n",
      "Epoch 5 - loss: 0.4952\n",
      "Epoch 6 - loss: 0.5789\n",
      "Epoch 7 - loss: 0.6157\n",
      "Epoch 8 - loss: 0.4032\n",
      "Epoch 9 - loss: 0.8901\n",
      "Epoch 10 - loss: 0.7420\n",
      "Epoch 11 - loss: 0.5190\n",
      "Epoch 12 - loss: 0.4272\n",
      "Epoch 13 - loss: 0.3402\n",
      "Epoch 14 - loss: 0.3383\n",
      "Epoch 15 - loss: 0.4935\n",
      "Epoch 16 - loss: 0.4414\n",
      "Epoch 17 - loss: 0.3320\n",
      "Epoch 18 - loss: 0.3234\n",
      "Epoch 19 - loss: 0.2709\n",
      "Epoch 20 - loss: 0.2871\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for labels, pairs in dataloader:\n",
    "        if pairs.nelement() == 0:\n",
    "            continue\n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        outs = model(pairs)\n",
    "        loss = criterion(outs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch {} - loss: {:2.4f}\".format(epoch+1, loss.item()), end=\"\\r\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similar vectors\n",
    "\n",
    "We get top 10 words which has close cosine similarity with the word \"obama\" (who is a former president) in the generated target vectors.\n",
    "\n",
    "Note that here we have used a limited number of articles in news paper and repeatedly trained with this same dataset. (Here we have trained with 20 epochs.) However, you can train using more large corpus (unlabeled dataset) in this unsupervised way (such as, using Wikipedia) to get more generalized vectors.<br>\n",
    "\n",
    "It will also include a lot of contrasting conjunctions (antonyms), such as, \"democratic\" and \"republican\", \"obama\" and \"trump\", and so on, rather than contexts with close relationship (synonyms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obama',\n",
       " 'president',\n",
       " 'barack',\n",
       " 'trump',\n",
       " 'donald',\n",
       " 'administration',\n",
       " 'nominee',\n",
       " 'secretary',\n",
       " 'senate',\n",
       " 'erdogan']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# get embedding vector for the word \"obama\"\n",
    "target_index = torch.tensor(stoi[\"obama\"], dtype=torch.int64).to(device)\n",
    "target_emb = model.embedding_target(target_index).cpu().detach().numpy()\n",
    "\n",
    "# get vectors for all words (top frequent 20000 words)\n",
    "sorted_tuples = sorted(ratio_dic.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_tuples = sorted_tuples[:20000] # reduce samples\n",
    "sorted_dict = OrderedDict(sorted_tuples)\n",
    "index_all = torch.tensor(list(sorted_dict.keys()), dtype=torch.int64).to(device)\n",
    "all_target_emb = model.embedding_target(index_all).cpu().detach().numpy()\n",
    "\n",
    "# get L2 distance for all words\n",
    "all_distance = np.array([np.dot(target_emb,v)/(norm(target_emb)*norm(v)) for v in all_target_emb])\n",
    "\n",
    "# get top 10 words similar to the word \"obama\"\n",
    "indices_list = np.argsort(-all_distance)\n",
    "token_indices_list = [index_all[i].item() for i in indices_list]\n",
    "[itos[i] for i in token_indices_list[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above example, I have implemented Word2Vec and Negative Sampling (NS) algorithm from scratch, but you can use the efficient implementations for Word2vec algorithm in ```gensim``` package.\n",
    "\n",
    "Pre-trained word vectors for English (which are well-trained by large corpora) is also available, such as, in Google (Word2Vec) or Stanford (GloVe), and pre-trained word vectors for other languages are also available in Polyglot project.\n",
    "\n",
    "**Note** : When you use these off-the-shelf embeddings, it's better to apply the same standarization scheme (the normalization scheme used in the training) in pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I load the model trained with news dataset by Google, in which the vector has 300 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "trained_model = gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model[\"dog\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show top 10 similar words to the word \"dog\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dogs', 0.8680489659309387),\n",
       " ('puppy', 0.8106428384780884),\n",
       " ('pit_bull', 0.780396044254303),\n",
       " ('pooch', 0.7627376914024353),\n",
       " ('cat', 0.7609457969665527),\n",
       " ('golden_retriever', 0.7500901818275452),\n",
       " ('German_shepherd', 0.7465174198150635),\n",
       " ('Rottweiler', 0.7437615394592285),\n",
       " ('beagle', 0.7418621778488159),\n",
       " ('pup', 0.740691065788269)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.most_similar(\"dog\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show most similar word to the semantic of \"king\" - \"man\" + \"woman\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118193507194519)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
