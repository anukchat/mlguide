{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16df9564",
   "metadata": {},
   "source": [
    "# Word Vectors & Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb2f672",
   "metadata": {},
   "source": [
    "**References** [Stanford CS224N](https://www.youtube.com/watch?v=rmVRLeJRkl4)\n",
    "\n",
    "(This course is the gold mine by Professor Christopher Manning from Stanford University, that you wouldn't want to miss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa7ec6",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68e7dc",
   "metadata": {},
   "source": [
    "If we want to solve Natural Language Processing (NLP) tasks with neural networks, we need some way to represent text as tensors. \n",
    "\n",
    "Computers already represent textual characters as numbers that map to fonts on your screen using encodings such as ASCII or UTF-8.\n",
    "\n",
    "<img alt=\"Image showing diagram mapping a character to an ASCII and binary representation\" src=\"img/ascii-character-map.png\" width=\"50%\"/>\n",
    "\n",
    "[Image source](https://www.seobility.net/en/wiki/ASCII)\n",
    "\n",
    "As humans, we understand what each letter **represents**, and how all characters come together to form the words of a sentence. \n",
    "\n",
    "However, <mark>computers by themselves do not have such an understanding</mark>, and neural network has to learn the meaning during training.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**But what does \"meaning\" mean to us?**\n",
    "\n",
    "- idea that is represented by a word/phrase\n",
    "- idea that a person wants to express using words/signs\n",
    "- idea that is expressed in a work of writing or art\n",
    "\n",
    "We usually model the way we think of meaning as __denotational semantics__:\n",
    "</div>\n",
    "\n",
    "Therefore, we can use different approaches when representing text:\n",
    "\n",
    "* **Character-level representation**, when we represent text by treating each character as a number. Given that we have *C* different characters in our text corpus, the word *Hello* would be represented by 5x*C* tensor. Each letter would correspond to a tensor column in one-hot encoding.\n",
    "* **Word-level representation**, in which we create a **vocabulary** of all words in our text, and then represent words using <mark>one-hot encoding</mark>. This approach is somehow better, because **each letter by itself does not have much meaning**, and thus by using higher-level semantic concepts - words - we simplify the task for the neural network. \n",
    "\n",
    "\n",
    "<mark>Regardless of the representation, we first need to convert the text into a sequence of **tokens**</mark>, one token being either a character, a word, or sometimes even part of a word.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**What is One-Hot Encoding?**\n",
    "\n",
    "One-hot encoding is a representation of categorical variables as binary vectors. In other words, each categorical variable is represented by a binary vector of length equal to the number of categories. \n",
    "\n",
    "<img alt=\"One Hot Encoding\" src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ggtP4a5YaRx6l09KQaYOnw.png\" width=\"50%\">\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea77c66",
   "metadata": {},
   "source": [
    "In traditional NLP, we represent words as one-hot vectors. This is called a __localist representation__. \n",
    "\n",
    "However, <mark>using one-hot vectors to encode words **lacks** similarity between words and context in general</mark>.\n",
    "\n",
    "In the modern day, we use __embedding vectors__ built on the idea of __distributional semantics__ which defines a words meaning not by what idea it's associated with, but by <mark>what context or window of words the word appears in.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0a5bf",
   "metadata": {},
   "source": [
    "![Distributional Semantics](./img/distributional_semantics.PNG) <br>\n",
    "_Figure 1. Distributional Semantics._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b88f4",
   "metadata": {},
   "source": [
    "### WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa1258",
   "metadata": {},
   "source": [
    "__WordNet__ groups together words that are syonymous but is limited in capability.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "**What is WordNet?**\n",
    "\n",
    "WordNET is a lexical **database** of words in more than 200 languages in which we have adjectives, adverbs, nouns, and verbs grouped differently into a set of cognitive synonyms, where **each word in the database is expressing its distinct concept**. \n",
    "\n",
    "The cognitive synonyms which are called **synsets** are presented in lexical and semantic relations.\n",
    "\n",
    "Synonyms--words that denote the same concept and are interchangeable in many contexts--are grouped into unordered sets (synsets)\n",
    "\n",
    "[WordNet](https://en.wikipedia.org/wiki/WordNet#:~:text=WordNet%20is%20a%20lexical%20database,of%20a%20dictionary%20and%20thesaurus)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2d382",
   "metadata": {},
   "source": [
    "WordNet can be used using nltk package in python, let's understand using an example. Nltk stands for Natural Language Toolkit, is a python package for traditional natural language processing tasks.\n",
    "\n",
    "```bash\n",
    "pip install -U nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61574970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Download teh wordnet dataset\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd486fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synsets for the word \"invite\" in WordNet:\n",
      "\n",
      "\n",
      "Synset('invite.n.01')\n",
      "Synset('invite.v.01')\n",
      "Synset('invite.v.02')\n",
      "Synset('tempt.v.03')\n",
      "Synset('invite.v.04')\n",
      "Synset('invite.v.05')\n",
      "Synset('invite.v.06')\n",
      "Synset('invite.v.07')\n",
      "Synset('receive.v.05')\n"
     ]
    }
   ],
   "source": [
    "# import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# look up a word using synsets(): a set of synonyms that share a common meaning.\n",
    "# a synset is identified with a 3-part name of the form: word.pos.nn [Word, part of speech, number of synsets]\n",
    "\n",
    "print('Synsets for the word \"invite\" in WordNet:\\n\\n')\n",
    "for synset in wn.synsets('invite'):\n",
    "    print(synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ca6c1",
   "metadata": {},
   "source": [
    "**What do we interpret from this ?**\n",
    "\n",
    "Let's take some examples and understand:\n",
    "\n",
    "- Synset('invite.n.01')\n",
    "    - **Part of Speech**: Noun (n)\n",
    "    - **Interpretation**: Represents the noun form of \"invite,\" likely meaning an invitation.\n",
    "    \n",
    "- Synset('invite.v.01')\n",
    "    - **Part of Speech**: Verb (v)\n",
    "    - **Interpretation**: Represents the primary verb sense of \"invite,\" such as to request someone's presence.\n",
    "\n",
    "- Synset('invite.v.02')\n",
    "    - **Part of Speech**: Verb (v)\n",
    "    - **Interpretation**: Another verb sense of \"invite,\" possibly with a different nuance, like enticing or attracting.\n",
    "- .\n",
    "- .\n",
    "\n",
    "\n",
    "The word \"**invite**\" has multiple synsets, both as a **noun** and a **verb**, each capturing a different meaning or usage.\n",
    "\n",
    "The inclusion of **tempt.v.03** and **receive.v.05** implies that some senses of \"**invite**\" are closely related or synonymous with \"**tempt**\" and \"**receive**,\" respectively.\n",
    "\n",
    "\"**Invite**\" functions as both a noun and a verb, and WordNet categorizes its meanings accordingly.\n",
    "\n",
    "\n",
    "We can further fetch the definition, examples and hypernyms of the specified synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bff3f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition:\n",
      "The definition for invite as a noun:\n",
      "\n",
      " a colloquial expression for invitation\n",
      "------------------------------------------\n",
      "Examples\n",
      "The definition for invite as a noun:\n",
      "\n",
      " [\"he didn't get no invite to the party\"]\n",
      "------------------------------------------\n",
      "Hypernyms\n",
      "The hypernyms for invite as a noun:\n",
      "\n",
      " [Synset('invitation.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# check definition of a synset\n",
    "print('Definition:')\n",
    "print('The definition for invite as a noun:\\n\\n', wn.synset('invite.n.01').definition())\n",
    "print(\"------------------------------------------\")\n",
    "# check the related examples\n",
    "print('Examples')\n",
    "print('The definition for invite as a noun:\\n\\n', wn.synset('invite.n.01').examples())\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# check the hypernyms\n",
    "print('Hypernyms')\n",
    "print('The hypernyms for invite as a noun:\\n\\n', wn.synset('invite.n.01').hypernyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa3dcc",
   "metadata": {},
   "source": [
    "#### Limitations\n",
    "\n",
    "- Great as a resource but missing nuance ‚Ä¢ e.g., ‚Äúproficient‚Äù is listed as a synonym for ‚Äúgood‚Äù This is only correct in some contexts\n",
    "\n",
    "- Missing new meanings of words ‚Ä¢ e.g., wicked, badass, nifty, wizard, genius, ninja, bombest\n",
    "- Impossible to keep up-to-date!\n",
    "- Subjective\n",
    "- **Requires human labor to create and adapt**\n",
    "- Can‚Äôt compute accurate word similarity\n",
    "    - nltk does provide a way to compute word similarity, but it is too simplistic and subjective: synset1.path_similarity(synset2): Return a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. The score is in the range 0 to 1. See: http://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac22080",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7daeb1",
   "metadata": {},
   "source": [
    "__Word2vec__ is a 2013 framework for <mark>learning word vectors </mark>. \n",
    "\n",
    "üëâ The idea is to go through a large corpus/body of text and define a vocabulary table where each word is represented by an embedding vector. \n",
    "\n",
    "üëâ Go through each position $t$ in the text with center word $c$ and context words $o$ and calculate the **probability of $o$ given $c$**. We adjust the word vectors to maximize this probability.\n",
    "\n",
    "üëâ We Keep adjusting the word vectors to maximize this probability (backpropogation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ddf05",
   "metadata": {},
   "source": [
    "<img src='./img/stanford/1-word2vec-example.png' width='600' height='300' alt=\"Word2Vec\">\n",
    "\n",
    "_Figure 2. Word2vec._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ab290",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbc9615",
   "metadata": {},
   "source": [
    "**Objective function**\n",
    "\n",
    "For each position $t = 1, ... , ùëá$, predict context words within a window of fixed size $m$, given center word $w_j$. Data likelihood:\n",
    "\n",
    "$$Likelihood = L(\\theta) = \\prod_{t}^{T} \\prod_{\\substack{-m \\leq j \\leq m \\\\ j \\neq m}} P(w_{t+j}|{w_{t}; \\theta})$$\n",
    "\n",
    "The objective/loss/cost function for $(1)$ is the average negative log likelihood: \n",
    "\n",
    "$$J(\\theta) = - \\frac{1}{T} logL(\\theta) = - \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq m}} log P(w_{t+j}|{w_{t}; \\theta})$$\n",
    "\n",
    "\n",
    "**Prediction function**\n",
    "\n",
    "Denote by $v_{c}$ and $v_{o}$ respectively the center word and the context word, using **softmax**, we get the following prediction function of predicting $v_{c}$ given $v_{o}$ and the vocubulary $V$:\n",
    "\n",
    "$$P(o|c) = \\frac{exp(\\mathbf{u_{o}^{T} v_{c}})}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})} \\tag{3}$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src='./img/stanford/1-softmax.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d8bd12",
   "metadata": {},
   "source": [
    "While **Word2vec** is a framework for learning embedding vectors. There are also variants of this framework. \n",
    "\n",
    "* __Skip-grams (SG)__ predicts context words based on center word (this is word2vec)\n",
    "* __Continuous Bag of Words (CBOW)__ predicts the center word from context words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2891651b",
   "metadata": {},
   "source": [
    "### Skip-Gram With Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1994156",
   "metadata": {},
   "source": [
    "$$\n",
    "P(o ~|~ c) = \\frac{exp(u_{o}^T v_c)}{\\sum_{w \\in V} exp(u_{w}^T v_c)}\n",
    "$$\n",
    "\n",
    "Consider the above equation in the previous lecture. It iterates through all the words in the vocabulary in the denominator. This is infeasible. So instead, we sample a $k$ subset of negative pairs/samples. The rewritten objective function is:\n",
    "\n",
    "$$\n",
    "J_{t}(\\theta) = log \\sigma (u_{o}^{T} v_{c}) + \\sum_{i = 1}^{k} \\mathbb{E}_{j \\sim P(w)} [log \\sigma (-u_{j}^{T}v_{c})] \\hspace{1em} (Eq.~1)\\\\\n",
    "$$\n",
    "\n",
    "This objective function maximizes the probability that real context word appears for a given center word and minimizes the probability that random context words appear around a given center word.\n",
    "\n",
    "The negative words are sampled such that less frequent words are sampled more often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9790bef",
   "metadata": {},
   "source": [
    "### Co-occurrence Vectors\n",
    "\n",
    "Co-occurrence matrices are used to model the relationships between words. The matrix is a $ V \\times V $ matrix where $V$ is the number of words in the vocabulary. \n",
    "\n",
    "The $i$ th and $j$ th entry of the matrix is the number of times that word $i$ and $j$ co-occur in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f544a2",
   "metadata": {},
   "source": [
    "<img src='./img/stanford/2-cooccurrence-matrix-example.png' width=60% alt='Co-occurrence Matrix'>\n",
    "\n",
    "_Figure 1. Co-Occurrence Matrix._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1aa7ac",
   "metadata": {},
   "source": [
    "This is a way of modeling the relationships between words. There are 2 ways to make a co-occurrence matrix:\n",
    "* Using a sliding window over a corpus of text \n",
    "* Using the entire document \n",
    "    * this way is great at capturing general topics in the document (global context)\n",
    "    \n",
    "#### Limitations\n",
    "- Simple count co-occurrence vectors\n",
    "- Vectors increase in size with vocabulary\n",
    "- Very high dimensional: require a lot of storage (though sparse)\n",
    "- Subsequent classification models have sparsity issues, Models are less robust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7bea2",
   "metadata": {},
   "source": [
    "Let's understand **How to reduce the dimensionality**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5451a",
   "metadata": {},
   "source": [
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "Idea is to store ‚Äúmost‚Äù of the important information in a fixed, small number of dimensions: a dense vector\n",
    ", Usually 25‚Äì1000 dimensions, similar to word2vec\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Let's first understand **Single Value Decomposition** briefly\n",
    "\n",
    "Singular Value Decomposition is a method of decomposing a matrix into three other matrices, revealing many of the useful and intrinsic properties of the original matrix. Formally, for any real or complex $( m \\times n )$ matrix $A$, SVD is given by:\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$( U )$** is an $( m \\times m )$ orthogonal matrix whose columns are called the left singular vectors of $A$.\n",
    "- **$\\Sigma $** is an $ m \\times n $ diagonal matrix containing the singular values of $A$ in descending order.\n",
    "- **$V^T$** (the transpose of $V$) is an $n \\times n$ orthogonal matrix whose rows are called the right singular vectors of $A$.\n",
    "\n",
    "</div>\n",
    "\n",
    "[Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c53fe",
   "metadata": {},
   "source": [
    "<img src='./img/stanford/2-svd.png' width=60%> <br>\n",
    "\n",
    "_Figure 2. Reducing the dimensionality of co-occurrence matrices via SVD._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b24b28",
   "metadata": {},
   "source": [
    "SVD on raw counts doesn't work well. \n",
    "\n",
    "Problems:\n",
    "\n",
    "- The dimensions of the matrix change very often (new words are added very frequently and corpus changes in size).\n",
    "- The matrix is extremely sparse since most words do not co-occur.\n",
    "- The matrix is very high dimensional in general (‚âà 106 √ó 106)\n",
    "- Quadratic cost to train (i.e. to perform SVD)\n",
    "- Requires the incorporation of some hacks on X to account for the drastic imbalance in word frequency (see below)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6706c",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a383c",
   "metadata": {},
   "source": [
    "<img src='./img/stanford/2-counts-based-versus-direct-prediction.png' width=60%> <br>\n",
    "\n",
    "_Figure 3. Count-based vs direct prediction._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd47818",
   "metadata": {},
   "source": [
    "GloVe combines the predictive modeling that is in the purple box with the methods for word similarity in the blue box. It also has an additional feature: encoding meaning components in vector differences.\n",
    "\n",
    "Basic idea is <mark>Ratios of co-occurrence probabilities can encode meaning components </mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b732ad2c",
   "metadata": {},
   "source": [
    "<img src='./img/stanford/2-glove-basic-ideas.png' width='600' height='300'>\n",
    "\n",
    "<br>\n",
    "\n",
    "_Figure 4. Encoding meaning in vector differences._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad59be",
   "metadata": {},
   "source": [
    "To create an objective function from this idea, we see that a __log-bilinear model__ can be used to capture the ratio of co-occurrence probabilities.\n",
    "\n",
    "$$\n",
    "w_i ~ \\cdot ~ w_j = log P(i~|~j)\\\\\n",
    "w_x ~ \\cdot (w_a - w_b) =log \\frac{P(x~|~a)}{P(x~|~b)}\\\\\n",
    "$$\n",
    "\n",
    "__GloVe__ uses the following objective function:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i, j = 1}^{V} f(X_{ij})(\\underbrace{w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j}_{pred} - \\underbrace{log X_{ij}}_{label})^2 \\hspace{1em} (Eq.~2)\\\\\n",
    "$$\n",
    "\n",
    "$w_i$ and $w_j$ are embedding vectors for the $i$-th and $j$-th words. They also have their corresponding bias terms. This objective function can directly optimize on the ground truth co-occurrence matrix $X$. The function $f(X_{ij})$ is shown below. Its purpose is to limit the frequency of very frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35092e",
   "metadata": {},
   "source": [
    "<img src='./img/stanford/2-glove-subsampling-f.png' width='400' height='200'>\n",
    "\n",
    "_Figure 5. F function in GloVe._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9ef4ab",
   "metadata": {},
   "source": [
    "### Evaluating Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd60bd5",
   "metadata": {},
   "source": [
    "There are 2 ways to evaluate word vectors:\n",
    "- **Intrinsic**:\n",
    "    - Evaluation on a specific/intermediate subtask\n",
    "    - Fast to compute\n",
    "    - Helps to understand that system\n",
    "    - **Not clear if really helpful unless correlation to real task is established**\n",
    "- **Extrinsic**:\n",
    "    - Evaluation on a real task\n",
    "    - Can take a long time to compute accuracy\n",
    "    - Unclear if the subsystem is the problem or its interaction or other subsystems\n",
    "    - Unstable metrics: if replacing exactly one subsystem with another, accuracy may just be improved\n",
    "    \n",
    "    \n",
    "For **intrinsic word vector evaluation**, you can evaluate it like: woman - man + king = queen.\n",
    "\n",
    "$$\n",
    "d = \\underset{i}{argmax} \\frac{(x_b - x_a + x_c)^T x_i}{||x_b - x_a + x_c||} \\hspace{1em} (Eq.~3)\\\\\n",
    "$$\n",
    "\n",
    "In GloVe, they found a dimension parameter of 300 was good with more data leading to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffbcad9",
   "metadata": {},
   "source": [
    "There are shortcomings to GloVe. For one, many words have multiple meanings. Sarcasm and other things like mood and tone are harder to pick up.\n",
    "\n",
    "One idea is to have multiple sensor vectors for all the definitions of a word. Though in practice it is often enough to just have 1 embedding vector per word as it is a weighted sum (or linear superposition) of all the definitions of that word. Because of _sparse encoding_ (high-dimensional embedding dimension), we can actually separate out the senses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53c548",
   "metadata": {},
   "source": [
    "## Dependency Parsing\n",
    "\n",
    "Dependency parsing is the process of inferring the syntactic structure of a sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6599080",
   "metadata": {},
   "source": [
    "### Syntactic structure: constituency and dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a523dcb",
   "metadata": {},
   "source": [
    "### **<mark>Constituency</mark>**\n",
    "\n",
    "- Also known as **phrase structure grammar** or **context-free grammars (CFGs)**\n",
    "\n",
    "- Key assumption: **Phrase structure organizes words into nested constituents**\n",
    "    - Words (basic units, assigned with parts of speech) $\\longrightarrow$ phrases $\\longrightarrow$ bigger phrases $\\longrightarrow$ ... \n",
    "- Example:\n",
    "\n",
    "<img src='./img/stanford/5-constituency.png' width='600' height='300'>\n",
    "\n",
    "<br>\n",
    "\n",
    "Phrase structure does this by defining a __lexicon__ and a __grammar__. \n",
    "\n",
    "<mark>The lexicon is simply the type of word (noun, preposition, verb, etc) mapped to the word (a dictionary mapping).</mark>.\n",
    "\n",
    "The grammar is a list of rules that map from, say, a noun to a verb, or a verb to a preposition, etc. This is considered __context-free grammar (CFG)__ because it doesn't account for what phrases would be generated.\n",
    "\n",
    "The other way at viewing linguistic structure is __dependency structure__ and that is by looking at a word in a sentence and seeing what other words in the sentence modifies it.\n",
    "\n",
    "### **<mark>Dependency</mark>**\n",
    "\n",
    "- Dependency structure: **which words depend on (modify or are arguments of) which other words**; semantic relationship\n",
    "\n",
    "- Example (from the internet):\n",
    "\n",
    "\n",
    "<img src='./img/stanford/5-dependency-example.png' width='400' height='200'>\n",
    "\n",
    "<br>\n",
    "\n",
    "- Why dependency?\n",
    "    - We need to understand sentence structure in order to be able to interpret language correctly\n",
    "    - Humans communicate complex ideas by composing words together into bigger units to convey complex meanings\n",
    "    - We need to know what is connected to what\n",
    "    - Otehrwise, ambiguity\n",
    "    \n",
    "\n",
    "- Dependency paths identify semantic relations\n",
    "    - e.g.: The result demonstrated that KaiC interacts rhythmically with SasA, KaiA, and KaiB.\n",
    "\n",
    "\n",
    "<img src='./img/stanford/5-dependency-example2.png' width='400' height='200'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a004e0",
   "metadata": {},
   "source": [
    "Below are some examples of ambiguities in the English language (and thus why sentence structure is important):\n",
    "\n",
    "* **Prepositional phrase attachment ambiguity**\n",
    "    * <mark>San Jose cops kill man with knife</mark>\n",
    "    * did the cops kill a man holding a knife or did they use a knife to kill him\n",
    "* **Coordination scope ambiguity**\n",
    "    * <mark>Shuttle veteran and longtime NASA executive Fred Gregory appointed to board</mark>\n",
    "    * is this describing 1 person (Fred Gregory as the shuttle veteran and longtime NASA executive)\n",
    "    * or is this describing 2 people (a shuttle veteran and Fred Gregory who is a longtime NASA executive)\n",
    "* **Adjectival/adverbial modifier ambiguity**\n",
    "    * <mark>Students get first hand job experience</mark>\n",
    "        * what is this saying?\n",
    "* **Verb phrase attachment ambiguity**\n",
    "    * <mark>Mutilated body washes up on Rio beach to be used for Olympics beach volleyball </mark>\n",
    "        * is the mutilated body going to be used as a volleyball?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754548ca",
   "metadata": {},
   "source": [
    "### Dependency grammar and dependency structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682ff2e",
   "metadata": {},
   "source": [
    "Dependency syntax postulates that syntactic structure consists of relations between lexical items, **normally binary asymmetric relations** (‚Äúarrows‚Äù) called **dependencies**.\n",
    "\n",
    "Currently, there is no unified method to represent dependency structures graphically. For example, in the notebook, the arrows start from the head and point to the dependent. However, some may have the arrows start from the dependent and point to the head. Nevertheless, the following two methods are two basic ways to represent dependency structures in a graph. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**<mark>Represented by a connected, acyclic, single-head tree**</mark>\n",
    "\n",
    "<img src='./img/stanford/5-dependency-grammar.png' width='600' height='300'>\n",
    "\n",
    "\n",
    "**<mark>Represented by dependency arcs (curved arrows) above a sentence</mark>**\n",
    "- Usually add a fake ROOT so every word is a dependent of precisely 1 other node\n",
    "\n",
    "<img src='./img/stanford/5-dependency-grammar-repr.png' width='600' height='300'>\n",
    "\n",
    "<br>\n",
    "\n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b8b6b1",
   "metadata": {},
   "source": [
    "Dependency structure is usually done by drawing an arrow from the head to the dependent word or the words that modify the head. We also usually add a fake ROOT so every word is dependent on roughly 1 other node.\n",
    "\n",
    "A __dependency parser__ is basically an algorithm that will generate some quantified format of the dependency structure you see in Figure 3. \n",
    "\n",
    "A __tree bank__ is a huge collection of hand-annotated trees.\n",
    "\n",
    "Usually, we start with writing a grammar or lexicon, but with tree banks, it's a little different. Granted, treebanks are slower and are initially less useful than writing a grammar by hand. However, treebanks are reusable for many different tasks (dependency parsing, other things can be built to extract information from parsing). They can also be used to evaluate NLP systems. I'm imagining a hand-annotated treebank can be compared to different dependency parsers to see which algorithm works best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf21989",
   "metadata": {},
   "source": [
    "### Building A Dependency Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c338475",
   "metadata": {},
   "source": [
    "Dependency parsers leverage 4 common themes:\n",
    "- Bilexical affinities \n",
    "    - basically how likely is one word to be dependent on another word (it's a pairing like discussion -> issues)\n",
    "- Dependency distance\n",
    "    - dependencies are usually between nearby words\n",
    "- Intervening material\n",
    "    - dependencies rarely span intervening verbs or punctuation\n",
    "- Valency of heads\n",
    "    - how many dependents on which side are usual for a head?\n",
    "    \n",
    "To perform dependency parsing, **we parse through a sentence and choose for each word what other word is it a dependent of**. \n",
    "\n",
    "There are some constraints:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f731a",
   "metadata": {},
   "source": [
    "<img src='./img/stanford/5-parsing-general.png' width='600' height='300'> <br>\n",
    "\n",
    "_Figure 4. Crossing arcs._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
