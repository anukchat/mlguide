{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "___\n",
    "\n",
    "In NLP, embeddings are the vectors which represent some aspects (meaning) for words or documents, they are represented in the form of mathematical matrices.\n",
    "\n",
    "There are many types of embeddings - such as, character embedding, word embedding, sentence embedding, or document embedding, we will explore sentence vectorization in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Vectors\n",
    "\n",
    "A sparse vector is a type of vector in which most of its elements are zero. This concept is prevalent in various fields such as computer science, mathematics, and data science, particularly in areas dealing with high-dimensional data. Understanding sparse vectors is crucial for efficient data storage, processing, and analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn nltk pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorize\n",
    "\n",
    "One of primitive method to vectorize a text is count vectorization.<br>\n",
    "\n",
    "This method is based on one hot vectorizing and each element represents the count of that word in a document as follows.\n",
    "\n",
    "![Count vectorize](images/count_vectorize.png)\n",
    "\n",
    "Count vectorization is very straighforward and comprehensive for humans, but it'll build sparse vectors (in which, almost elements are zero) and also resource-intensive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>book</th>\n",
       "      <th>here</th>\n",
       "      <th>is</th>\n",
       "      <th>my</th>\n",
       "      <th>pen</th>\n",
       "      <th>these</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  and  are  book  here  is  my  pen  these  this\n",
       "0  1    0    0     1     0   1   0    0      0     1\n",
       "1  0    1    1     0     1   1   1    2      1     0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "#  Lemmatizer helps to reduce words to their root form \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Convert :\n",
    "# \"pens\" -> \"pen\"\n",
    "# \"wolves\" -> \"wolf\"\n",
    "def my_lemmatizer(text):\n",
    "    return [lemmatizer.lemmatize(t) for t in word_tokenize(text)]\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=my_lemmatizer)\n",
    "texts = [\n",
    "    \"This is a book\",\n",
    "    \"These are pens and my pen is here\"\n",
    "]\n",
    "vectors = vectorizer.fit_transform(texts)\n",
    "\n",
    "cols = [k for k, v in sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])]\n",
    "df = pd.DataFrame(vectors.toarray(), columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>This vectorization often results into low performance (low accuracy) in several ML use-cases. </mark> \n",
    "\n",
    "(Since the neural network won't work well with very high-dimensional and sparse vectors.)<br>\n",
    "\n",
    "The following is the example for classifying document into 20 e-mail groups.\n",
    "\n",
    "> Note : In the real usage, train with unknown words with a specific symbol, such as \"[UNK]\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsmatsuz/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy: 0.6240042485395645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Load train dataset\n",
    "train = fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "\n",
    "# Count vectorize\n",
    "vectorizer.fit(train.data)\n",
    "X_trian = vectorizer.transform(train.data)\n",
    "y_train = train.target\n",
    "\n",
    "# Train\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(X_trian, y_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "test = fetch_20newsgroups(\n",
    "    subset=\"test\",\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "X_test = vectorizer.transform(test.data)\n",
    "y_test = test.target\n",
    "y_pred = clf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"classification accuracy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF weighting\n",
    "\n",
    "TF-IDF (Term Frequency - Inverse Document Frequency) weighting is a widely used statistical method for evaluating the importance of a word in a document relative to a collection of documents (corpus).\n",
    "\n",
    "\n",
    "In earlier example, the weight of word \"book\" or \"pen\" is the same as the weight of words \"a\", \"for\", \"the\", etc.<br>\n",
    "\n",
    "<mark>Using TF-IDF, you can prioritize the words that rarely appear in the given corpus.</mark>\n",
    "\n",
    "**TF (=**T**erm **F**requency) is**\n",
    "\n",
    "Measures how frequently a term appears in a document.\n",
    "\n",
    "$$ \\frac{\\#d(w)}{\\sum_{w^{\\prime} \\in d} \\#d(w^{\\prime})} $$\n",
    "\n",
    "in which, $ \\#d(w) $ means the count of word $w$ in document $d$.<br>\n",
    "\n",
    "TF is the normalized value of the count of word $w$ in document $d$. \n",
    "\n",
    "**IDF (=**I**nverse **D**ocument **F**requency) is**\n",
    "\n",
    "Measures how important a term is across the entire corpus.\n",
    "\n",
    "$$\\log{\\frac{|D|}{|\\{d \\in D:w\\in d\\}|}}$$\n",
    "\n",
    "This term diminishes the weight of terms that occur very frequently in the corpus and increases the weight of terms that are rare. This helps in highlighting terms that are more informative.\n",
    "\n",
    "**TF-IDF is**\n",
    "\n",
    "$$ \\frac{\\#d(w)}{\\sum_{w^{\\prime} \\in d} \\#d(w^{\\prime})} \\times \\log{\\frac{|D|}{|\\{d \\in D:w\\in d\\}|}}$$\n",
    "\n",
    "where $D$ is large corpus (a set of documents).\n",
    "\n",
    "If some word $w$ (such like, \"a\", \"the\") is included in all document $d \\in D$, the second term will be **relatively small**. \n",
    "\n",
    "If some word is rarely included in $d \\in D$, the second term will be **relatively large**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the following example.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Convert :\n",
    "# \"pens\" -> \"pen\"\n",
    "# \"wolves\" -> \"wolf\"\n",
    "def my_lemmatizer(text):\n",
    "    return [lemmatizer.lemmatize(t) for t in word_tokenize(text)]\n",
    "\n",
    "# Count vectorize\n",
    "count_vectorizer = CountVectorizer(tokenizer=my_lemmatizer)\n",
    "texts = [\n",
    "    \"This is a book\",\n",
    "    \"These are pens and my pen is here\"\n",
    "]\n",
    "count_vectors = count_vectorizer.fit_transform(texts)\n",
    "\n",
    "# TF-IDF weighting\n",
    "tfidf_trans = TfidfTransformer(use_idf=True).fit(count_vectors)\n",
    "tfidf_vectors = tfidf_trans.transform(count_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, only the word \"is\" is included in both documents. \n",
    "\n",
    "The word \"pen\" is also used twice, however, this word is not used in the first document.<br>\n",
    "\n",
    "As a result, only the word \"is\" has small value for IDF weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>book</th>\n",
       "      <th>here</th>\n",
       "      <th>is</th>\n",
       "      <th>my</th>\n",
       "      <th>pen</th>\n",
       "      <th>these</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>1.405465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a       and       are      book      here   is        my       pen  \\\n",
       "0  1.405465  1.405465  1.405465  1.405465  1.405465  1.0  1.405465  1.405465   \n",
       "\n",
       "      these      this  \n",
       "0  1.405465  1.405465  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [k for k, v in sorted(count_vectorizer.vocabulary_.items(), key=lambda item: item[1])]\n",
    "df = pd.DataFrame([tfidf_trans.idf_], columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated vectors has the following values.<br>\n",
    "\n",
    "As you can see below, the word \"is\" has relatively small value compared with other words in the same document.<br>\n",
    "\n",
    "The second document (\"These are pens and my pen is here\") has more words than the first document (\"This is a book\"), and then TF values (normalized values) in the second document are small rather than ones in the first document.<br>\n",
    "\n",
    "The word \"pen\" appears in the second documnt twice, and it then has 2x values compared to other words in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>book</th>\n",
       "      <th>here</th>\n",
       "      <th>is</th>\n",
       "      <th>my</th>\n",
       "      <th>pen</th>\n",
       "      <th>these</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.230768</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.648673</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a       and       are      book      here        is        my  \\\n",
       "0  0.534046  0.000000  0.000000  0.534046  0.000000  0.379978  0.000000   \n",
       "1  0.000000  0.324336  0.324336  0.000000  0.324336  0.230768  0.324336   \n",
       "\n",
       "        pen     these      this  \n",
       "0  0.000000  0.000000  0.534046  \n",
       "1  0.648673  0.324336  0.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tfidf_vectors.toarray(), columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the example for classifying text into 20 e-mail groups. (Compare the result with the previous one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsmatsuz/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification accuracy: 0.6964949548592672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Load train dataset\n",
    "train = fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "\n",
    "# Count vectorize\n",
    "count_vectorizer.fit(train.data)\n",
    "X_train_count = count_vectorizer.transform(train.data)\n",
    "\n",
    "# TF-IDF weighting\n",
    "tfidf_trans = TfidfTransformer(use_idf=True).fit(X_train_count)\n",
    "X_train_tfidf = tfidf_trans.transform(X_train_count)\n",
    "\n",
    "# Train\n",
    "y_train = train.target\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "test = fetch_20newsgroups(\n",
    "    subset=\"test\",\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "X_test_count = count_vectorizer.transform(test.data)\n",
    "X_test_tfidf = tfidf_trans.transform(X_test_count)\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "y_test = test.target\n",
    "score = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"classification accuracy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF can also be applied to dense vectors** as follows :\n",
    "\n",
    "$$ \\frac{1}{\\sum_{i=1}^{k} \\verb|tfidf|(w_i)} \\sum_{i=1}^{k} \\verb|tfidf|(w_i) v(w_i) $$\n",
    "\n",
    "where $v(\\cdot)$ is word's vectorization (dense vector) and $\\verb|tfidf|(\\cdot)$ is TF-IDF weighting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Vectors\n",
    "\n",
    "As you saw in the previous examples, the <mark>generated count vectors are sparse</mark> and a lot of algorithms won't work well with this high-dimensional vectors.<br>\n",
    "\n",
    "For this reason, <mark>refined trainers will transform sparse vectors into non-sparse forms (**dense vectors**)</mark> and process some tasks (such as, NLP classification, etc) againt these dense vectors in practice.\n",
    "\n",
    "![Dense vectorize](images/dense_vectorize.png)\n",
    "\n",
    "👉 In today's advanced embedding, the embedding layer can sometimes be the model of non-linear neural networks.<br>\n",
    "\n",
    "👉 But, in most cases, word embedding is essentially a lookup table which maps a sparse vector into a dense vector. \n",
    "\n",
    "Assuming $\\mathbf{w}$ is a word index vector (i.e, sparse vector) with voculabrary size $|V|$, \n",
    "\n",
    "👉 in which the i-th element of $\\mathbf{w}$ is $1$ and other elements are $0$, \n",
    "\n",
    "👉 the embedding table $\\mathbf{E}$ will then be a $ |V| \\times d $ matrix which converts a sparse vector $\\mathbf{w}$ to a $d$-dimensional dense vector by $ \\mathbf{w} \\mathbf{E} $. (i.e, The i-th row of $\\mathbf{E}$ is a dense vector for a word $\\mathbf{w}$.)<br>\n",
    "\n",
    "The $ |V| \\times d $ parameters will then be trained by some task.\n",
    "\n",
    "![Embedding](images/embedding_matrix.png)\n",
    "\n",
    "<div class=\"alert alert-info alrt-block\">\n",
    "\n",
    "👉 **Note** : A lookup table cannot be used in sentence embedding, because the number of sentences is not finite.<br>\n",
    "\n",
    "</div>\n",
    "\n",
    "👉 <mark>The generated dense vector (i.e, non-sparse form) will represent some aspects (meaning) for words or documents.</mark>\n",
    "\n",
    "In order to get the trained (optimal) parameters of $\\mathbf{E}$ (and optimal dense vectors), you can take either of the following 3 options :\n",
    "\n",
    "1. Train embeddings $\\mathbf{E}$ from the beginning.\n",
    "2. Use existing pre-trained embeddings $\\mathbf{E_0}$ trained by a large text corpus. (For instance, see Hugging Face hub for a lot of pre-trained SOTA models.)\n",
    "3. Download pre-trained embeddings $\\mathbf{E_0}$ and train (fine-tune) $\\mathbf{E_0}$ furthermore to get new optimal $\\mathbf{E_1}$.\n",
    "\n",
    "<div class=\"alert alert-info alert-block\">\n",
    "\n",
    "💡 **Note** : In order to fine-tune the pre-trained vectors, there also exists the following approaches :<br>\n",
    "- Find an additional matrix $\\mathbf{T} \\in \\mathbb{R}^{d \\times d} $, with which we can obtain new embedding $\\mathbf{E} \\mathbf{T}$\n",
    "- Find an additional matrix $\\mathbf{A} \\in \\mathbb{R}^{|V| \\times d} $, with which we can obtain new embedding $\\mathbf{E} + \\mathbf{A}$\n",
    "- Hybrid of 1 and 2\n",
    "\n",
    "</div>\n",
    "\n",
    "In a lot of today's NLP models, <mark>the word is embedded into dense vectors and the sequence of words in document is trained by RNN-based learners , Attention-based learners, or Transformers with a large corpus </mark>.\n",
    "\n",
    "However, for the purpose of understanding, we'll see a simple classification trainer, in which the word is embedded and the sequence is combined by using primitive continuos bag-of-words (CBOW) representation.<br>\n",
    "\n",
    "In this example, we'll train a model (which includes custom embedding) to detect sentiment with movie review dataset (IMDB) for natural language processing.\n",
    "\n",
    "CBOW (continuos bag-of-words) representation is a combination of vectors, which is obtained by the mean (average) of vectors as follows. (The magnitude of vector doesn't then depend on the length of sentence.)\n",
    "\n",
    "$ \\frac{1}{k} \\sum_{i=1}^{k} v(w_i) $ &nbsp;&nbsp;&nbsp; where $v(\\cdot)$ is dense vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.13.1 torchtext==0.14.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use IMDB dataset (movie review dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_iter = IMDB(split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The record number is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(list(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pick up and see the first row of records.<br>\n",
    "In this dataset, it includes the review text and 2-class flag 1 or 2 for satisfied/dissatisfied respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** text *****\n",
      "I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n",
      "***** label *****\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# test (print first row)\n",
    "for label, text in train_iter:\n",
    "    print(\"***** text *****\")\n",
    "    print(text)\n",
    "    print(\"***** label *****\")\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the better performance (accuracy), we standarize the input's review text as follows.\n",
    "\n",
    "👉 Make all words to lowercase<br>\n",
    "  (ex. \"I am a Greatest Showman !\" -> \"i am a greatest showman !\")\n",
    "\n",
    "👉 Remove all stop words, such as, \"a\", \"the\", \"is\", \"i\", etc<br>\n",
    "  (ex. \"i am a greatest showman !\" -> \"greatest showman !\")\n",
    "\n",
    "👉 Remove all punctuation, such as, \"!\", \"?\", \"#\", etc<br>\n",
    "  (ex. \"greatest showman !\" -> \"greatest showman\")\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note** : Some normalization - such as, changing to lower case - is also done in the following tokenizer.<br>\n",
    "\n",
    "**N-gram words** (such as, \"New York\", \"Barack Obama\") and lemmatization (standardization for such as \"have\", \"had\" or \"having\") should be dealed with, but here I have skipped these pre-processing. \n",
    "\n",
    "In the strict pre-processing, we should also care about the polysemy. (The different meanings in the same word should have different tokens.)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def standarize_text(text):\n",
    "    new_text = text\n",
    "\n",
    "    # 1. To lowercase\n",
    "    new_text = new_text.lower()\n",
    "\n",
    "    # 2. Remove stop words\n",
    "    for w in stopwords.words(\"english\"):\n",
    "        new_text = re.sub(\n",
    "            \"(^|\\s+)%s(\\s+|$)\" % re.escape(w),\n",
    "            \" \",\n",
    "            new_text)\n",
    "    new_text = new_text.strip()\n",
    "\n",
    "    # 3. Remove punctuation\n",
    "    new_text = new_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    new_text = new_text.strip()\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'greatest showman'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "standarize_text(\"I am a Greatest Showman !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a word's index vector as follows, first we create a list for words (```vocab```) used in the training set.\n",
    "\n",
    "![Index vectorize](images/index_vectorize.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "vocab_size = 10000\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# define tokenization function\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        text = standarize_text(text)\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# build vocabulary list\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_iter),\n",
    "    specials=[\"<unk>\"],\n",
    "    max_tokens=vocab_size\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# get list for index-to-word, and word-to-index\n",
    "itos = vocab.get_itos()\n",
    "stoi = vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[681, 2, 41, 0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "vocab([\"greatest\", \"movie\", \"show\", \"abcdefghijk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build data loader with a collator function, in which data is pre-processed.\n",
    "\n",
    "In this collator,\n",
    "\n",
    "1. The input's text is standarized with previous ```standarize_text``` function.\n",
    "2. The input's text is then tokenized into word's index (integer's list).\n",
    "3. Limit to 256 tokens.\n",
    "4. Generate mask array. For instance, if the length of token is 3, it will become ```[1.0, 1.0, 1.0, 0.0, 0.0, ..., 0.0]```.\n",
    "5. Pad all sequence by zero, if the length of sequence is shorter than max tokens.\n",
    "6. Convert curent label 1 or 2 into 0 or 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seq_len = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_list, mask_list = [], [], []\n",
    "    for (label, text) in batch:\n",
    "        # 1. standarize text\n",
    "        text = standarize_text(text)\n",
    "        # 2. generate word's index vector\n",
    "        tokens = vocab(tokenizer(text))\n",
    "        # 3. limit to first tokens\n",
    "        tokens = tokens[:seq_len]\n",
    "        # 4. generate mask array\n",
    "        length = len(tokens)\n",
    "        mask_array = [float(i < length) for i in range(seq_len)]\n",
    "        # 5. pad sequence\n",
    "        tokens += [0] * (seq_len - len(tokens))\n",
    "        # 6. convert label into 0 or 1\n",
    "        label = label - 1\n",
    "        # add to list\n",
    "        label_list.append(label)\n",
    "        token_list.append(tokens)\n",
    "        mask_list.append(mask_array)\n",
    "    # convert to tensor\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64).to(device)\n",
    "    token_list = torch.tensor(token_list, dtype=torch.int64).to(device)\n",
    "    mask_list = torch.tensor(mask_list, dtype=torch.float).to(device)\n",
    "    return label_list, token_list, mask_list\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_iter,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape in batch : torch.Size([128])\n",
      "token shape in batch : torch.Size([128, 256])\n",
      "mask  shape in batch : torch.Size([128, 256])\n",
      "***** label sample *****\n",
      "tensor(0, device='cuda:0')\n",
      "***** token sample *****\n",
      "tensor([8261,    4, 1440,   23,    5,   17,   27,    0,    0, 2639,  232, 3768,\n",
      "        3301,  852,    0,  226,    0,    0, 6659,    0,  343,   87,  310,  224,\n",
      "        1476, 4918,    0,    0,    0,    0,    0, 4035,  680,  138, 3386,    0,\n",
      "          62,  859, 1068,   11,    0,  379, 3355,    0,  105,    5,   10,   71,\n",
      "         530,  200,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0], device='cuda:0')\n",
      "***** input text *****\n",
      "['file', 'one', 'how', 'movies', 'like', 'get', 'made', '<unk>', '<unk>', 'indie', 'version', 'macbeth', 'adapted', 'fairly', '<unk>', 'but', '<unk>', '<unk>', 'unconventional', '<unk>', 'style', 'cast', 'gives', 'shot', 'christopher', 'walken', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'rising', 'dull', 'script', 'pat', '<unk>', 'actors', 'wasted', 'audiences', 'time', '<unk>', 'fans', 'brand', '<unk>', 'may', 'like', 'it', 'though', '4', '10', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "***** mask *****\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for labels, tokens, masks in dataloader:\n",
    "    break\n",
    "\n",
    "print(\"label shape in batch : {}\".format(labels.size()))\n",
    "print(\"token shape in batch : {}\".format(tokens.size()))\n",
    "print(\"mask  shape in batch : {}\".format(masks.size()))\n",
    "print(\"***** label sample *****\")\n",
    "print(labels[0])\n",
    "print(\"***** token sample *****\")\n",
    "print(tokens[0])\n",
    "print(\"***** input text *****\")\n",
    "print([itos[i] for i in tokens[0]])\n",
    "print(\"***** mask *****\")\n",
    "print(masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll build the embedding module.\n",
    "\n",
    "![Embedding module](images/embedding_layer.png)\n",
    "\n",
    "This module converts each word's index into corresponding embedded vector (dense vector) as follows.<br>\n",
    "If the size of inputs is ```[128, 256]``` and embedding dimension is ```16```, the size of inputs will then become ```[128, 256, 16]```\n",
    "\n",
    "![Word embeddings](images/word_embedding.png)\n",
    "\n",
    "> Note : Here for learning purpose, we are creating a custom embedding module from scratch, but we can use ```torch.nn.Embedding``` module in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 16\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.empty((num_embeddings, embedding_dim)))\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.weight, -0.1, 0.1)\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        return self.weight[input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape : torch.Size([128, 256, 16])\n",
      "***** output *****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0537,  0.0995, -0.0639,  ..., -0.0117, -0.0210, -0.0004],\n",
       "         [ 0.0994, -0.0006,  0.0512,  ..., -0.0381,  0.0587, -0.0776],\n",
       "         [-0.0408, -0.0242, -0.0683,  ..., -0.0763, -0.0033,  0.0752],\n",
       "         ...,\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391],\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391],\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391]],\n",
       "\n",
       "        [[-0.0635, -0.0664,  0.0637,  ..., -0.0536, -0.0019,  0.0504],\n",
       "         [ 0.0791,  0.0604,  0.0386,  ...,  0.0170,  0.0313, -0.0888],\n",
       "         [-0.0570, -0.0719, -0.0392,  ..., -0.0905,  0.0871, -0.0014],\n",
       "         ...,\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391],\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391],\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391]],\n",
       "\n",
       "        [[ 0.0164,  0.0208, -0.0965,  ..., -0.0159, -0.0105, -0.0977],\n",
       "         [-0.0785,  0.0128,  0.0330,  ..., -0.0051,  0.0685, -0.0039],\n",
       "         [ 0.0897, -0.0164,  0.0486,  ...,  0.0224, -0.0740,  0.0836],\n",
       "         ...,\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391],\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391],\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0767,  0.0026,  0.0898,  ...,  0.0570, -0.0813,  0.0734],\n",
       "         [ 0.0147,  0.0882, -0.0059,  ..., -0.0005,  0.0965,  0.0330],\n",
       "         [-0.0840,  0.0118, -0.0646,  ...,  0.0153,  0.0669, -0.0279],\n",
       "         ...,\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391],\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391],\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391]],\n",
       "\n",
       "        [[ 0.0865, -0.0626, -0.0700,  ..., -0.0905,  0.0485, -0.0487],\n",
       "         [-0.0176,  0.0219, -0.0390,  ..., -0.0985, -0.0119, -0.0879],\n",
       "         [-0.0978, -0.0426,  0.0983,  ...,  0.0707,  0.0164, -0.0214],\n",
       "         ...,\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391],\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391],\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391]],\n",
       "\n",
       "        [[-0.0801, -0.0978, -0.0325,  ...,  0.0299,  0.0905, -0.0513],\n",
       "         [ 0.0348, -0.0246,  0.0625,  ...,  0.0199, -0.0878, -0.0442],\n",
       "         [-0.0831,  0.0064,  0.0506,  ...,  0.0753, -0.0491,  0.0541],\n",
       "         ...,\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391],\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391],\n",
       "         [ 0.0227,  0.0767,  0.0386,  ..., -0.0433,  0.0949, -0.0391]]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "e = Embedding(vocab_size, embedding_dim).to(device)\n",
    "embs = e(tokens)\n",
    "\n",
    "print(\"output shape : {}\".format(embs.size()))\n",
    "print(\"***** output *****\")\n",
    "embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get CBOW (continuous bag-of-words) representation for word's embedded vectors as follows.\n",
    "\n",
    "$$ \\frac{1}{k} \\sum_{i=1}^{k} v(w_i) $$\n",
    "\n",
    "Where $w_i$ is a word vector (in this case, the scalar number representing a word) and $v(\\cdot)$ is embedding function.\n",
    "\n",
    "![CBOW](images/continuous_bow.png)\n",
    "\n",
    "👉 In this CBOW representation, the order of words in the sentence will be ignored, and it won't then capture contexts, such as :\n",
    "\n",
    "\"it's exciting, but it's unfavorable.\"\n",
    "\n",
    "Furthermore it won't understand n-grams, such as, \"don't like\".<br>\n",
    "\n",
    "In the following CBOW representation's implementation, I use mask (in which 0 is assigned in padded positions, and otherwise 1) in order to skip computation in padded positions.\n",
    "\n",
    "> Note : In PyTorch, you can use ```torch.nn.functional.avg_pool1d()``` for averaging globally. You can also use ```torch.nn.EmbeddingBag``` to get the mean of embedded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def forward(self, embedded, masks):\n",
    "        # generate mask with embedding --> [batch_size, seq_len, embedding_dim]\n",
    "        extend_masks = masks.unsqueeze(dim=2)\n",
    "        extend_masks = extend_masks.expand(-1, -1, embedding_dim)\n",
    "        # filter embedding by multiplication\n",
    "        masked_embedded = embedded * extend_masks\n",
    "        # sum all embedding in each sequence --> [batch_size, embedding_dim]\n",
    "        embedded_sum = masked_embedded.sum(dim=1)\n",
    "        # compute token length --> [batch_size]\n",
    "        token_length = masks.sum(dim=1)\n",
    "        # divide by token length\n",
    "        # [batch_size, embedding_dim] / [batch_size] --> [batch_size, embedding_dim]\n",
    "        embedded_sum = embedded_sum.transpose(0,1) / token_length\n",
    "        return embedded_sum.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape : torch.Size([128, 16])\n",
      "***** output *****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0109,  0.0271,  0.0090,  ..., -0.0210,  0.0267, -0.0084],\n",
       "        [ 0.0064,  0.0199,  0.0137,  ..., -0.0186,  0.0045, -0.0095],\n",
       "        [ 0.0061,  0.0170, -0.0031,  ..., -0.0107,  0.0222, -0.0050],\n",
       "        ...,\n",
       "        [ 0.0050,  0.0042, -0.0062,  ..., -0.0173,  0.0200, -0.0131],\n",
       "        [-0.0021,  0.0077, -0.0024,  ..., -0.0098,  0.0032, -0.0044],\n",
       "        [ 0.0041,  0.0017,  0.0063,  ..., -0.0084,  0.0047, -0.0023]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "c = CBOW().to(device)\n",
    "cbow = c(embs, masks)\n",
    "\n",
    "print(\"output shape : {}\".format(cbow.size()))\n",
    "print(\"***** output *****\")\n",
    "cbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll build the task layer.\n",
    "\n",
    "![Task layer](images/task_layer.png)\n",
    "\n",
    "In our network, we just use fully connected feed-forward network (DenseNet), in which the final output is one-hot logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape : torch.Size([128, 2])\n",
      "***** output *****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1841, -0.0176],\n",
       "        [ 0.1860, -0.0200],\n",
       "        [ 0.1846, -0.0180],\n",
       "        [ 0.1830, -0.0159],\n",
       "        [ 0.1825, -0.0143],\n",
       "        [ 0.1820, -0.0134],\n",
       "        [ 0.1762, -0.0149],\n",
       "        [ 0.1812, -0.0116],\n",
       "        [ 0.1811, -0.0171],\n",
       "        [ 0.1789, -0.0131],\n",
       "        [ 0.1859, -0.0158],\n",
       "        [ 0.1821, -0.0178],\n",
       "        [ 0.1768, -0.0121],\n",
       "        [ 0.1690, -0.0115],\n",
       "        [ 0.1768, -0.0138],\n",
       "        [ 0.1794, -0.0155],\n",
       "        [ 0.1816, -0.0166],\n",
       "        [ 0.1573, -0.0196],\n",
       "        [ 0.1725, -0.0146],\n",
       "        [ 0.1761, -0.0134],\n",
       "        [ 0.1796, -0.0169],\n",
       "        [ 0.1806, -0.0131],\n",
       "        [ 0.1821, -0.0139],\n",
       "        [ 0.1806, -0.0194],\n",
       "        [ 0.1818, -0.0148],\n",
       "        [ 0.1746, -0.0088],\n",
       "        [ 0.1834, -0.0149],\n",
       "        [ 0.1837, -0.0178],\n",
       "        [ 0.1864, -0.0232],\n",
       "        [ 0.1800, -0.0160],\n",
       "        [ 0.1772, -0.0126],\n",
       "        [ 0.1855, -0.0177],\n",
       "        [ 0.1804, -0.0155],\n",
       "        [ 0.1908, -0.0187],\n",
       "        [ 0.1863, -0.0173],\n",
       "        [ 0.1746, -0.0128],\n",
       "        [ 0.1857, -0.0180],\n",
       "        [ 0.1756, -0.0130],\n",
       "        [ 0.1836, -0.0153],\n",
       "        [ 0.1831, -0.0137],\n",
       "        [ 0.1800, -0.0171],\n",
       "        [ 0.1820, -0.0110],\n",
       "        [ 0.1765, -0.0135],\n",
       "        [ 0.1839, -0.0156],\n",
       "        [ 0.1767, -0.0137],\n",
       "        [ 0.1806, -0.0149],\n",
       "        [ 0.1824, -0.0170],\n",
       "        [ 0.1720, -0.0106],\n",
       "        [ 0.1751, -0.0115],\n",
       "        [ 0.1871, -0.0164],\n",
       "        [ 0.1848, -0.0188],\n",
       "        [ 0.1736, -0.0184],\n",
       "        [ 0.1861, -0.0167],\n",
       "        [ 0.1811, -0.0149],\n",
       "        [ 0.1813, -0.0125],\n",
       "        [ 0.1873, -0.0162],\n",
       "        [ 0.1753, -0.0105],\n",
       "        [ 0.1823, -0.0159],\n",
       "        [ 0.1818, -0.0155],\n",
       "        [ 0.1724, -0.0100],\n",
       "        [ 0.1878, -0.0152],\n",
       "        [ 0.1823, -0.0155],\n",
       "        [ 0.1781, -0.0126],\n",
       "        [ 0.1825, -0.0147],\n",
       "        [ 0.1840, -0.0126],\n",
       "        [ 0.1821, -0.0114],\n",
       "        [ 0.1826, -0.0158],\n",
       "        [ 0.1797, -0.0110],\n",
       "        [ 0.1854, -0.0164],\n",
       "        [ 0.1795, -0.0169],\n",
       "        [ 0.1912, -0.0220],\n",
       "        [ 0.1789, -0.0136],\n",
       "        [ 0.1828, -0.0167],\n",
       "        [ 0.1779, -0.0124],\n",
       "        [ 0.1877, -0.0150],\n",
       "        [ 0.1755, -0.0172],\n",
       "        [ 0.1816, -0.0126],\n",
       "        [ 0.1829, -0.0150],\n",
       "        [ 0.1822, -0.0157],\n",
       "        [ 0.1731, -0.0136],\n",
       "        [ 0.1791, -0.0172],\n",
       "        [ 0.1860, -0.0201],\n",
       "        [ 0.1766, -0.0163],\n",
       "        [ 0.1737, -0.0159],\n",
       "        [ 0.1795, -0.0175],\n",
       "        [ 0.1859, -0.0173],\n",
       "        [ 0.1768, -0.0142],\n",
       "        [ 0.1850, -0.0175],\n",
       "        [ 0.1801, -0.0129],\n",
       "        [ 0.1897, -0.0168],\n",
       "        [ 0.1747, -0.0095],\n",
       "        [ 0.1874, -0.0193],\n",
       "        [ 0.1769, -0.0137],\n",
       "        [ 0.1797, -0.0186],\n",
       "        [ 0.1820, -0.0155],\n",
       "        [ 0.1803, -0.0138],\n",
       "        [ 0.1813, -0.0164],\n",
       "        [ 0.1699, -0.0124],\n",
       "        [ 0.1863, -0.0180],\n",
       "        [ 0.1835, -0.0118],\n",
       "        [ 0.1832, -0.0149],\n",
       "        [ 0.1777, -0.0148],\n",
       "        [ 0.1844, -0.0144],\n",
       "        [ 0.1723, -0.0120],\n",
       "        [ 0.1764, -0.0115],\n",
       "        [ 0.1775, -0.0117],\n",
       "        [ 0.1815, -0.0141],\n",
       "        [ 0.1831, -0.0192],\n",
       "        [ 0.1838, -0.0144],\n",
       "        [ 0.1808, -0.0142],\n",
       "        [ 0.1800, -0.0151],\n",
       "        [ 0.1744, -0.0137],\n",
       "        [ 0.1829, -0.0149],\n",
       "        [ 0.1829, -0.0156],\n",
       "        [ 0.1799, -0.0149],\n",
       "        [ 0.1790, -0.0162],\n",
       "        [ 0.1760, -0.0121],\n",
       "        [ 0.1806, -0.0144],\n",
       "        [ 0.1884, -0.0207],\n",
       "        [ 0.1837, -0.0169],\n",
       "        [ 0.1841, -0.0180],\n",
       "        [ 0.1878, -0.0178],\n",
       "        [ 0.1782, -0.0112],\n",
       "        [ 0.1881, -0.0163],\n",
       "        [ 0.1762, -0.0126],\n",
       "        [ 0.1819, -0.0160],\n",
       "        [ 0.1727, -0.0174],\n",
       "        [ 0.1792, -0.0140]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "l = nn.Linear(embedding_dim, 2).to(device)\n",
    "logits = l(cbow)\n",
    "\n",
    "print(\"output shape : {}\".format(logits.size()))\n",
    "print(\"***** output *****\")\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put it all together and build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, class_num):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.cbow = CBOW()\n",
    "        self.linear = nn.Linear(embedding_dim, class_num)\n",
    "    def forward(self, tokens, masks):\n",
    "        embs = self.embedding(tokens)\n",
    "        output = self.cbow(embs, masks)\n",
    "        logits = self.linear(output)\n",
    "        return logits\n",
    "\n",
    "model = CBOWClassifier(vocab_size, embedding_dim, 2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to predict sentiment (0 - negative, 1 - positive) before training.<br>\n",
    "As you can see, the result is not correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>True</th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love this movie.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's so disappointed.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have recognized that a lot of people liked t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I agree with the reviewer who said this work i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I found it so turgid and poorly expressed by c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It helps you put into words what you want from...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I think the ending is illogical at least and i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  True  Pred\n",
       "0                                 I love this movie.     1     1\n",
       "1                              It's so disappointed.     0     1\n",
       "2  I have recognized that a lot of people liked t...     1     1\n",
       "3  I agree with the reviewer who said this work i...     0     1\n",
       "4  I found it so turgid and poorly expressed by c...     0     1\n",
       "5  It helps you put into words what you want from...     1     1\n",
       "6  I think the ending is illogical at least and i...     0     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text = [\n",
    "    [2,\"I love this movie.\"],\n",
    "    [1,\"It's so disappointed.\"],\n",
    "    [2,\"I have recognized that a lot of people liked this story.\"],\n",
    "    [1,\"I agree with the reviewer who said this work is boring.\"],\n",
    "    [1,\"I found it so turgid and poorly expressed by casts.\"],\n",
    "    [2,\"It helps you put into words what you want from main character.\"],\n",
    "    [1,\"I think the ending is illogical at least and is fiction.\"],\n",
    "]\n",
    "true_labels, tokens, masks = collate_batch(text)\n",
    "pred_logits = model(tokens, masks)\n",
    "pred_labels = pred_logits.argmax(dim=1)\n",
    "df = pd.DataFrame(\n",
    "    list(zip(\n",
    "        [t[1] for t in text],\n",
    "        true_labels.tolist(),\n",
    "        pred_labels.tolist())),\n",
    "    columns =[\"Text\", \"True\", \"Pred\"]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: 0.6533 - accuracy: 0.6250\n",
      "Epoch 2 - loss: 0.5507 - accuracy: 0.7750\n",
      "Epoch 3 - loss: 0.3829 - accuracy: 0.8750\n",
      "Epoch 4 - loss: 0.3148 - accuracy: 0.9250\n",
      "Epoch 5 - loss: 0.3395 - accuracy: 0.8500\n",
      "Epoch 6 - loss: 0.3428 - accuracy: 0.8000\n",
      "Epoch 7 - loss: 0.2202 - accuracy: 0.9250\n",
      "Epoch 8 - loss: 0.1837 - accuracy: 0.9500\n",
      "Epoch 9 - loss: 0.2222 - accuracy: 0.9250\n",
      "Epoch 10 - loss: 0.1923 - accuracy: 0.9000\n",
      "Epoch 11 - loss: 0.2358 - accuracy: 0.9250\n",
      "Epoch 12 - loss: 0.1134 - accuracy: 1.0000\n",
      "Epoch 13 - loss: 0.1036 - accuracy: 0.9750\n",
      "Epoch 14 - loss: 0.1339 - accuracy: 0.9500\n",
      "Epoch 15 - loss: 0.2219 - accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "for epoch in range(num_epochs):\n",
    "    for labels, tokens, masks in dataloader:\n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(tokens, masks)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate accuracy\n",
    "        pred_labels = logits.argmax(dim=1)\n",
    "        num_correct = (pred_labels == labels).float().sum()\n",
    "        accuracy = num_correct / len(labels)\n",
    "        print(\"Epoch {} - loss: {:2.4f} - accuracy: {:2.4f}\".format(epoch+1, loss.item(), accuracy), end=\"\\r\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training has completed, predict sample text again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>True</th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love this movie.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's so disappointed.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have recognized that a lot of people liked t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I agree with the reviewer who said this work i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I found it so turgid and poorly expressed by c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It helps you put into words what you want from...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I think the ending is illogical at least and i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  True  Pred\n",
       "0                                 I love this movie.     1     1\n",
       "1                              It's so disappointed.     0     0\n",
       "2  I have recognized that a lot of people liked t...     1     1\n",
       "3  I agree with the reviewer who said this work i...     0     0\n",
       "4  I found it so turgid and poorly expressed by c...     0     0\n",
       "5  It helps you put into words what you want from...     1     1\n",
       "6  I think the ending is illogical at least and i...     0     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = [\n",
    "    [2,\"I love this movie.\"],\n",
    "    [1,\"It's so disappointed.\"],\n",
    "    [2,\"I have recognized that a lot of people liked this story.\"],\n",
    "    [1,\"I agree with the reviewer who said this work is boring.\"],\n",
    "    [1,\"I found it so turgid and poorly expressed by casts.\"],\n",
    "    [2,\"It helps you put into words what you want from main character.\"],\n",
    "    [1,\"I think the ending is illogical at least and is fiction.\"],\n",
    "]\n",
    "true_labels, tokens, masks = collate_batch(text)\n",
    "pred_logits = model(tokens, masks)\n",
    "pred_labels = pred_logits.argmax(dim=1)\n",
    "df = pd.DataFrame(\n",
    "    list(zip(\n",
    "        [t[1] for t in text],\n",
    "        true_labels.tolist(),\n",
    "        pred_labels.tolist())),\n",
    "    columns =[\"Text\", \"True\", \"Pred\"]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embedding is a byproduct in this example.\n",
    "\n",
    "For instance, let's get top 10 words similar to the word \"```great```\" with trained embedding.<br>\n",
    "\n",
    "\n",
    "<mark>This embedding is trained to capture the tone for sentiment, and it won't then detect other contexts of similarity, such like, \"```dog```\" and \"```puppy```\".</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'different',\n",
       " 'gem',\n",
       " 'bravo',\n",
       " 'refreshing',\n",
       " '710',\n",
       " 'excellent',\n",
       " 'perfect',\n",
       " 'beauty',\n",
       " 'ward']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# create new embedding and restore weight from trained model\n",
    "e = model.embedding\n",
    "# get embedding vector for the word \"great\"\n",
    "token = stoi[\"great\"]\n",
    "# get embedded vector for \"great\"\n",
    "vec_t = e(token).tolist()\n",
    "# get vector list for all words (10,000 words)\n",
    "all_vec = [e(i).tolist() for i in range(vocab.__len__())]\n",
    "# get L2 distance in all words\n",
    "vec_t = np.array(vec_t)\n",
    "all_vec = np.array(all_vec)\n",
    "all_distance = np.array([np.dot(vec_t,v)/(norm(vec_t)*norm(v)) for v in all_vec])\n",
    "# get top 10 words similar to the word \"great\"\n",
    "indices_list = np.argsort(-all_distance)\n",
    "[itos[i] for i in indices_list[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
