{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data Preparation\n","\n","We have done some exploratory data analysis in the previous section and found out some patterns. We also cleaned the data to some extent. Lets go deeper into some of the more techniques.\n","\n","## Data Cleaning\n","\n","Most of the times, the data is damaged, or missing, we need to take care of it since Machine Learning models don't work when the data is missing or not a number. "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.impute import SimpleImputer\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["### Imputing missing values"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Country</th>\n","      <th>Age</th>\n","      <th>Salary</th>\n","      <th>Purchased</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>France</td>\n","      <td>44.0</td>\n","      <td>72000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Spain</td>\n","      <td>27.0</td>\n","      <td>48000.0</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Germany</td>\n","      <td>30.0</td>\n","      <td>54000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Spain</td>\n","      <td>38.0</td>\n","      <td>61000.0</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Germany</td>\n","      <td>40.0</td>\n","      <td>NaN</td>\n","      <td>Yes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Country   Age   Salary Purchased\n","0   France  44.0  72000.0        No\n","1    Spain  27.0  48000.0       Yes\n","2  Germany  30.0  54000.0        No\n","3    Spain  38.0  61000.0        No\n","4  Germany  40.0      NaN       Yes"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('Data.csv')\n","df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Country</th>\n","      <th>Age</th>\n","      <th>Salary</th>\n","      <th>Purchased</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>France</td>\n","      <td>44.0</td>\n","      <td>72000.000000</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Spain</td>\n","      <td>27.0</td>\n","      <td>48000.000000</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Germany</td>\n","      <td>30.0</td>\n","      <td>54000.000000</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Spain</td>\n","      <td>38.0</td>\n","      <td>61000.000000</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Germany</td>\n","      <td>40.0</td>\n","      <td>63777.777778</td>\n","      <td>Yes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Country   Age        Salary Purchased\n","0   France  44.0  72000.000000        No\n","1    Spain  27.0  48000.000000       Yes\n","2  Germany  30.0  54000.000000        No\n","3    Spain  38.0  61000.000000        No\n","4  Germany  40.0  63777.777778       Yes"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# replace every occurrence of missing_values to one defined by strategy\n","# which can be mean, median, mode.\n","\n","imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","df.iloc[:, 1:3] = imputer.fit_transform(df.iloc[:, 1:3])\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Encoding categorical data  \n","\n","Our dataset has numerical and categorical features. We need to convert categorical features to numerical. For that we can use `LabelEncoder` or `OneHotEncoder`. \n","\n","One hot encoding is a popular technique to convert categorical variables to numerical. It creates a separate column for every variable and gives a value of 1 where the variable is present otherwise 0. \n","\n","Label encoding is another popular technique to convert categorical variables to numerical. It replaces every categorical variable with a number. "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Label Encoder will replace every categorical variable with number. Useful for replacing yes by 1, no by 0.\n","# One Hot Encoder will create a separate column for every variable and give a value of 1 where the variable is present\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Country</th>\n","      <th>Age</th>\n","      <th>Salary</th>\n","      <th>Purchased</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>44.0</td>\n","      <td>72000.000000</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>27.0</td>\n","      <td>48000.000000</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>30.0</td>\n","      <td>54000.000000</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>38.0</td>\n","      <td>61000.000000</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>40.0</td>\n","      <td>63777.777778</td>\n","      <td>Yes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Country   Age        Salary Purchased\n","0       0  44.0  72000.000000        No\n","1       2  27.0  48000.000000       Yes\n","2       1  30.0  54000.000000        No\n","3       2  38.0  61000.000000        No\n","4       1  40.0  63777.777778       Yes"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["lable_encoder = LabelEncoder()\n","temp = df.copy()\n","temp.iloc[:, 0] = lable_encoder.fit_transform(df.iloc[:, 0])\n","temp.head()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Age</th>\n","      <th>Salary</th>\n","      <th>Country_France</th>\n","      <th>Country_Germany</th>\n","      <th>Country_Spain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>44.000000</td>\n","      <td>72000.000000</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>27.000000</td>\n","      <td>48000.000000</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30.000000</td>\n","      <td>54000.000000</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>38.000000</td>\n","      <td>61000.000000</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>40.000000</td>\n","      <td>63777.777778</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>35.000000</td>\n","      <td>58000.000000</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>38.777778</td>\n","      <td>52000.000000</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>48.000000</td>\n","      <td>79000.000000</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>50.000000</td>\n","      <td>83000.000000</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>37.000000</td>\n","      <td>67000.000000</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Age        Salary  Country_France  Country_Germany  Country_Spain\n","0  44.000000  72000.000000            True            False          False\n","1  27.000000  48000.000000           False            False           True\n","2  30.000000  54000.000000           False             True          False\n","3  38.000000  61000.000000           False            False           True\n","4  40.000000  63777.777778           False             True          False\n","5  35.000000  58000.000000            True            False          False\n","6  38.777778  52000.000000           False            False           True\n","7  48.000000  79000.000000            True            False          False\n","8  50.000000  83000.000000           False             True          False\n","9  37.000000  67000.000000            True            False          False"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# you can pass an array of indices of categorical features\n","# one_hot_encoder = OneHotEncoder(categorical_features=[0])\n","# temp = df.copy()\n","# temp.iloc[:, 0] = one_hot_encoder.fit_transform(df.iloc[:, 0])\n","\n","# you can achieve the same thing using get_dummies\n","pd.get_dummies(df.iloc[:, :-1])"]},{"cell_type":"markdown","metadata":{},"source":["### Binarizing\n","\n","Often we need to do the reverse of what we've done above. That is, convert continuous features to discrete values. For instance, we want to convert the output to 0 or 1 depending on the threshold. "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from sklearn.datasets import load_iris\n","\n","iris_dataset = load_iris()\n","X = iris_dataset.data\n","y = iris_dataset.target\n","feature_names = iris_dataset.feature_names\n"]},{"cell_type":"markdown","metadata":{},"source":["Now we'll binarize the sepal width with 0 or 1 indicating whether the current value is below or above mean. "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["array([3.5, 3. , 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3. ,\n","       3. , 4. , 4.4, 3.9, 3.5, 3.8, 3.8, 3.4, 3.7, 3.6, 3.3, 3.4, 3. ,\n","       3.4, 3.5, 3.4, 3.2, 3.1, 3.4, 4.1, 4.2, 3.1, 3.2, 3.5, 3.6, 3. ,\n","       3.4, 3.5, 2.3, 3.2, 3.5, 3.8, 3. , 3.8, 3.2, 3.7, 3.3, 3.2, 3.2,\n","       3.1, 2.3, 2.8, 2.8, 3.3, 2.4, 2.9, 2.7, 2. , 3. , 2.2, 2.9, 2.9,\n","       3.1, 3. , 2.7, 2.2, 2.5, 3.2, 2.8, 2.5, 2.8, 2.9, 3. , 2.8, 3. ,\n","       2.9, 2.6, 2.4, 2.4, 2.7, 2.7, 3. , 3.4, 3.1, 2.3, 3. , 2.5, 2.6,\n","       3. , 2.6, 2.3, 2.7, 3. , 2.9, 2.9, 2.5, 2.8, 3.3, 2.7, 3. , 2.9,\n","       3. , 3. , 2.5, 2.9, 2.5, 3.6, 3.2, 2.7, 3. , 2.5, 2.8, 3.2, 3. ,\n","       3.8, 2.6, 2.2, 3.2, 2.8, 2.8, 2.7, 3.3, 3.2, 2.8, 3. , 2.8, 3. ,\n","       2.8, 3.8, 2.8, 2.8, 2.6, 3. , 3.4, 3.1, 3. , 3.1, 3.1, 3.1, 2.7,\n","       3.2, 3.3, 3. , 2.5, 3. , 3.4, 3. ])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["X[:, 1]"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["array([1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n","       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n","       1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n","       1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n","       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n","       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n","       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import Binarizer\n","X[:, 1:2] = Binarizer(threshold=X[:, 1].mean()).fit_transform(X[:, 1].reshape(-1, 1))\n","X[:, 1]"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Scaling\n","\n","Because in Machine Learning models, features are mapped into n-dimensional space. So let's say there are two variables (x, y) which will be mapped in 2D co-ordinate system. If one variable, say y, is very huge and other, x, is very small, then the euclidean distance will be dominated by the bigger one and smaller one will be ignored. In this case we are losing valuable information, hence feature scaling is used to solve this problem. \n","\n","\n","Additional reasons for transformation:\n","\n","1. To more closely approximate a theoretical distribution that has nice statistical properties. \n","2. To spread out data more evenly.\n","3. To make data distribution more symmetric\n","4. To make relationships between variables more linear. \n","5. To make data more constant in variance (homoscedasticity). \n","\n","#### There are 3 most used ways to scale features. \n","1. __Min Max Scaling__: \n","Will scale the input to have minimum of 0 and maximum of 1. That is, it scales the data in the range of [0, 1] This is useful when the parameters have to be on same positive scale. But in this case, the outliers are lost. \n","$$X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n","\n","2. __Standardization__:\n","Will scale the input to have mean of 0 and variance of 1. \n","$$X_{stand} = \\frac{X - \\mu}{\\sigma}$$\n","\n","3. __Normalizing__: \n","Will scale the input to make the norm of 1. For instance, for 3D data the 3 independent variables will lie on a unit Sphere. \n","\n","4. __Log Transformation__:\n","Taking the log of data after any of above transformation. \n","\n","Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.\n","\n","For most applications, Standardization is recommended. Min Max Scaling is recommended for Neural Networks. Normalizing is recommended when Clustering eg. KMeans. "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["   Country   Age   Salary Purchased\n","0   France  44.0  72000.0        No\n","1    Spain  27.0  48000.0       Yes\n","2  Germany  30.0  54000.0        No\n","3    Spain  38.0  61000.0        No\n","5   France  35.0  58000.0       Yes\n","7   France  48.0  79000.0       Yes\n","8  Germany  50.0  83000.0        No\n","9   France  37.0  67000.0       Yes\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n","\n","df = pd.read_csv('Data.csv').dropna()\n","print(df)\n","X = df[[\"Age\", \"Salary\"]].values.astype(np.float64)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Standardization\n","[[ 0.69985807  0.58989097]\n"," [-1.51364653 -1.50749915]\n"," [-1.12302807 -0.98315162]\n"," [-0.08137885 -0.37141284]\n"," [-0.47199731 -0.6335866 ]\n"," [ 1.22068269  1.20162976]\n"," [ 1.48109499  1.55119478]\n"," [-0.211585    0.1529347 ]]\n","Normalizing\n","[[6.11110997e-04 9.99999813e-01]\n"," [5.62499911e-04 9.99999842e-01]\n"," [5.55555470e-04 9.99999846e-01]\n"," [6.22950699e-04 9.99999806e-01]\n"," [6.03448166e-04 9.99999818e-01]\n"," [6.07594825e-04 9.99999815e-01]\n"," [6.02409529e-04 9.99999819e-01]\n"," [5.52238722e-04 9.99999848e-01]]\n","MinMax Scaling\n","[[0.73913043 0.68571429]\n"," [0.         0.        ]\n"," [0.13043478 0.17142857]\n"," [0.47826087 0.37142857]\n"," [0.34782609 0.28571429]\n"," [0.91304348 0.88571429]\n"," [1.         1.        ]\n"," [0.43478261 0.54285714]]\n"]}],"source":["standard_scaler = StandardScaler()\n","normalizer = Normalizer()\n","min_max_scaler = MinMaxScaler()\n","\n","print(\"Standardization\")\n","print(standard_scaler.fit_transform(X))\n","\n","print(\"Normalizing\")\n","print(normalizer.fit_transform(X))\n","\n","print(\"MinMax Scaling\")\n","print(min_max_scaler.fit_transform(X))"]},{"cell_type":"markdown","metadata":{},"source":["## Feature extraction\n","\n","Let's explore some of the feature extraction techniques. Feature extraction is the process of transforming the data into a format that can be used for machine learning. It is a way to extract features from the data. \n","\n","### CountVectorizer\n","\n","CountVectorizer converts a bunch of documents to vector so that we can use it with models. It basically just counts the number of times a particular word has occured. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 0 1 1 1 0 0 1 0 0 0]\n"," [0 0 0 0 1 0 0 0 0 1 1]\n"," [1 1 0 1 1 1 1 0 1 0 0]]\n","{u'and': 1, u'boy': 2, u'name': 6, u'is': 3, u'mayur': 4, u'am': 0, u'wohooo': 10, u'rock': 9, u'nice': 7, u'my': 5, u'pythonista': 8}\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","docs = [\"Mayur is a nice boy.\", \"Mayur rock! wohooo!\", \"My name is Mayur, and I am a Pythonista!\"]\n","cv = CountVectorizer()\n","X = cv.fit_transform(docs)\n","print(X.todense())\n","print(cv.vocabulary_)"]},{"cell_type":"markdown","metadata":{},"source":["### DictVectorizer\n","\n","DictVectorizer will convert mappings to vectors. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.  1.  0.  2.  0.  0.  0.  1.  0.  0.]\n"," [ 1.  0.  1.  0.  2.  1.  2.  0.  3.  3.]]\n"]}],"source":["from sklearn.feature_extraction import DictVectorizer\n","\n","docs = [{\"Mayur\": 1, \"is\": 1, \"awesome\": 2}, {\"No\": 1, \"I\": 1, \"dont\": 2, \"wanna\": 3, \"fall\": 1, \"in\": 2, \"love\": 3}]\n","dv = DictVectorizer()\n","X = dv.fit_transform(docs)\n","print(X.todense())"]},{"cell_type":"markdown","metadata":{},"source":["### TfidfVectorizer\n","\n","In many text analytics applications, we need to convert the text into vectors to use with Machine Learning algorithms. This is known as the Vector Space Model. \n","\n","While CountVectorizer could be a solution, words like \"the\", \"a\", \"in\" etc. are common words and often are used in all kinds of documents. Using CountVectorizer gives more emphasis on such word counts which are not relevant. \n","\n","You could circumvent this problems by using `stop_words=\"english\"` which would filter out common words but let's say you have a different vocabulary, for instance a conversation between 2 Computer Science students would have words like \"RAM\", \"processor\", \"GPU\" mentioned too often and you'd have to manually add the stop words everytime for all the problems you solve. \n","\n","Thus in such scenarios, it is recommended to use `TfidfVectorizer` which will take care of such things. Every word is given a number according to the following formula:\n","\n","$$ \\text{tfidf }\\left(\\text{word}\\right)=\\text{tf}\\left(\\text{word},\\text{document}_i\\right)\\cdot\\text{idf}\\left(\\text{word}\\right) $$\n","\n","Where, \n","1. tf(word, document_i) = Term Frequency of a word in the specific document i.\n","2. idf(word) = Inverse Document Frequency of the word. \n","\n","Inverse Document Frequency is defined as the log of ratio of number of documents to the number of times the word has occured in the any document. \n","\n","$$ \\text{idf }\\left(w\\right)=\\log\\left(\\frac{n_d}{df\\left(w\\right)}\\right)$$\n","\n","Where, \n","1. df(w) = number of times the word has occured in the any document. \n","\n","What is does intuitively is if a word has occured too many times in other document as well (common words like \"the\", \"is\") then it gives lesser weightage to such words in contrast to words that have occured more number of times in a single document in contrast to others. Which basically means that if a particular word occurs more number of times in a single document only, then it might be an important feature. \n","\n","Note that numerator and denominator are added with `1` to avoid underflow eg. when the document frequency is 0. \n","\n","Sklearn additionally also normalizes the output of tfidf to have a norm of 1. This is important since we're interested in similarities hence vectors like (1, 1) and (3, 3) are really the same (they go in same direction, just have different weights) which is achieved by dividing by the length of the vector.\n","\n","$$v_i=\\frac{v_i}{\\left|v\\right|_2}=\\frac{v_i}{\\sqrt{v_1^2+v_2^2+v_3^2+....+v_n^2}}$$"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.         0.76749457 0.45329466 0.45329466 0.         0.        ]\n"," [0.         0.         0.45329466 0.45329466 0.76749457 0.        ]\n"," [0.6088451  0.         0.35959372 0.35959372 0.         0.6088451 ]]\n","{'mayur': 3, 'is': 2, 'guitarist': 1, 'musician': 4, 'also': 0, 'programmer': 5}\n","[[0 1 1 1 0 0]\n"," [0 0 1 1 1 0]\n"," [1 0 1 1 0 1]]\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","cv_vectorizer = CountVectorizer()\n","docs = [\"Mayur is a Guitarist\", \"Mayur is Musician\", \"Mayur is also a programmer\"]\n","X_idf = tfidf_vectorizer.fit_transform(docs)\n","X_cv = cv_vectorizer.fit_transform(docs)\n","print(X_idf.todense())\n","print(tfidf_vectorizer.vocabulary_)\n","print(X_cv.todense())\n"]},{"cell_type":"markdown","metadata":{},"source":["We can see the \"Mayur\" and \"is\" are given less weightage than \"guitarist\", \"musician\", \"programmer\""]}],"metadata":{"kernelspec":{"display_name":"ml-notes","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":2}
