{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "CNNs are Neural Networks that are used to classify images. They do so by using `filters` and `convolutions`. `filters` are the weights in CNNs. They're a column vector in NNs(e.g. hidden layer with 3 nodes), and a matrix in CNNs (3x3 filter). One more way how they differ from ordinary NNs is they use the idea of shared weights. \n",
    "\n",
    "### Shared Weights\n",
    "CNNs achieve translational invarince using shared weights. The basic idea is: if a filter detects a horizontal line, then it is intuitive for it to detect the line anywhere in the image irrespective of the location. Hence, there is no need to learn how to detect a line at a different location again and again. Also: __it enormously decreases the number of parameters to learn__. In a normal NN, it would have one weight for every pixel in the image which are too many hyper parameters.  \n",
    "\n",
    "Steps:\n",
    "1. Convolution\n",
    "2. Max Pooling\n",
    "3. ReLU\n",
    "4. Flattening\n",
    "5. Full Connection\n",
    "\n",
    "\n",
    "### Convolution layer\n",
    "For an image, we create a filter/kernel (from the same image) that performs element wise multiplication (convolution). Hence using this we can detect edges, and various other information about the image. The convolution matrix is initially initialised with random zero-centered numbers. Later it will automatically learn to figure out various aspects of the image. \n",
    "\n",
    "Important thing to remember: depth of filter in current layer equals the number of channels in previous layer. \n",
    "\n",
    "### Interpreting Convolutions\n",
    "Initial convolution layers learn basic things like horizontal lines, vertical lines, small shapes. And as the layers go on increasing they learn more and more high level features. Like a facial recognition model will learn basic lines in the initial layers, then learn nose, eyes, etc in the next layers, then faces in the final layers. \n",
    "\n",
    "### Filters\n",
    "Filters are the weights in CNNs. These filters have depth, another hyperparameter. One filter is a 2D array which can be interpreted as something that learn a shape. We can create an array of such filters thus adding a depth to it. Each filter learns a different element: one might learn horizontal lines, one might learn a basic circle shape, etc. \n",
    "\n",
    "\n",
    "### Padding\n",
    "In padding we add an extra layer of 0s accross the dimension so that adding multiple convolutions won't shrink the dimensions quickly. \n",
    "\n",
    "\n",
    "### Output Dimensions\n",
    "The output dimensions after padding are:\n",
    "\n",
    "$$W_0 = \\frac{W_i - F + 2P}{2}$$\n",
    "\n",
    "\n",
    "$$H_0 = \\frac{H_i - F + 2P}{2}$$\n",
    "\n",
    "Where, \n",
    "W_i is the input width, H_i is input height, F is filter size (it's symmetric), and P is the padding\n",
    "\n",
    "### Structuring (selecting filter size and depth)\n",
    "It is suggested that the filter size should be bigger in the initial layers, and depth should be smaller. For subsequent layers, you should be reducing the filter size and increasing the depth. That's because the final layers are high level representations so increase the depth in the final layers means more high level features will be learned. The size should generally be a multiple of 2. \n",
    "\n",
    "### Pooling\n",
    "Pooling is used for following reasons:\n",
    "1. Translational Invariance: we don't care where the face is for a facial classifier. \n",
    "2. Reduction in number of parameters, while respecting the spatial ascept. \n",
    "3. Reduce overfitting. \n",
    "\n",
    "There are couple of ways of pooling: mean, max. Max pooling is most commonly used with 2x2 ksize and 2 stride. There are also overlapping ksizes in pooling. \n",
    "\n",
    "Recently, pooling is not much used because:\n",
    "1. Datasets are so diverse, we're more concerned about underfitting, than overfitting. \n",
    "2. Dropout is a much better regularizer. \n",
    "3. Downsampling image results in information loss. \n",
    "\n",
    "### ReLU (Activation Function)\n",
    "This operation add non-linearity in the network. It is a computationally efficient activation function and has many advantages over other. \n",
    "\n",
    "### Flattening\n",
    "This operation simply flattens the matrix, that is, converts the matrix into single column by appending all the rows below one another. \n",
    "\n",
    "### Fully connected layer\n",
    "Once we have the flattened input, imagine this step as creating a neural network with hidden layers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# output depth\n",
    "k_output = 64\n",
    "\n",
    "# Image properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 3\n",
    "\n",
    "# convolutional filter\n",
    "filter_width = 5\n",
    "filter_height = 5\n",
    "\n",
    "# input image (batch_size is dynamic)\n",
    "input_image = torch.randn((1, color_channels, image_height, image_width))  # PyTorch uses (batch_size, channels, height, width)\n",
    "\n",
    "# Define the convolution layer (in_channels, out_channels, kernel_size)\n",
    "conv_layer = nn.Conv2d(in_channels=color_channels, \n",
    "                       out_channels=k_output, \n",
    "                       kernel_size=(filter_height, filter_width), \n",
    "                       stride=(2, 2), \n",
    "                       padding=2)  # \"SAME\" padding equivalent in PyTorch\n",
    "\n",
    "# Apply convolution\n",
    "conv = conv_layer(input_image)\n",
    "\n",
    "# Define and apply bias (conv_layer already has bias by default)\n",
    "bias = conv_layer.bias\n",
    "conv = conv + bias.view(1, -1, 1, 1)  # Add bias along the output channels\n",
    "\n",
    "# Apply max pooling (kernel_size, stride, padding)\n",
    "conv = F.max_pool2d(conv, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "# Apply ReLU activation\n",
    "conv = F.relu(conv)\n",
    "\n",
    "print(conv.shape)  # Check the output shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classifier in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        # 1. Convolution\n",
    "        # (in_channels, out_channels, kernel_size, stride)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=3)\n",
    "        \n",
    "        # 2. Max Pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 3. Flattening is done in forward pass with view()\n",
    "        \n",
    "        # 4. Fully connected layer (Flattened size needs to be calculated based on input size)\n",
    "        self.fc1 = nn.Linear(32 * 10 * 10, 128)  # Adjust 32 * 10 * 10 based on input size post pooling\n",
    "        \n",
    "        # 5. Output layer\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution + ReLU activation\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        # Apply max pooling\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = x.view(-1, 32 * 10 * 10)  # Flatten to (batch_size, num_features)\n",
    "        \n",
    "        # Fully connected layer + ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Output layer with sigmoid activation\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = Classifier()\n",
    "\n",
    "# Compile the model: use optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy for binary classification\n",
    "\n",
    "# Example input (batch_size=1, channels=3, height=64, width=64)\n",
    "input_data = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# 1. Data Augmentation for Training Set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),         # Resize to match the input size expected by the model\n",
    "    transforms.RandomHorizontalFlip(),   # Horizontal flipping\n",
    "    transforms.RandomResizedCrop(64),    # Random cropping\n",
    "    transforms.RandomAffine(degrees=0, shear=0.2),  # Shearing\n",
    "    transforms.RandomZoom(0.2),          # Random Zooming (may require custom transform or adjusted RandomResizedCrop)\n",
    "    transforms.ToTensor(),               # Convert the image to a tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalization (mean, std for each channel)\n",
    "])\n",
    "\n",
    "# 2. Data Preprocessing for Test Set (only rescaling)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = ImageFolder(root='dataset/training_set', transform=train_transform)\n",
    "test_dataset = ImageFolder(root='dataset/test_set', transform=test_transform)\n",
    "\n",
    "# Data Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Assuming you have a model defined as 'model'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.BCELoss()  # or CrossEntropyLoss for multi-class classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 25\n",
    "steps_per_epoch = len(train_loader)\n",
    "validation_steps = len(test_loader)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/steps_per_epoch:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {validation_loss/validation_steps:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
