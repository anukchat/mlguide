{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569bc2c9",
   "metadata": {},
   "source": [
    "# Question Answering\n",
    "\n",
    "Question answering (QA) using Large Language Models (LLMs) involves leveraging these models to interpret and generate natural language responses to user queries based on vast amounts of pre-trained knowledge. \n",
    "\n",
    "LLMs, like GPT-4, have been trained on diverse datasets and are capable of understanding context, nuances, and relationships in language, enabling them to provide answers on a wide range of topics.\n",
    "\n",
    "**Closed-Domain QA:** Focused on a specific topic or domain (e.g., medical, legal) with answers extracted from a defined set of documents or a knowledge base.\n",
    "\n",
    "**Open-Domain QA:** General questions where the model retrieves information from its entire training data, offering a broad, context-aware response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b715375",
   "metadata": {},
   "source": [
    "## Top-K Similarity Search - Ask A Book A Question\n",
    "\n",
    "In this tutorial we will see a simple example of basic retrieval via Top-K Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d615a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain --upgrade\n",
    "# Version: 0.0.164\n",
    "\n",
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ee985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip data folder\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('../../data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3e92ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF Loaders. If unstructured gives you a hard time, try PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166d759",
   "metadata": {},
   "source": [
    "### Load your data\n",
    "\n",
    "Next let's load up some data. I've put a few 'loaders' on there which will load data from different locations. Feel free to use the one that suits you. The default one queries one of Paul Graham's essays for a simple example. This process will only stage the loader, not actually load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a2d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(file_path=\"../data/PaulGrahamEssays/vb.txt\")\n",
    "\n",
    "## Other options for loaders \n",
    "# loader = PyPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
    "# loader = UnstructuredPDFLoader(\"../data/field-guide-to-data-science.pdf\")\n",
    "# loader = OnlinePDFLoader(\"https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d38044",
   "metadata": {},
   "source": [
    "Then let's go ahead and actually load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcdac23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a744a",
   "metadata": {},
   "source": [
    "Then let's actually check out what's been loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4fd7c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document(s) in your data\n",
      "There are 9155 characters in your sample document\n",
      "Here is a sample: January 2016Life is short, as everyone knows. When I was a kid I used to wonder\n",
      "about this. Is life actually short, or are we really complaining\n",
      "about its finiteness?  Would we be just as likely to fe\n"
     ]
    }
   ],
   "source": [
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your sample document')\n",
    "print (f'Here is a sample: {data[0].page_content[:200]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9b604",
   "metadata": {},
   "source": [
    "### Chunk your data up into smaller documents\n",
    "\n",
    "While we could pass the entire essay to a model w/ long context, we want to be picky about which information we share with our model. The better signal to noise ratio we have the more likely we are to get the right answer.\n",
    "\n",
    "The first thing we'll do is chunk up our document into smaller pieces. The goal will be to take only a few of those smaller pieces and pass them to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3c6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll split our data into chunks around 500 characters each with a 50 character overlap. These are relatively small.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "879873a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 20 documents\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many small chunks we have\n",
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b2843",
   "metadata": {},
   "source": [
    "### Create embeddings of your documents to get ready for semantic search\n",
    "\n",
    "Next up we need to prepare for similarity searches. The way we do this is through embedding our documents (getting a vector per document).\n",
    "\n",
    "This will help us compare documents later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373e695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e7857",
   "metadata": {},
   "source": [
    "Check to see if there is an environment variable with you API keys, if not, use what you put below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a1d5c3",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'YourAPIKey')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205993a",
   "metadata": {},
   "source": [
    "Then we'll get our embeddings engine going. You can use whatever embeddings engine you would like. We'll use OpenAI's ada today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4619d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d66c06",
   "metadata": {},
   "source": [
    "### Option #1: Chroma (for local)\n",
    "\n",
    "I like Chroma becauase it's local and easy to set up without an account.\n",
    "\n",
    "First we'll pass our texts to Chroma via `.from_documents`, this will 1) embed the documents and get a vector, then 2) add them to the vectorstore for retrieval later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e0d1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "vectorstore = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4750ab",
   "metadata": {},
   "source": [
    "Let's test it out. I want to see which documents are most closely related to a query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34929595",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is great about having kids?\"\n",
    "docs = vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec60de1",
   "metadata": {},
   "source": [
    "Then we can check them out. In theory, the texts which are deemed most similar should hold the answer to our question.\n",
    "But keep in mind that our query just happens to be a question, it could be a random statement or sentence and it would still work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e0f5b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jabs into your consciousness like a pin.The things that matter aren't necessarily the ones people would\n",
      "call \"important.\"  Having coffee with a friend matters.  You won't\n",
      "feel later like that was a waste of time.One great thing about having small children is that they make you\n",
      "spend time on things that matter: them. They grab your sleeve as\n",
      "you're staring at your phone and say \"will you play with me?\" And\n",
      "\n",
      "the question, and the answer is that life actually is short.Having kids showed me how to convert a continuous quantity, time,\n",
      "into discrete quantities. You only get 52 weekends with your 2 year\n",
      "old.  If Christmas-as-magic lasts from say ages 3 to 10, you only\n",
      "get to watch your child experience it 8 times.  And while it's\n",
      "impossible to say what is a lot or a little of a continuous quantity\n",
      "like time, 8 is not a lot of something.  If you had a handful of 8\n",
      "\n",
      "January 2016Life is short, as everyone knows. When I was a kid I used to wonder\n",
      "about this. Is life actually short, or are we really complaining\n",
      "about its finiteness?  Would we be just as likely to feel life was\n",
      "short if we lived 10 times as long?Since there didn't seem any way to answer this question, I stopped\n",
      "wondering about it.  Then I had kids.  That gave me a way to answer\n",
      "\n",
      "done that we didn't.  My oldest son will be 7 soon.  And while I\n",
      "miss the 3 year old version of him, I at least don't have any regrets\n",
      "over what might have been.  We had the best time a daddy and a 3\n",
      "year old ever had.Relentlessly prune bullshit, don't wait to do things that matter,\n",
      "and savor the time you have.  That's what you do when life is short.Notes[1]\n",
      "At first I didn't like it that the word that came to mind was\n",
      "one that had other meanings.  But then I realized the other meanings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here's an example of the first document that was returned\n",
    "for doc in docs:\n",
    "    print (f\"{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d8504",
   "metadata": {},
   "source": [
    "### Option #2: Pinecone (for cloud)\n",
    "If you want to use pinecone, run the code below, if not then skip over to Chroma below it. You must go to [Pinecone.io](https://www.pinecone.io/) and set up an account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e093ef3",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# PINECONE_API_KEY = os.getenv('PINECONE_API_KEY', 'YourAPIKey')\n",
    "# PINECONE_API_ENV = os.getenv('PINECONE_API_ENV', 'us-east1-gcp') # You may need to switch with your env\n",
    "\n",
    "# # initialize pinecone\n",
    "# pinecone.init(\n",
    "#     api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "#     environment=PINECONE_API_ENV  # next to api key in console\n",
    "# )\n",
    "# index_name = \"langchaintest\" # put in the name of your pinecone index here\n",
    "\n",
    "# docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35dcd9",
   "metadata": {},
   "source": [
    "### Query those docs to get your answer back\n",
    "\n",
    "Great, those are just the docs which should hold our answer. Now we can pass those to a LangChain chain to query the LLM.\n",
    "\n",
    "We could do this manually, but a chain is a convenient helper for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f051337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b9b1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67ea7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is great about having kids?\"\n",
    "docs = vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dfd2b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One great thing about having kids is that they make you spend time on things that matter. They remind you to prioritize the important things in life, like spending quality time with them. Having kids can also bring a sense of joy and fulfillment as you watch them grow and experience new things.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23369bf3",
   "metadata": {},
   "source": [
    "## Advanced Retrieval With LangChain\n",
    "\n",
    "Let's go over a few more complex and advanced retrieval methods with LangChain.\n",
    "\n",
    "There is no one right way to retrieve data - it'll depend on your application so take some time to think about it before you jump in\n",
    "\n",
    "Let's have some fun\n",
    "\n",
    "* **Multi Query** - Given a single user query, use an LLM to synthetically generate multiple other queries. Use each one of the new queries to retrieve documents, take the union of those documents for the final context of your prompt\n",
    "* **Contextual Compression** - Fluff remover. Normal retrieval but with an extra step of pulling out relevant information from each returned document. This makes each relevant document smaller for your final prompt (which increases information density)\n",
    "* **Parent Document Retriever** - Split and embed *small* chunks (for maximum information density), then return the parent documents (or larger chunks) those small chunks come from\n",
    "* **Ensemble Retriever** - Combine multiple retrievers together\n",
    "* **Self-Query** - When the retriever infers filters from a users query and applies those filters to the underlying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip data folder\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('../../data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f0ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key=os.getenv('OPENAI_API_KEY', 'YourAPIKey')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712489d",
   "metadata": {},
   "source": [
    "## Load up our texts and documents\n",
    "\n",
    "Then chunk them, and put them into a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365dda30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.7.2) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9af3c",
   "metadata": {},
   "source": [
    "We're going to load up Paul Graham's essays. In this repo there are various sizes of folders (`PaulGrahamEssaysSmall`, `PaulGrahamEssaysMedium`, `PaulGrahamEssaysLarge` or `PaulGrahamEssays` for the full set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c31d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 49/49 [00:30<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader('../data/PaulGrahamEssaysLarge/', glob=\"**/*.txt\", show_progress=True)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d9eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 49 essays loaded\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(docs)} essays loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872a11c",
   "metadata": {},
   "source": [
    "Then we'll split up our text into smaller sized chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3598bfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your 49 documents have been split into 471 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print (f\"Your {len(docs)} documents have been split into {len(splits)} chunks\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8054d56",
   "metadata": {},
   "source": [
    "If you do `Chroma.from_documents` multiple times you'll re-add the documents (and duplicate them) which is annoying. I check to see if we've already made our vectordb, if so delete what's in there, then go and make it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'vectordb' in globals(): # If you've already made your vectordb this will delete it so you start fresh\n",
    "    vectordb.delete_collection()\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a832b",
   "metadata": {},
   "source": [
    "### MultiQuery\n",
    "\n",
    "This retrieval method will generated 3 additional questions to get a total of 4 queries (with the users included) that will be used to go retrieve documents. This is helpful when you want to retrieve documents which are similar in meaning to your question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Set logging for the queries\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6689d",
   "metadata": {},
   "source": [
    "Doing some logging to see the other questions that were generated. I tried to find a way to get these via a model property but couldn't, lmk if you find a way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d95f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0621a",
   "metadata": {},
   "source": [
    "Then we set up the MultiQueryRetriever which will generate other questions for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the authors view on the early stages of a startup?\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a878504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How does the author perceive the early stages of a startup?', \"2. What are the author's thoughts on the initial phases of a startup?\", \"3. What is the author's perspective on the beginning stages of a startup?\"]\n"
     ]
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fef7aa",
   "metadata": {},
   "source": [
    "Check out how there are other questions which are related to but slightly different than the question I asked.\n",
    "\n",
    "Let's see how many docs were actually returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7160777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e89969",
   "metadata": {},
   "source": [
    "Ok now let's put those docs into a prompt template which we'll use as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5223beeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The author believes that it is important for startups to release an early version of their product quickly and then improve it based on user feedback. They emphasize the importance of getting version 1 done fast and state that startups that are too slow to release often fail.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm.predict(text=PROMPT.format_prompt(\n",
    "    context=unique_docs,\n",
    "    question=question\n",
    ").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d4a6a4",
   "metadata": {},
   "source": [
    "### Contextual Compression\n",
    "\n",
    "Then we'll move onto contextual compression. This will take the chunk that you've made (above) and compress it's information down to the parts relevant to your query.\n",
    "\n",
    "Say that you have a chunk that has 3 topics within it, you only really care about one of them though, this compressor will look at your query, see that you only need one of the 3 topics, then extract & return that one topic.\n",
    "\n",
    "This one is a bit more expensive because each doc returned will get processed an additional time (to pull out the relevant data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3602d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3b58a",
   "metadata": {},
   "source": [
    "We first need to set up our compressor, it's cool that it's a separate object because that means you can use it elsewhere outside this retriever as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model='gpt-4')\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor,\n",
    "                                                       base_retriever=vectordb.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c620556",
   "metadata": {},
   "source": [
    "First, an example of compression. Below we have one of our splits that we made above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cfa62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"July 2006I've discovered a handy test for figuring out what you're addicted\\n\\nto.  Imagine you were going to spend the weekend at a friend's house\\n\\non a little island off the coast of Maine.  There are no shops on\\n\\nthe island and you won't be able to leave while you're there.  Also,\\n\\nyou've never been to this house before, so you can't assume it will\\n\\nhave more than any house might.What, besides clothes and toiletries, do you make a point of packing?\\n\\nThat's what you're addicted to.  For example, if you find yourself\\n\\npacking a bottle of vodka (just in case), you may want to stop and\\n\\nthink about that.For me the list is four things: books, earplugs, a notebook, and a\\n\\npen.There are other things I might bring if I thought of it, like music,\\n\\nor tea, but I can live without them.  I'm not so addicted to caffeine\\n\\nthat I wouldn't risk the house not having any tea, just for a\\n\\nweekend.Quiet is another matter.  I realize it seems a bit eccentric to\\n\\ntake earplugs on a trip to an island off the coast of Maine.  If\\n\\nanywhere should be quiet, that should.  But what if the person in\\n\\nthe next room snored?  What if there was a kid playing basketball?\\n\\n(Thump, thump, thump... thump.)  Why risk it?  Earplugs are small.Sometimes I can think with noise.  If I already have momentum on\\n\\nsome project, I can work in noisy places.  I can edit an essay or\\n\\ndebug code in an airport.  But airports are not so bad: most of the\\n\\nnoise is whitish.  I couldn't work with the sound of a sitcom coming\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba62359f",
   "metadata": {},
   "source": [
    "Now we are going to pass a question to it and with that question we will compress the doc. The cool part is this doc will be contextually compressed, meaning the resulting file will only have the information relevant to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ff389",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor.compress_documents(documents=[splits[0]], query=\"test for what you like to do\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ac597",
   "metadata": {},
   "source": [
    "Great so we had a long document, now we have a shorter document with more dense information. Great for getting rid of the fluff. Let's try it out on our essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the authors view on the early stages of a startup?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ca868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The thing I probably repeat most is this recipe for a startup: get\\n\\na version 1 out fast, then improve it based on users\\' reactions.By \"release early\" I don\\'t mean you should release something full\\n\\nof bugs, but that you should release something minimal.  Users hate\\n\\nbugs, but they don\\'t seem to mind a minimal version 1, if there\\'s\\n\\nmore coming soon.There are several reasons it pays to get version 1 done fast.  One\\n\\nis that this is simply the right way to write software, whether for\\n\\na startup or not.  I\\'ve been repeating that since 1993, and I haven\\'t seen much since to\\n\\ncontradict it.  I\\'ve seen a lot of startups die because they were\\n\\ntoo slow to release stuff, and none because they were too quick.', metadata={'source': '../data/PaulGrahamEssaysLarge/startuplessons.txt'}),\n",
       " Document(page_content='\"Bring us your startups early,\" said Google\\'s speaker at the Startup School.  They\\'re quite\\n\\nexplicit about it: they like to acquire startups at just the point\\n\\nwhere they would do a Series A round.  (The Series A round is the\\n\\nfirst round of real VC funding; it usually happens in the first\\n\\nyear.) It is a brilliant strategy, and one that other big technology\\n\\ncompanies will no doubt try to duplicate.', metadata={'source': '../data/PaulGrahamEssaysLarge/vcsqueeze.txt'}),\n",
       " Document(page_content=\"Building office buildings for technology companies won't get you a\\n\\nsilicon valley, because the key stage in the life of a startup\\n\\nhappens before they want that kind of space.  The key stage is when\\n\\nthey're three guys operating out of an apartment.  Wherever the\\n\\nstartup is when it gets funded, it will stay.  The defining quality\", metadata={'source': '../data/PaulGrahamEssaysLarge/siliconvalley.txt'}),\n",
       " Document(page_content='of a startup, which in its raw form is more a distraction than a motivator.', metadata={'source': '../data/PaulGrahamEssaysLarge/vcsqueeze.txt'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (len(compressed_docs))\n",
    "compressed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64873a56",
   "metadata": {},
   "source": [
    "We now have 4 docs but they are shorter and only contain the information that is relevant to our query.\n",
    "\n",
    "Let's put it in our prompt template again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aad3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The author believes that the early stages of a startup are crucial. He advises startups to release a minimal version 1 of their product quickly and then improve it based on user feedback. He also mentions that many startups fail because they are too slow to release their products. Furthermore, he notes that the key stage in a startup's life often happens when it's still a small operation, possibly operating out of an apartment.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm.predict(text=PROMPT.format_prompt(\n",
    "    context=compressed_docs,\n",
    "    question=question\n",
    ").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0fc0a",
   "metadata": {},
   "source": [
    "### Parent Document Retriever\n",
    "\n",
    "[LangChain documentation](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever) does a great job describing this - my minor edits below:\n",
    "\n",
    "When you split your docs, you generally may want to have small documents, so that their embeddings can most accurately reflect their meaning. If too long, then the embeddings can lose meaning.\n",
    "\n",
    "But at the same time you may want to have information around those small chunks to keep context of the longer document.\n",
    "\n",
    "The ParentDocumentRetriever strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.\n",
    "\n",
    "Note that \"parent document\" refers to the document that a small chunk originated from. This can either be the whole raw document OR a larger chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the child documents. They should be small chunk size.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13536804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"return_full_documents\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    docstore=store, \n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33188eb",
   "metadata": {},
   "source": [
    "Now we will add the whole essays that we split above. We haven't chunked these essays yet, but the `.add_documents` will do the small chunking for us with the `child_splitter` above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aeab3c",
   "metadata": {},
   "source": [
    "Now if we were to put in a question or query, we'll get small chunks returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"what is some investing advice?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e267b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"people there are rich, or expect to be when their options vest.\\n\\nOrdinary employees find it very hard to recommend an acquisition;\\n\\nit's just too annoying to see a bunch of twenty year olds get rich\\n\\nwhen you're still working for salary.  Even if it's the right thing\\n\\nfor your company to do.The Solution(s)Bad as things look now, there is a way for VCs to save themselves.\", metadata={'doc_id': 'a4372dda-31dc-477f-9239-2ac45d11f3db', 'source': '../data/PaulGrahamEssaysLarge/vcsqueeze.txt'}),\n",
       " Document(page_content=\"the product is expensive to develop or sell, or simply because\\n\\nthey're wasteful.If you're paying attention, you'll be asking at this point not just\\n\\nhow to avoid the fatal pinch, but how to avoid being default dead.\\n\\nThat one is easy: don't hire too fast.  Hiring too fast is by far\\n\\nthe biggest killer of startups that raise money.\", metadata={'doc_id': '0314fd17-e53a-4c6e-9d80-2a721b8800df', 'source': '../data/PaulGrahamEssaysLarge/aord.txt'}),\n",
       " Document(page_content=\"[1]\\n\\nBut investors are so fickle that you can never\\n\\ndo more than start to count on them.  Sometimes something about your\\n\\nbusiness will spook investors even if your growth is great.  So no\\n\\nmatter how good your growth is, you can never safely treat fundraising\\n\\nas more than a plan A. You should always have a plan B as well: you\\n\\nshould know (as in write down) precisely what you'll need to do to\", metadata={'doc_id': '0314fd17-e53a-4c6e-9d80-2a721b8800df', 'source': '../data/PaulGrahamEssaysLarge/aord.txt'}),\n",
       " Document(page_content=\"commitment.If an acquirer thinks you're going to stick around no matter what,\\n\\nthey'll be more likely to buy you, because if they don't and you\\n\\nstick around, you'll probably grow, your price will go up, and\\n\\nthey'll be left wishing they'd bought you earlier.  Ditto for\\n\\ninvestors.  What really motivates investors, even big VCs, is not\\n\\nthe hope of good returns, but the fear of missing out.\\n\\n[6]\", metadata={'doc_id': 'ad7c0de6-ec3d-4637-9dea-8de2d37fa505', 'source': '../data/PaulGrahamEssaysLarge/startuplessons.txt'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d25ec4",
   "metadata": {},
   "source": [
    "Look how small those chunks are. Now we want to get the parent doc which those small docs are a part of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"what is some investing advice?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90334e60",
   "metadata": {},
   "source": [
    "I'm going to only do the first doc to save space, but there are more waiting for you. Keep in mind that LangChain will do the union of docs, so if you have two child docs from the same parent doc, you'll only return the parent doc once, not twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6dba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"November 2005In the next few years, venture capital funds will find themselves\\n\\nsqueezed from four directions.  They're already stuck with a seller's\\n\\nmarket, because of the huge amounts they raised at the end of the\\n\\nBubble and still haven't invested.  This by itself is not the end\\n\\nof the world.  In fact, it's just a more extreme version of the\\n\\nnorm\\n\\nin the VC business: too much money chasing too few deals.Unfortunately, those few deals now want less and less money, because\\n\\nit's getting so cheap to start a startup.  The four causes: open\\n\\nsource, which makes software free; Moore's law, which makes hardware\\n\\ngeometrically closer to free; the Web, which makes promotion free\\n\\nif you're good; and better languages, which make development a lot\\n\\ncheaper.When we started our startup in 1995, the first three were our biggest\\n\\nexpenses.  We had to pay $5000 for the Netscape Commerce Server,\\n\\nthe only software that then supported secure http connections.  We\\n\\npaid $3000 for a server with a 90\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieved_docs[0].page_content[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248003cc",
   "metadata": {},
   "source": [
    "However here we got the full document back. Sometimes this will be too long and we actually just want to get a larger chunk instead. Let's do that.\n",
    "\n",
    "Notice the chunk size difference between the parent splitter and child splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"return_split_parent_documents\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730beb7",
   "metadata": {},
   "source": [
    "This will set up our retriever for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a02dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    docstore=store, \n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3957715e",
   "metadata": {},
   "source": [
    "Now this time when we add documents two things will happen\n",
    "1. Larger chunks - We'll split our docs into large chunks\n",
    "2. Smaller chunks - We'll split our docs into smaller chunks\n",
    "\n",
    "Both of them will be combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15300930",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599816ab",
   "metadata": {},
   "source": [
    "Let's check out how many documents we have now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915585b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8feec5",
   "metadata": {},
   "source": [
    "Then let's go get our small chunks to make sure it's working and see how long they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46d334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"people there are rich, or expect to be when their options vest.\\n\\nOrdinary employees find it very hard to recommend an acquisition;\\n\\nit's just too annoying to see a bunch of twenty year olds get rich\\n\\nwhen you're still working for salary.  Even if it's the right thing\\n\\nfor your company to do.The Solution(s)Bad as things look now, there is a way for VCs to save themselves.\", metadata={'doc_id': 'd7cfabc8-b712-4fd1-8f1c-917c66cfdb68', 'source': '../data/PaulGrahamEssaysLarge/vcsqueeze.txt'}),\n",
       " Document(page_content=\"commitment.If an acquirer thinks you're going to stick around no matter what,\\n\\nthey'll be more likely to buy you, because if they don't and you\\n\\nstick around, you'll probably grow, your price will go up, and\\n\\nthey'll be left wishing they'd bought you earlier.  Ditto for\\n\\ninvestors.  What really motivates investors, even big VCs, is not\\n\\nthe hope of good returns, but the fear of missing out.\\n\\n[6]\", metadata={'doc_id': 'f6df5dfa-47e4-46be-9e77-8303a512b8c1', 'source': '../data/PaulGrahamEssaysLarge/startuplessons.txt'}),\n",
       " Document(page_content=\"April 2006(This essay is derived from a talk at the 2006\\n\\nStartup School.)The startups we've funded so far are pretty quick, but they seem\\n\\nquicker to learn some lessons than others.  I think it's because\\n\\nsome things about startups are kind of counterintuitive.We've now\\n\\ninvested\\n\\nin enough companies that I've learned a trick\\n\\nfor determining which points are the counterintuitive ones:\", metadata={'doc_id': 'd9e056c8-fee1-43e7-bbb1-8c3c435d663c', 'source': '../data/PaulGrahamEssaysLarge/startuplessons.txt'}),\n",
       " Document(page_content=\"the title of this essay, you already know most of what you need to\\n\\nknow about M&A in the first year.Notes[1]\\n\\nI'm not saying you should never sell.  I'm saying you should\\n\\nbe clear in your own mind about whether you want to sell or not,\\n\\nand not be led by manipulation or wishful thinking into trying to\\n\\nsell earlier than you otherwise would have.[2]\", metadata={'doc_id': '88d1d80d-dbb7-4c10-bfee-48df7f551452', 'source': '../data/PaulGrahamEssaysLarge/corpdev.txt'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"what is some investing advice?\")\n",
    "sub_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51162174",
   "metadata": {},
   "source": [
    "Now, let's do the full process, we'll see what small chunks are generated, but then return the larger chunks as our relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eaa7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='means VCs are now in the business of finding promising little 2-3\\n\\nman startups and pumping them up into companies that cost $100\\n\\nmillion to acquire.   They didn\\'t mean to be in this business; it\\'s\\n\\njust what their business has evolved into.Hence the fourth problem: the acquirers have begun to realize they\\n\\ncan buy wholesale.  Why should they wait for VCs to make the startups\\n\\nthey want more expensive?  Most of what the VCs add, acquirers don\\'t\\n\\nwant anyway.  The acquirers already have brand recognition and HR\\n\\ndepartments.  What they really want is the software and the developers,\\n\\nand that\\'s what the startup is in the early phase: concentrated\\n\\nsoftware and developers.Google, typically, seems to have been the first to figure this out.\\n\\n\"Bring us your startups early,\" said Google\\'s speaker at the Startup School.  They\\'re quite\\n\\nexplicit about it: they like to acquire startups at just the point\\n\\nwhere they would do a Series A round.  (The Series A round is the\\n\\nfirst round of real VC funding; it usually happens in the first\\n\\nyear.) It is a brilliant strategy, and one that other big technology\\n\\ncompanies will no doubt try to duplicate.  Unless they want to have\\n\\nstill more of their lunch eaten by Google.Of course, Google has an advantage in buying startups: a lot of the\\n\\npeople there are rich, or expect to be when their options vest.\\n\\nOrdinary employees find it very hard to recommend an acquisition;\\n\\nit\\'s just too annoying to see a bunch of twenty year olds get rich\\n\\nwhen you\\'re still working for salary.  Even if it\\'s the right thing\\n\\nfor your company to do.The Solution(s)Bad as things look now, there is a way for VCs to save themselves.\\n\\nThey need to do two things, one of which won\\'t surprise them, and\\n\\nanother that will seem an anathema.Let\\'s start with the obvious one: lobby to get Sarbanes-Oxley\\n\\nloosened.  This law was created to prevent future Enrons, not to\\n\\ndestroy the IPO market.  Since the IPO market was practically dead', metadata={'source': '../data/PaulGrahamEssaysLarge/vcsqueeze.txt'})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "larger_chunk_relevant_docs = retriever.get_relevant_documents(\"what is some investing advice?\")\n",
    "larger_chunk_relevant_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c03c518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One piece of investing advice is to release a minimal version 1 of a product quickly, then improve it based on users' reactions. This is because it's dangerous to guess what users will like without knowing them. Another advice is for Venture Capitalists to lobby to get Sarbanes-Oxley loosened, as this law was not created to destroy the IPO market. Additionally, it's advised not to sell a startup too early, but to be clear about whether you want to sell or not, and not be led by manipulation or wishful thinking.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "question = \"what is some investing advice?\"\n",
    "\n",
    "llm.predict(text=PROMPT.format_prompt(\n",
    "    context=larger_chunk_relevant_docs,\n",
    "    question=question\n",
    ").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6afd3b",
   "metadata": {},
   "source": [
    "### Ensemble Retriever\n",
    "\n",
    "The next one on our list combines multiple retrievers together. The goal here is to see what multiple methods return, then pull them together for (hopefully) better results.\n",
    "\n",
    "You may need to install bm25 with `!pip install rank_bm25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e2c8d6",
   "metadata": {},
   "source": [
    "We'll use a [BM25 retriever](https://en.wikipedia.org/wiki/Okapi_BM25) for this one which is really good at keyword matching (vs semantic). When you combine this method with regular semantic search it's known as hybrid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37920118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "bm25_retriever.k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b8f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(splits, embedding)\n",
    "vectordb = vectordb.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5457e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, vectordb], weights=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930afade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ensemble_docs = ensemble_retriever.get_relevant_documents(\"what is some investing advice?\")\n",
    "len(ensemble_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7059fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One piece of investing advice is to make a larger number of smaller investments instead of a handful of giant ones. It is also suggested to fund younger, more technical founders instead of MBAs and let the founders remain as CEO. Another advice is that the best sources of seed funding are successful startup founders, as they can also provide valuable advice. However, it's important to be aware of the changing nature of the world and industries, as what may seem like a bad idea initially could become a good one due to changes in the world.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "question = \"what is some investing advice?\"\n",
    "\n",
    "llm.predict(text=PROMPT.format_prompt(\n",
    "    context=ensemble_docs,\n",
    "    question=question\n",
    ").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16310f1e",
   "metadata": {},
   "source": [
    "### Self Querying\n",
    "\n",
    "The last one we'll look at today is self querying. This is when the retriever has the ability to query itself. It does this so it can use filters when doing it's final query.\n",
    "\n",
    "This means it'll use the users query for semantic search, but also its own query for filtering (so the user doesn't have to give a structured filter).\n",
    "\n",
    "You may need to install `!pip install lark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9424495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b170e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'vectorstore' in globals(): # If you've already made your vectordb this will delete it so you start fresh\n",
    "    vectorstore.delete_collection()\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    splits, embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cff0b6",
   "metadata": {},
   "source": [
    "Below is the information on the fitlers available. This will help the model know which filters to semantically search for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info=[\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The filename of the essay\", \n",
    "        type=\"string or list[string]\", \n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ad39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Essays from Paul Graham\"\n",
    "retriever = SelfQueryRetriever.from_llm(llm,\n",
    "                                        vectorstore,\n",
    "                                        document_content_description,\n",
    "                                        metadata_field_info,\n",
    "                                        verbose=True,\n",
    "                                        enable_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487467a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='figure out what you like to do' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='source', value='../data/PaulGrahamEssaysLarge/island.txt') limit=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"if I could only figure out what.As for books, I know the house would probably have something to\\n\\nread.  On the average trip I bring four books and only read one of\\n\\nthem, because I find new books to read en route.  Really bringing\\n\\nbooks is insurance.I realize this dependence on books is not entirely good—that what\\n\\nI need them for is distraction.  The books I bring on trips are\\n\\noften quite virtuous, the sort of stuff that might be assigned\\n\\nreading in a college class.  But I know my motives aren't virtuous.\\n\\nI bring books because if the world gets boring I need to be able\\n\\nto slip into another distilled by some writer.  It's like eating\\n\\njam when you know you should be eating fruit.There is a point where I'll do without books.  I was walking in\\n\\nsome steep mountains once, and decided I'd rather just think, if I\\n\\nwas bored, rather than carry a single unnecessary ounce.  It wasn't\\n\\nso bad.  I found I could entertain myself by having ideas instead\\n\\nof reading other people's.  If you stop eating jam, fruit starts\\n\\nto taste better.So maybe I'll try not bringing books on some future trip.  They're\\n\\ngoing to have to pry the plugs out of my cold, dead ears, however.\", metadata={'source': '../data/PaulGrahamEssaysLarge/island.txt'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"Return only 1 essay. What is one thing you can do to figure out what you like to do from source '../data/PaulGrahamEssaysLarge/island.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac641cb",
   "metadata": {},
   "source": [
    "It's kind of annoying to have to put in the full file name, a user doesn't want to do that. Let's change `source` to `essay` and the file path w/ the essay name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for split in splits:\n",
    "    split.metadata['essay'] = re.search(r'[^/]+(?=\\.\\w+$)', split.metadata['source']).group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0405deea",
   "metadata": {},
   "source": [
    "Ok now that we did that, let's make a new field info config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info=[\n",
    "    AttributeInfo(\n",
    "        name=\"essay\",\n",
    "        description=\"The name of the essay\", \n",
    "        type=\"string or list[string]\", \n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb497f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'vectorstore' in globals(): # If you've already made your vectordb this will delete it so you start fresh\n",
    "    vectorstore.delete_collection()\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    splits, embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Essays from Paul Graham\"\n",
    "retriever = SelfQueryRetriever.from_llm(llm,\n",
    "                                        vectorstore,\n",
    "                                        document_content_description,\n",
    "                                        metadata_field_info,\n",
    "                                        verbose=True,\n",
    "                                        enable_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57baa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='investment advice' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='essay', value='worked') limit=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='should make a larger number of smaller investments instead of a\\n\\nhandful of giant ones, they should be funding younger, more technical\\n\\nfounders instead of MBAs, they should let the founders remain as\\n\\nCEO, and so on.One of my tricks for writing essays had always been to give talks.\\n\\nThe prospect of having to stand up in front of a group of people\\n\\nand tell them something that won\\'t waste their time is a great\\n\\nspur to the imagination. When the Harvard Computer Society, the\\n\\nundergrad computer club, asked me to give a talk, I decided I would\\n\\ntell them how to start a startup. Maybe they\\'d be able to avoid the\\n\\nworst of the mistakes we\\'d made.So I gave this talk, in the course of which I told them that the\\n\\nbest sources of seed funding were successful startup founders,\\n\\nbecause then they\\'d be sources of advice too. Whereupon it seemed\\n\\nthey were all looking expectantly at me. Horrified at the prospect\\n\\nof having my inbox flooded by business plans (if I\\'d only known),\\n\\nI blurted out \"But not me!\" and went on with the talk. But afterward\\n\\nit occurred to me that I should really stop procrastinating about\\n\\nangel investing. I\\'d been meaning to since Yahoo bought us, and now\\n\\nit was 7 years later and I still hadn\\'t done one angel investment.Meanwhile I had been scheming with Robert and Trevor about projects\\n\\nwe could work on together. I missed working with them, and it seemed', metadata={'essay': 'worked', 'source': '../data/PaulGrahamEssaysLarge/worked.txt'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"Tell me about investment advice the 'worked' essay? return only 1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
